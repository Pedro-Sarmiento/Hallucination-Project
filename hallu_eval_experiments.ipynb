{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h4I9BbUWNGAT"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:10:48.042177Z",
     "start_time": "2025-04-04T12:10:48.038856Z"
    }
   },
   "source": [
    "# %pip uninstall torch torchvision torchaudio -y\n",
    "# %pip install torch==2.2.2+cu121 torchvision==0.17.2+cu121 torchaudio==2.2.2+cu121 -f https://download.pytorch.org/whl/torch_stable.html\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S_orD5O6M0Sz",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:21.743708Z",
     "start_time": "2025-04-04T12:10:48.523481Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import pipeline\n",
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\HalluDetect-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulEMNzTdNK6p"
   },
   "source": [
    "# Reading Dataset\n",
    "\n",
    "`The method loadDataset receieves the path where the datasets json files of the HaluEval repository are. You just need to pass your path and the name of the dataset you are going to use.`\n",
    "\n",
    "## Dataset Names:\n",
    "- summarization\n",
    "- dialogue\n",
    "- qa\n",
    "- general\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BBBHKHm1PLgN",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:21.751636Z",
     "start_time": "2025-04-04T12:11:21.748611Z"
    }
   },
   "source": [
    "## As a recomendation keep these two with the same naming if you do not want to change many things\n",
    "datasetName = \"qa\"\n",
    "task = datasetName\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z559eoe9EsIw",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:21.794580Z",
     "start_time": "2025-04-04T12:11:21.788932Z"
    }
   },
   "source": [
    "def loadDataset(path=\"HaluEval/data\", datasetName=\"qa\"):\n",
    "    data = pd.read_json(\n",
    "        (path + \"/\" + datasetName + \"_data.json\"), lines=True\n",
    "    )\n",
    "    return data"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUanNTojFm-j"
   },
   "source": [
    "## For this particular example we are loading the qa_data.json since is the one that takes the less time to process in case you want to test quickly how it works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fwC9pOCKM0XJ",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:21.960900Z",
     "start_time": "2025-04-04T12:11:21.805150Z"
    }
   },
   "source": [
    "data = loadDataset(datasetName=datasetName)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "h2DOjPBTM0ZE",
    "outputId": "4942e66a-a7ad-42a0-88b5-c3d0e69f4c35",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:21.986081Z",
     "start_time": "2025-04-04T12:11:21.971030Z"
    }
   },
   "source": [
    "data.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                           knowledge  \\\n",
       "0  Arthur's Magazine (1844–1846) was an American ...   \n",
       "1  The Oberoi family is an Indian family that is ...   \n",
       "2  Allison Beth \"Allie\" Goertz (born March 2, 199...   \n",
       "3  Margaret \"Peggy\" Seeger (born June 17, 1935) i...   \n",
       "4   It is a hygroscopic solid that is highly solu...   \n",
       "\n",
       "                                            question             right_answer  \\\n",
       "0  Which magazine was started first Arthur's Maga...        Arthur's Magazine   \n",
       "1  The Oberoi family is part of a hotel company t...                    Delhi   \n",
       "2  Musician and satirist Allie Goertz wrote a son...  President Richard Nixon   \n",
       "3    What nationality was James Henry Miller's wife?                 American   \n",
       "4  Cadmium Chloride is slightly soluble in this c...                  alcohol   \n",
       "\n",
       "                                 hallucinated_answer  \n",
       "0                 First for Women was started first.  \n",
       "1  The Oberoi family's hotel company is based in ...  \n",
       "2  Allie Goertz wrote a song about Milhouse, a po...  \n",
       "3             James Henry Miller's wife was British.  \n",
       "4                       water with a hint of alcohol  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knowledge</th>\n",
       "      <th>question</th>\n",
       "      <th>right_answer</th>\n",
       "      <th>hallucinated_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arthur's Magazine (1844–1846) was an American ...</td>\n",
       "      <td>Which magazine was started first Arthur's Maga...</td>\n",
       "      <td>Arthur's Magazine</td>\n",
       "      <td>First for Women was started first.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Oberoi family is an Indian family that is ...</td>\n",
       "      <td>The Oberoi family is part of a hotel company t...</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>The Oberoi family's hotel company is based in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Allison Beth \"Allie\" Goertz (born March 2, 199...</td>\n",
       "      <td>Musician and satirist Allie Goertz wrote a son...</td>\n",
       "      <td>President Richard Nixon</td>\n",
       "      <td>Allie Goertz wrote a song about Milhouse, a po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Margaret \"Peggy\" Seeger (born June 17, 1935) i...</td>\n",
       "      <td>What nationality was James Henry Miller's wife?</td>\n",
       "      <td>American</td>\n",
       "      <td>James Henry Miller's wife was British.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is a hygroscopic solid that is highly solu...</td>\n",
       "      <td>Cadmium Chloride is slightly soluble in this c...</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>water with a hint of alcohol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPhslb8B2LK_",
    "outputId": "d69bcedf-4bd2-40a7-cc56-097631b62d90",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:22.032511Z",
     "start_time": "2025-04-04T12:11:22.028850Z"
    }
   },
   "source": [
    "len(data)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zw6PgoaCNRCz"
   },
   "source": [
    "# Setting Device to use the GPU\n",
    "\n",
    "We use the T4 GPU in Colab since the heaviest computation for us is the inference of the LLM-Evaluator. Therefore, T4 seem as the better fit."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gpd-VUHOM0bn",
    "outputId": "755c5ea9-0c01-4c91-dc12-383b6f048515",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:22.202826Z",
     "start_time": "2025-04-04T12:11:22.121393Z"
    }
   },
   "source": [
    "import torch\n",
    "print(torch.__version__)  # E.g., 2.2.1\n",
    "print(torch.cuda.is_available())  # Should be True with CUDA support\n",
    "print(torch.version.cuda)  # Should say 12.1 if matched"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n",
      "True\n",
      "12.1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "M-imxNzmM0du",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:22.259785Z",
     "start_time": "2025-04-04T12:11:22.255617Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5GcfQIoNWfm"
   },
   "source": [
    "## Generic LLMModel class to reuse the functionality of extracting the features.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q_H0RABQM0f7",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:26.599305Z",
     "start_time": "2025-04-04T12:11:22.293822Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Cargar el modelo de spaCy (español en este caso; cambia a 'en_core_web_sm' para inglés si lo prefieres)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "class LLMModel:\n",
    "    def __init__(self):\n",
    "        self.model_name = None\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def getName(self) -> str:\n",
    "        return self.model_name\n",
    "\n",
    "    def getSanitizedName(self) -> str:\n",
    "        return self.model_name.replace(\"/\", \"__\")\n",
    "\n",
    "    def getMaxLength(self):\n",
    "        return self.model.config.max_position_embeddings\n",
    "\n",
    "    def truncate_string_by_len(self, s, truncate_len):\n",
    "        words = s.split()\n",
    "        truncated_words = words[:-truncate_len] if truncate_len > 0 else words\n",
    "        return \" \".join(truncated_words)\n",
    "\n",
    "    def getVocabProbsAtPos(self, pos, token_probs):\n",
    "        sorted_probs, sorted_indices = torch.sort(token_probs[pos, :], descending=True)\n",
    "        return sorted_probs\n",
    "\n",
    "    def getDiffVocab(self, vocabProbs, tprob):\n",
    "        return (vocabProbs[0] - tprob).item()\n",
    "\n",
    "    def getDiffMaximumWithMinimum(self, vocabProbs):\n",
    "        return (vocabProbs[0] - vocabProbs[-1]).item()\n",
    "\n",
    "    def extractFeatures(\n",
    "        self,\n",
    "        knowledge=\"\",\n",
    "        conditionted_text=\"\",\n",
    "        generated_text=\"\",\n",
    "        features_to_extract={\"mtp\": True},\n",
    "    ):\n",
    "        total_len = len(knowledge) + len(conditionted_text) + len(generated_text)\n",
    "        truncate_len = max(total_len - self.tokenizer.model_max_length, 0)\n",
    "\n",
    "        # Truncar si es necesario\n",
    "        knowledge = self.truncate_string_by_len(knowledge, truncate_len // 2)\n",
    "        conditionted_text = self.truncate_string_by_len(\n",
    "            conditionted_text, truncate_len - (truncate_len // 2)\n",
    "        )\n",
    "\n",
    "        # Tokenizar la entrada completa\n",
    "        inputs = self.tokenizer(\n",
    "            [knowledge + conditionted_text + generated_text],\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.getMaxLength(),\n",
    "            truncation=True,\n",
    "        )\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "        tokens_generated_length = len(self.tokenizer.tokenize(generated_text))\n",
    "        start_index = logits.shape[1] - tokens_generated_length\n",
    "        conditional_probs = probs[0, start_index:]\n",
    "\n",
    "        token_ids_generated = inputs[\"input_ids\"][0, start_index:].tolist()\n",
    "        token_probs_generated = [\n",
    "            conditional_probs[i, tid].item() for i, tid in enumerate(token_ids_generated)\n",
    "        ]\n",
    "        tokens_generated = self.tokenizer.convert_ids_to_tokens(token_ids_generated)\n",
    "\n",
    "        # Filtrar tokens de fin de secuencia\n",
    "        EOS_TOKENS = {'</s>', '<EOS>', '<eos>'}\n",
    "        non_eos = [\n",
    "            (token, prob)\n",
    "            for token, prob in zip(tokens_generated, token_probs_generated)\n",
    "            if token not in EOS_TOKENS\n",
    "        ]\n",
    "        if non_eos:\n",
    "            min_prob_token, min_prob = min(non_eos, key=lambda x: x[1])\n",
    "        else:\n",
    "            min_prob_token, min_prob = None, None\n",
    "\n",
    "        # Calcular características usando el token filtrado\n",
    "        allFeatures = {\"mtp\": min_prob, \"MDVTP\": -1, \"MMDVP\": 100000000000}\n",
    "        selectedFeatures = {}\n",
    "        if features_to_extract.get(\"mtp\", False):\n",
    "            selectedFeatures[\"mtp\"] = min_prob\n",
    "\n",
    "        if features_to_extract.get(\"MDVTP\", False) or features_to_extract.get(\"MMDVP\", False):\n",
    "            maximum_diff_with_vocab = -1\n",
    "            minimum_vocab_extreme_diff = 100000000000\n",
    "            size = len(token_probs_generated)\n",
    "            for pos in range(size):\n",
    "                vocabProbs = self.getVocabProbsAtPos(pos, conditional_probs)\n",
    "                maximum_diff_with_vocab = max(\n",
    "                    maximum_diff_with_vocab,\n",
    "                    self.getDiffVocab(vocabProbs, token_probs_generated[pos]),\n",
    "                )\n",
    "                minimum_vocab_extreme_diff = min(\n",
    "                    minimum_vocab_extreme_diff,\n",
    "                    self.getDiffMaximumWithMinimum(vocabProbs),\n",
    "                )\n",
    "            if features_to_extract.get(\"MDVTP\", False):\n",
    "                selectedFeatures[\"MDVTP\"] = maximum_diff_with_vocab\n",
    "            if features_to_extract.get(\"MMDVP\", False):\n",
    "                selectedFeatures[\"MMDVP\"] = minimum_vocab_extreme_diff\n",
    "\n",
    "        # Ahora se retornan, además, el token y probabilidad filtrados\n",
    "        return selectedFeatures, tokens_generated, token_probs_generated, min_prob_token, min_prob\n",
    "\n",
    "\n",
    "    def get_mtp_by_pos(self, knowledge=\"\", conditionted_text=\"\", generated_text=\"\"):\n",
    "        features, tokens_generated, token_probs_generated, _, _ = self.extractFeatures(\n",
    "            knowledge, conditionted_text, generated_text\n",
    "        )\n",
    "\n",
    "\n",
    "        generated_text_clean = self.tokenizer.convert_tokens_to_string(tokens_generated).replace(\"Ġ\", \" \").strip()\n",
    "        doc = nlp(generated_text_clean)\n",
    "\n",
    "\n",
    "        pos_probs = {}\n",
    "        token_idx = 0\n",
    "        for spacy_token in doc:\n",
    "            word = spacy_token.text\n",
    "            pos = spacy_token.pos_\n",
    "            while token_idx < len(tokens_generated) and tokens_generated[token_idx].replace(\"Ġ\", \"\").strip() in word:\n",
    "                if pos not in pos_probs:\n",
    "                    pos_probs[pos] = []\n",
    "                pos_probs[pos].append(token_probs_generated[token_idx])\n",
    "                token_idx += 1\n",
    "                break\n",
    "            if token_idx >= len(tokens_generated):\n",
    "                break\n",
    "\n",
    "        mtp_by_pos = {pos: min(probs) for pos, probs in pos_probs.items() if probs}\n",
    "        return features, mtp_by_pos\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:26.612251Z",
     "start_time": "2025-04-04T12:11:26.607358Z"
    }
   },
   "source": [
    "class BartCNN(LLMModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()  # Llamar primero a la inicialización de la clase base\n",
    "        self.model_name = \"facebook/bart-large-cnn\"\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(self.model_name)\n",
    "        self.model.to(device)  # Mover el modelo al dispositivo después de cargarlo\n",
    "\n",
    "    def generate(self, inpt):\n",
    "        inputs = self.tokenizer(\n",
    "            [inpt],\n",
    "            max_length=self.getMaxLength(),\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True\n",
    "        )\n",
    "        inputs = {key: value.to(self.model.device) for key, value in inputs.items()}\n",
    "        summary_ids = self.model.generate(inputs[\"input_ids\"])\n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtKuMaZNQ8MO"
   },
   "source": [
    "# The Dictionary `features_to_extract` defines which features will be use in this experiment.\n",
    "\n",
    "## Features Meaning:\n",
    "\n",
    "- `mtp` : Take the minimum of the probabilities that the LLM_E gives to the tokens on the generated-text.\n",
    "- `avgtp` : Take the average of the probabilities that the LLM_E\n",
    "gives to the tokens on the generated-text.\n",
    "- `MDVTP` : Take the maximum from all the differences\n",
    "between the token with the highest probability\n",
    "according to LLM_E at position i and the\n",
    "assigned probability from LLM_E to the token at position i in the generated_text.\n",
    "- `MMDVP` : Take the maximum from all the differences between the token with the highest probability according to $LLM_E$ at position $i$ ($v^*$) and the token with the lowest probability according to $LLM_E$ at position $i$ ($v^-$).\n",
    "- Probabilidad mínima del token: El valor más bajo entre las probabilidades de los tokens generados, que puede indicar incertidumbre.\n",
    "- Probabilidad promedio del token: El promedio de las probabilidades, mostrando la confianza general del modelo.\n",
    "- Diferencia máxima con la probabilidad más alta del vocabulario: Compara el token elegido con el más probable en el vocabulario, y si la diferencia es grande, podría ser una alucinación.\n",
    "- Diferencia mínima entre máxima y mínima del vocabulario: Mide la dispersión de probabilidades, que puede indicar si el modelo está muy seguro o no.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5OjJxW1PRHrE",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:26.633184Z",
     "start_time": "2025-04-04T12:11:26.627922Z"
    }
   },
   "source": [
    "feature_to_extract = 'mtp'\n",
    "\n",
    "available_features_to_extract = [\"mtp\", \"avgtp\", \"MDVTP\", \"MMDVP\"]\n",
    "if feature_to_extract == 'all':\n",
    "    features_to_extract = {\n",
    "        feature: True for feature in available_features_to_extract\n",
    "    }\n",
    "else:\n",
    "    features_to_extract = {\n",
    "        feature: True if feature == feature_to_extract else False\n",
    "        for feature in available_features_to_extract\n",
    "    }\n",
    "\n",
    "features_to_extract"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mtp': True, 'avgtp': False, 'MDVTP': False, 'MMDVP': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpgpfLjKQiwe"
   },
   "source": [
    "## Cleaning Cache on GPU to save memory"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4t5DYYzaM0nE",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:26.841630Z",
     "start_time": "2025-04-04T12:11:26.684584Z"
    }
   },
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-GdklJnQIB8"
   },
   "source": [
    "## This cell is to instantiate the model you intend to use for the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUXW1LNxQnal"
   },
   "source": [
    "## This cell creates the dataset separation of `10%` for training and `90%` for testing depending on what task you are addressing. The following explanation is what happens if summarization is the task used. But the same explanation applies to all tasks and also you cand pass as parameter how many data points you want to include in training.\n",
    "\n",
    "## Example: The data is separated on 2000 (1000 of document with right summary and 1000 with the same document but with the hallucinated summary). The rest which is 18000 is used to for testing.\n",
    "\n",
    "### As expected from previous cells the task string expected are:\n",
    "- `summarization`\n",
    "- `qa`\n",
    "- `dialogue`\n",
    "- `general`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JCPqy3UTLrJa",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:26.873623Z",
     "start_time": "2025-04-04T12:11:26.860717Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "# Extrae y organiza los campos relevantes de cada fila del dataset según la tarea y la opción de incluir conocimiento.\n",
    "def loadRowData(taskName, row, includeKnowledge=False):\n",
    "    print(taskName)\n",
    "    if taskName == \"summarization\":\n",
    "        return \"\", row[\"document\"], row[\"right_summary\"], row[\"hallucinated_summary\"]\n",
    "    elif taskName == \"qa\":\n",
    "        if includeKnowledge:\n",
    "            return (\n",
    "                row[\"knowledge\"],\n",
    "                row[\"question\"],\n",
    "                row[\"right_answer\"],\n",
    "                row[\"hallucinated_answer\"],\n",
    "            )\n",
    "        else:\n",
    "            return \"\", row[\"question\"], row[\"right_answer\"], row[\"hallucinated_answer\"]\n",
    "\n",
    "    elif taskName == \"dialogue\":\n",
    "        if includeKnowledge:\n",
    "            return (\n",
    "                row[\"knowledge\"],\n",
    "                row[\"dialogue_history\"],\n",
    "                row[\"right_response\"],\n",
    "                row[\"hallucinated_response\"],\n",
    "            )\n",
    "        else:\n",
    "            return (\n",
    "                \"\",\n",
    "                row[\"dialogue_history\"],\n",
    "                row[\"right_response\"],\n",
    "                row[\"hallucinated_response\"],\n",
    "            )\n",
    "\n",
    "    elif taskName == \"general\":\n",
    "        return (\n",
    "            \"\",\n",
    "            row[\"user_query\"],\n",
    "            row[\"chatgpt_response\"],\n",
    "            row[\"hallucination_label\"],\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Task not supported\")\n",
    "\n",
    "# Transforma la estructura del dataset para que cada ejemplo contenga una entrada (por ejemplo, el texto condicionado) y una etiqueta que indique si la respuesta es correcta o es una alucinación.\n",
    "def adaptDataset(data, taskName):\n",
    "    datasetAdapted = None\n",
    "    if taskName == \"general\":\n",
    "        # There is data point that is filling the <mask> token but that gives error with some LLMs\n",
    "        datasetAdapted = [\n",
    "            (\n",
    "                (knowledge, document, response, 1)\n",
    "                if hallu == \"yes\"\n",
    "                else (knowledge, document, response, 0)\n",
    "            )\n",
    "            for knowledge, document, response, hallu in data\n",
    "            if \"<mask>\" not in document and \"<mask>\" not in response\n",
    "        ]\n",
    "\n",
    "    elif taskName == \"summarization\" or taskName == \"qa\" or taskName == \"dialogue\":\n",
    "        datasetAdapted = [\n",
    "            (knowledge, document, right, 1)\n",
    "            for knowledge, document, right, hallu in data\n",
    "        ] + [\n",
    "            (knowledge, document, hallu, 0)\n",
    "            for knowledge, document, right, hallu in data\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Task not supported\")\n",
    "\n",
    "    random.shuffle(datasetAdapted)\n",
    "    return datasetAdapted\n",
    "\n",
    "# Divide el DataFrame original en conjuntos de entrenamiento, validación y prueba, adaptándolos para que estén listos para ser usados en un modelo o algoritmo de aprendizaje supervisado.\n",
    "def splitDataset(\n",
    "    data: pd.DataFrame,\n",
    "    taskName: str,\n",
    "    trainingSize: int,\n",
    "    valSize: int,\n",
    "    includeKnowledge=False,\n",
    "):\n",
    "\n",
    "    dataset = []\n",
    "    for _, row in data.iterrows():\n",
    "        knowledge, text, right, hallu = loadRowData(taskName, row, includeKnowledge)\n",
    "        dataset.append((knowledge, text, right, hallu))\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    dataset_train = dataset[:trainingSize]  # Take only trainingSize\n",
    "    dataset_val = (\n",
    "        []\n",
    "    )  # dataset[trainingSize:trainingSize + valSize] # Take only trainingSize\n",
    "    dataset_test = dataset[trainingSize:]  # Take the rest as testing\n",
    "\n",
    "    datasetAdaptedTrain = adaptDataset(dataset_train, taskName)\n",
    "    datasetAdaptedValidation = adaptDataset(dataset_val, taskName)\n",
    "    datasetAdaptedTest = adaptDataset(dataset_test, taskName)\n",
    "\n",
    "    X_train = [(x, q, y) for x, q, y, _ in datasetAdaptedTrain]\n",
    "    Y_train = [z for _, _, _, z in datasetAdaptedTrain]\n",
    "\n",
    "    X_val = [(x, q, y) for x, q, y, _ in datasetAdaptedValidation]\n",
    "    Y_val = [z for _, _, _, z in datasetAdaptedValidation]\n",
    "\n",
    "    X_test = [(x, q, y) for x, q, y, _ in datasetAdaptedTest]\n",
    "    Y_test = [z for _, _, _, z in datasetAdaptedTest]\n",
    "\n",
    "    return X_train, Y_train, X_val, Y_val, X_test, Y_test"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "x8zc6RFq_Tx8",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:26.884709Z",
     "start_time": "2025-04-04T12:11:26.881901Z"
    }
   },
   "source": [
    "includeKnowledge = True\n",
    "includeConditioned = True"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dBUtDsPsPFy3",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:27.300087Z",
     "start_time": "2025-04-04T12:11:26.893199Z"
    }
   },
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = splitDataset(\n",
    "    data, task, 1000, 0, includeKnowledge=includeKnowledge\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n",
      "qa\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mHLWT2nrcli",
    "outputId": "ee93c5e0-807f-44e8-d461-851cb5dc483d",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:27.313178Z",
     "start_time": "2025-04-04T12:11:27.309781Z"
    }
   },
   "source": [
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_val), len(Y_val))\n",
    "print(len(X_test), len(Y_test))  # verify the sizes look right"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "0 0\n",
      "18000 18000\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fPTEeI-eBmxj"
   },
   "source": [
    "## To Save the separation if needed"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5aEkIWkkBlwb",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:27.700546Z",
     "start_time": "2025-04-04T12:11:27.344242Z"
    }
   },
   "source": [
    "train_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Knowledge\": [x[0] for x in X_train],\n",
    "        \"Conditioned Text\": [x[1] for x in X_train],\n",
    "        \"Generated Text\": [x[2] for x in X_train],\n",
    "        \"Label\": Y_train,\n",
    "    }\n",
    ")\n",
    "\n",
    "val_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Knowledge\": [x[0] for x in X_val],\n",
    "        \"Conditioned Text\": [x[1] for x in X_val],\n",
    "        \"Generated Text\": [x[2] for x in X_val],\n",
    "        \"Label\": Y_val,\n",
    "    }\n",
    ")\n",
    "\n",
    "test_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Knowledge\": [x[0] for x in X_test],\n",
    "        \"Conditioned Text\": [x[1] for x in X_test],\n",
    "        \"Generated Text\": [x[2] for x in X_test],\n",
    "        \"Label\": Y_test,\n",
    "    }\n",
    ")\n",
    "\n",
    "#Export to CSV\n",
    "if includeKnowledge:\n",
    "    train_df.to_csv( (task + '_knowledge_train_data.csv'), index=False)\n",
    "    test_df.to_csv( (task + '_knowledge_test_data.csv'), index=False)\n",
    "else:\n",
    "    train_df.to_csv( (task + '_train_data.csv'), index=False)\n",
    "    val_df.to_csv((task + '_val_data.csv'), index=False)\n",
    "    test_df.to_csv( (task + '_test_data.csv'), index=False)"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kNfL_hTsEZgB",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:27.708680Z",
     "start_time": "2025-04-04T12:11:27.704578Z"
    }
   },
   "source": [
    "# Esta función permite transformar un DataFrame en dos listas de datos, donde cada elemento de X es una tupla formada por los valores de \"Knowledge\", \"Conditioned Text\" y \"Generated Text\" (según los flags de inclusión), y cada elemento de Y es la etiqueta correspondiente, facilitando así su uso en tareas de entrenamiento o evaluación de modelos.\n",
    "\n",
    "def getXY(df: pd.DataFrame, includeKnowledge=True, includeConditioned=True):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Iterate over rows using itertuples\n",
    "    for _, row in df.iterrows():\n",
    "        x, c, g = (\n",
    "            row[\"Knowledge\"] if includeKnowledge else \"\",\n",
    "            row[\"Conditioned Text\"] if includeConditioned else \"\",\n",
    "            row[\"Generated Text\"],\n",
    "        )\n",
    "        y = row[\"Label\"]\n",
    "\n",
    "        # Append values to respective lists\n",
    "        X.append((x, c, g))\n",
    "        Y.append(y)\n",
    "    return X, Y"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BinJfLy9Dwjd",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:28.340756Z",
     "start_time": "2025-04-04T12:11:27.719679Z"
    }
   },
   "source": [
    "X_train, Y_train = getXY(\n",
    "    train_df, includeKnowledge=includeKnowledge, includeConditioned=includeConditioned\n",
    ")\n",
    "X_val, Y_val = getXY(\n",
    "    val_df, includeKnowledge=includeKnowledge, includeConditioned=includeConditioned\n",
    ")\n",
    "X_test, Y_test = getXY(\n",
    "    test_df, includeKnowledge=includeKnowledge, includeConditioned=includeConditioned\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z2BNY5b1M0ri",
    "outputId": "1bc0b612-91bc-4cf2-82d0-5b90a66f60ea",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:28.354766Z",
     "start_time": "2025-04-04T12:11:28.350957Z"
    }
   },
   "source": [
    "print(len(X_train), len(Y_train))\n",
    "print(len(X_val), len(Y_val))\n",
    "print(len(X_test), len(Y_test))  # verify the sizes look right"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 2000\n",
      "0 0\n",
      "18000 18000\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UDHkOclDQMDW",
    "outputId": "b68dea36-178c-49e8-bad8-0ee268689171",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:28.386717Z",
     "start_time": "2025-04-04T12:11:28.381688Z"
    }
   },
   "source": [
    "X_test[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The Rhymers\\' Club was a group of London-based male poets, founded in 1890 by W. B. Yeats and Ernest Rhys. They met at the London pub ‘Ye Olde Cheshire Cheese’ in Fleet Street and in the \\'Domino Room\\' of the \"Café Royal\".Ye Olde Cheshire Cheese is a Grade II listed public house at 145 Fleet Street, on Wine Office Court, City of London.',\n",
       " 'What was the address of the public house where a group of London-based male poets, founded in 1890 met?',\n",
       " '145 Fleet Street')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lo_9Sd3n9BYA",
    "outputId": "0c46c199-6e0c-459c-c1f2-27856896a814",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:11:28.417500Z",
     "start_time": "2025-04-04T12:11:28.413560Z"
    }
   },
   "source": "Y_test[0]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNNRZuisSUrf"
   },
   "source": [
    "## Extracting the features for the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.168864Z",
     "start_time": "2025-04-04T12:11:28.466143Z"
    }
   },
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# El código recorre todos los ejemplos del conjunto de entrenamiento, extrae características relevantes mediante un método del modelo y almacena los resultados en una lista.\n",
    "\n",
    "def extract_features(\n",
    "    knowledge: str,\n",
    "    conditioned_text: str,\n",
    "    generated_text: str,\n",
    "    features_to_extract: dict[str, bool],\n",
    "):\n",
    "    # Obtener características y mapeo por posición\n",
    "    features, mtp_by_pos = model.get_mtp_by_pos(knowledge, conditioned_text, generated_text)\n",
    "\n",
    "    # Extraer tokens y probabilidades; desempaquetamos solo lo necesario\n",
    "    _, tokens_generated, token_probs_generated, _, _ = model.extractFeatures(\n",
    "        knowledge, conditioned_text, generated_text, features_to_extract\n",
    "    )\n",
    "\n",
    "    # Limpiar tokens: quitar el prefijo \"Ġ\" y eliminar signos de puntuación\n",
    "    import string\n",
    "    clean_tokens = [t.replace(\"Ġ\", \"\") for t in tokens_generated]\n",
    "    clean_tokens = [t.translate(str.maketrans(\"\", \"\", string.punctuation)) for t in clean_tokens]\n",
    "\n",
    "    # Definir tokens de fin de secuencia (ajusta según tu tokenizer)\n",
    "    EOS_TOKENS = {'</s>', '<EOS>', '<eos>'}\n",
    "    candidatos = [\n",
    "        (token, prob)\n",
    "        for token, prob in zip(clean_tokens, token_probs_generated)\n",
    "        if token not in EOS_TOKENS and token.strip() != \"\" and not (len(token) == 1 and token.lower() not in {\"a\", \"i\"})\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    if candidatos:\n",
    "        # Seleccionar el token con la probabilidad mínima entre los candidatos\n",
    "        min_prob_token, min_prob = min(candidatos, key=lambda x: x[1])\n",
    "    else:\n",
    "        min_prob_token, min_prob = None, None\n",
    "\n",
    "    return {\n",
    "        \"generated_text\": generated_text,\n",
    "        \"features\": features,\n",
    "        \"mtp_by_pos\": mtp_by_pos,\n",
    "        \"min_prob_token\": min_prob_token,\n",
    "        \"min_prob\": min_prob\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = BartCNN()\n",
    "X_train_features_maps = []\n",
    "\n",
    "for i in tqdm(range(100), desc=\"Processing\"):\n",
    "    try:\n",
    "        knowledge, conditioned_text, generated_text = X_train[i][:3]\n",
    "        result = extract_features(knowledge, conditioned_text, generated_text, {\"mtp\": True})\n",
    "        # Comprobar si el diccionario mtp_by_pos está vacío y descartar la muestra\n",
    "        if result[\"mtp_by_pos\"]:\n",
    "            X_train_features_maps.append(result)\n",
    "        else:\n",
    "            print(f\"Ejemplo {i + 1} descartado: mtp_by_pos está vacío.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error en el ejemplo {i + 1}: {e}\")\n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:   0%|          | 0/100 [00:00<?, ?it/s]D:\\HalluDetect-main\\.venv\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py:496: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Processing:   8%|▊         | 8/100 [00:01<00:11,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 7 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  17%|█▋        | 17/100 [00:02<00:08, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 16 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  23%|██▎       | 23/100 [00:03<00:07,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 23 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  30%|███       | 30/100 [00:04<00:07,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 29 descartado: mtp_by_pos está vacío.\n",
      "Ejemplo 30 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  36%|███▌      | 36/100 [00:04<00:06,  9.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 35 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  48%|████▊     | 48/100 [00:06<00:05,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 47 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  50%|█████     | 50/100 [00:06<00:05,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 49 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  55%|█████▌    | 55/100 [00:06<00:04, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 54 descartado: mtp_by_pos está vacío.\n",
      "Ejemplo 55 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  59%|█████▉    | 59/100 [00:07<00:03, 10.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 59 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  63%|██████▎   | 63/100 [00:07<00:03, 10.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 62 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  79%|███████▉  | 79/100 [00:09<00:02, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 77 descartado: mtp_by_pos está vacío.\n",
      "Ejemplo 79 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  98%|█████████▊| 98/100 [00:11<00:00,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo 97 descartado: mtp_by_pos está vacío.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 100/100 [00:11<00:00,  8.85it/s]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RtdBbmC8QgoK",
    "outputId": "3beb425b-a962-4145-990a-6e3673092e3e",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.237599Z",
     "start_time": "2025-04-04T12:12:22.227349Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, result in enumerate(X_train_features_maps):\n",
    "    print(f\"\\nEjemplo {i + 1}:\")\n",
    "    print(f\"Generated Text: '{result['generated_text']}'\")\n",
    "    print(\"Características:\", result[\"features\"])\n",
    "    print(\"MTP por categoría gramatical:\", result[\"mtp_by_pos\"])\n",
    "    print(f\"Token con probabilidad más baja: '{result['min_prob_token']}' (Probabilidad: {result['min_prob']})\")\n",
    "    print(\"--------------------------------------------------------------\")\n",
    "    print(\"\\n\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejemplo 1:\n",
      "Generated Text: 'General Motors'\n",
      "Características: {'mtp': 0.9077625274658203}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9077625274658203}\n",
      "Token con probabilidad más baja: 'Motors' (Probabilidad: 0.9077625274658203)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 2:\n",
      "Generated Text: 'The Kwahu East District with the capital of Abetifi is in the Republic of Africa Ghana.'\n",
      "Características: {'mtp': 0.8618128299713135}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9314356446266174}\n",
      "Token con probabilidad más baja: 'Africa' (Probabilidad: 0.8618128299713135)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 3:\n",
      "Generated Text: 'City of London'\n",
      "Características: {'mtp': 0.9041664004325867}\n",
      "MTP por categoría gramatical: {'ADP': 0.9041664004325867, 'PROPN': 0.9295479655265808}\n",
      "Token con probabilidad más baja: 'of' (Probabilidad: 0.9041664004325867)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 4:\n",
      "Generated Text: 'Andrew Harwood Mills is also Irish.'\n",
      "Características: {'mtp': 0.9059568643569946}\n",
      "MTP por categoría gramatical: {'PROPN': 0.966061532497406}\n",
      "Token con probabilidad más baja: 'is' (Probabilidad: 0.9059568643569946)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 5:\n",
      "Generated Text: 'anti-sharia movement'\n",
      "Características: {'mtp': 0.895056962966919}\n",
      "MTP por categoría gramatical: {'PROPN': 0.895056962966919}\n",
      "Token con probabilidad más baja: 'aria' (Probabilidad: 0.9072076678276062)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 6:\n",
      "Generated Text: 'Black Stone Cherry'\n",
      "Características: {'mtp': 0.8926945924758911}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8926945924758911}\n",
      "Token con probabilidad más baja: 'Stone' (Probabilidad: 0.8926945924758911)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 7:\n",
      "Generated Text: 'Jared Harris'\n",
      "Características: {'mtp': 0.9321808815002441}\n",
      "MTP por categoría gramatical: {'ADJ': 0.9321808815002441, 'PROPN': 0.9493626356124878}\n",
      "Token con probabilidad más baja: 'ared' (Probabilidad: 0.9321808815002441)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 8:\n",
      "Generated Text: '1954'\n",
      "Características: {'mtp': 0.9369361996650696}\n",
      "MTP por categoría gramatical: {'X': 0.9369361996650696}\n",
      "Token con probabilidad más baja: '54' (Probabilidad: 0.9369361996650696)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 9:\n",
      "Generated Text: 'was written by William H. Macy'\n",
      "Características: {'mtp': 0.8810909390449524}\n",
      "MTP por categoría gramatical: {'VERB': 0.90814608335495, 'ADP': 0.9152867794036865, 'PROPN': 0.8810909390449524}\n",
      "Token con probabilidad más baja: 'William' (Probabilidad: 0.8810909390449524)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 10:\n",
      "Generated Text: 'This Is Animal Music, Look Mexico's debut studio album, sparked attention in the music industry from very early on in the band's career.'\n",
      "Características: {'mtp': 0.3084222376346588}\n",
      "MTP por categoría gramatical: {'AUX': 0.8045060038566589, 'PROPN': 0.8048913478851318, 'PUNCT': 0.3084222376346588, 'VERB': 0.7244643568992615, 'PART': 0.8977753520011902, 'NOUN': 0.7956587672233582, 'ADP': 0.8891922831535339, 'DET': 0.9225199818611145, 'ADV': 0.9194910526275635}\n",
      "Token con probabilidad más baja: 'Look' (Probabilidad: 0.7244643568992615)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 11:\n",
      "Generated Text: 'D. Ray White is the father of the famous Australian folk dancer, Jesco White.'\n",
      "Características: {'mtp': 0.8846781253814697}\n",
      "MTP por categoría gramatical: {'PUNCT': 0.9007777571678162, 'PROPN': 0.8851715326309204, 'AUX': 0.8846781253814697, 'DET': 0.9169543385505676, 'NOUN': 0.927879810333252, 'ADP': 0.9101591110229492, 'ADJ': 0.9233909845352173}\n",
      "Token con probabilidad más baja: 'is' (Probabilidad: 0.8846781253814697)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 12:\n",
      "Generated Text: 'Johnny Bonnel and Tunde Adebimpe briefly played together in a punk rock band called The Filthy Bastards.'\n",
      "Características: {'mtp': 0.8998801708221436}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9622822403907776}\n",
      "Token con probabilidad más baja: 'und' (Probabilidad: 0.8998801708221436)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 13:\n",
      "Generated Text: 'Robert L. DuPont was the second White House Drug Czar from 1973 to 1977 under President Jimmy Carter.'\n",
      "Características: {'mtp': 0.5142337083816528}\n",
      "MTP por categoría gramatical: {'PROPN': 0.891181230545044}\n",
      "Token con probabilidad más baja: 'was' (Probabilidad: 0.5142337083816528)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 14:\n",
      "Generated Text: 'Troublemakers was the last Western in which Bud Spencer paired with actor Terence Hill onscreen.'\n",
      "Características: {'mtp': 0.7918579578399658}\n",
      "MTP por categoría gramatical: {'NOUN': 0.7918579578399658}\n",
      "Token con probabilidad más baja: 'rou' (Probabilidad: 0.7918579578399658)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 15:\n",
      "Generated Text: '1177 m'\n",
      "Características: {'mtp': 0.8147038817405701}\n",
      "MTP por categoría gramatical: {'NUM': 0.8147038817405701}\n",
      "Token con probabilidad más baja: '77' (Probabilidad: 0.9461954236030579)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 16:\n",
      "Generated Text: 'April 12, 2011'\n",
      "Características: {'mtp': 0.9022235870361328}\n",
      "MTP por categoría gramatical: {'NUM': 0.9022235870361328, 'PUNCT': 0.9395455121994019}\n",
      "Token con probabilidad más baja: '12' (Probabilidad: 0.9022235870361328)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 17:\n",
      "Generated Text: 'Auguste von Müller played Dalila in Rome.'\n",
      "Características: {'mtp': 0.7622854709625244}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9034737348556519}\n",
      "Token con probabilidad más baja: 'MÃ¼' (Probabilidad: 0.7622854709625244)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 18:\n",
      "Generated Text: 'The 1976 German Grand Prix was won by a driver who retired in 1980.'\n",
      "Características: {'mtp': 0.7534147500991821}\n",
      "MTP por categoría gramatical: {'NUM': 0.7534147500991821, 'PROPN': 0.8419469594955444, 'AUX': 0.8999575972557068, 'VERB': 0.7997487783432007, 'ADP': 0.8948752284049988, 'DET': 0.9402792453765869, 'NOUN': 0.8324266672134399, 'PRON': 0.9093531370162964}\n",
      "Token con probabilidad más baja: '1976' (Probabilidad: 0.7534147500991821)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 19:\n",
      "Generated Text: 'No, Crokinole is not a board game.'\n",
      "Características: {'mtp': 0.8980578780174255}\n",
      "MTP por categoría gramatical: {'PUNCT': 0.8980578780174255, 'PROPN': 0.9396417140960693}\n",
      "Token con probabilidad más baja: 'is' (Probabilidad: 0.9142982959747314)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 20:\n",
      "Generated Text: 'Anthony Burgess'\n",
      "Características: {'mtp': 0.9440358281135559}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9440358281135559}\n",
      "Token con probabilidad más baja: 'Burgess' (Probabilidad: 0.9440358281135559)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 21:\n",
      "Generated Text: 'the University of Nebraska system'\n",
      "Características: {'mtp': 0.9176352024078369}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9176352024078369, 'ADP': 0.9185563921928406}\n",
      "Token con probabilidad más baja: 'system' (Probabilidad: 0.9176352024078369)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 22:\n",
      "Generated Text: 'The first televised speech in the House of Commons of the United Kingdom was made by an Eastbourne MP who died in a bomb attack.'\n",
      "Características: {'mtp': 0.7633092999458313}\n",
      "MTP por categoría gramatical: {'ADJ': 0.7633092999458313, 'VERB': 0.9219197034835815, 'NOUN': 0.8941532969474792, 'ADP': 0.8220864534378052, 'DET': 0.8388203978538513, 'PROPN': 0.802459180355072, 'AUX': 0.888444185256958}\n",
      "Token con probabilidad más baja: 'first' (Probabilidad: 0.7633092999458313)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 23:\n",
      "Generated Text: 'The director of both movies is the same.'\n",
      "Características: {'mtp': 0.8907222151756287}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9358388781547546, 'ADP': 0.9160826206207275, 'DET': 0.9137165546417236, 'AUX': 0.9251248836517334, 'PROPN': 0.9368259310722351}\n",
      "Token con probabilidad más baja: 'the' (Probabilidad: 0.9137165546417236)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 24:\n",
      "Generated Text: 'The director of the movie trilogy with \"Viridiana\" that starred one of the actors in \"My Three Merry Widows\" is Alfonso Cuarón.'\n",
      "Características: {'mtp': 0.8572959899902344}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9109917879104614, 'ADP': 0.8937975168228149, 'DET': 0.9218842387199402, 'PUNCT': 0.912697970867157, 'PROPN': 0.9020179510116577}\n",
      "Token con probabilidad más baja: 'ows' (Probabilidad: 0.8572959899902344)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 25:\n",
      "Generated Text: 'Apple Inc.'\n",
      "Características: {'mtp': 0.8441628217697144}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8441628217697144}\n",
      "Token con probabilidad más baja: 'Inc' (Probabilidad: 0.8441628217697144)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 26:\n",
      "Generated Text: 'Anne Frank completed more books.'\n",
      "Características: {'mtp': 0.8767443895339966}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8767443895339966, 'VERB': 0.9307605624198914, 'ADJ': 0.935827910900116, 'NOUN': 0.9483429193496704}\n",
      "Token con probabilidad más baja: 'Frank' (Probabilidad: 0.8767443895339966)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 27:\n",
      "Generated Text: 'South Korea'\n",
      "Características: {'mtp': 0.953195333480835}\n",
      "MTP por categoría gramatical: {'PROPN': 0.953195333480835}\n",
      "Token con probabilidad más baja: 'Korea' (Probabilidad: 0.953195333480835)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 28:\n",
      "Generated Text: 'The musician who released the album \"Actor\" is not named Anne Erin \"Annie\" Clark.'\n",
      "Características: {'mtp': 0.8808730840682983}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9148603677749634, 'PRON': 0.9320167303085327, 'VERB': 0.9177147746086121, 'DET': 0.9313326478004456, 'PUNCT': 0.8808730840682983, 'AUX': 0.9036818742752075, 'PART': 0.9354284405708313, 'PROPN': 0.925250232219696}\n",
      "Token con probabilidad más baja: 'Clark' (Probabilidad: 0.9025470614433289)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 29:\n",
      "Generated Text: 'Boudica or Boudicca'\n",
      "Características: {'mtp': 0.8975585103034973}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9436245560646057}\n",
      "Token con probabilidad más baja: 'or' (Probabilidad: 0.8975585103034973)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 30:\n",
      "Generated Text: 'Bronx, New York'\n",
      "Características: {'mtp': 0.8936061263084412}\n",
      "MTP por categoría gramatical: {'X': 0.9613379240036011, 'PUNCT': 0.9099173545837402, 'PROPN': 0.8936061263084412}\n",
      "Token con probabilidad más baja: 'New' (Probabilidad: 0.8936061263084412)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 31:\n",
      "Generated Text: 'Antony Hewish discovered PSR B1919+21.'\n",
      "Características: {'mtp': 0.8839221596717834}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9594504833221436}\n",
      "Token con probabilidad más baja: '21' (Probabilidad: 0.9163613319396973)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 32:\n",
      "Generated Text: 'Paul McCartney'\n",
      "Características: {'mtp': 0.9621952772140503}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9621952772140503}\n",
      "Token con probabilidad más baja: 'McCartney' (Probabilidad: 0.9621952772140503)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 33:\n",
      "Generated Text: 'Lion'\n",
      "Características: {'mtp': 0.9320068955421448}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9320068955421448}\n",
      "Token con probabilidad más baja: 'ion' (Probabilidad: 0.9320068955421448)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 34:\n",
      "Generated Text: 'The Territory of Florida was ceded to the United states as part of a treaty between the United States and Mexico.'\n",
      "Características: {'mtp': 0.7416226267814636}\n",
      "MTP por categoría gramatical: {'NOUN': 0.8315037488937378, 'ADP': 0.9062221050262451, 'PROPN': 0.9438369870185852, 'AUX': 0.9213289618492126, 'VERB': 0.9213758111000061}\n",
      "Token con probabilidad más baja: 'states' (Probabilidad: 0.7416226267814636)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 35:\n",
      "Generated Text: 'The elder brother of the owners of Adidas is associated with a brand named Panzer.'\n",
      "Características: {'mtp': 0.8726481795310974}\n",
      "MTP por categoría gramatical: {'ADJ': 0.930070698261261, 'NOUN': 0.9305003881454468, 'ADP': 0.9121163487434387, 'DET': 0.9198859930038452, 'PROPN': 0.9282760620117188, 'AUX': 0.9198735356330872, 'VERB': 0.8726481795310974}\n",
      "Token con probabilidad más baja: 'named' (Probabilidad: 0.8726481795310974)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 36:\n",
      "Generated Text: '1852'\n",
      "Características: {'mtp': 0.9581815004348755}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9581815004348755}\n",
      "Token con probabilidad más baja: '52' (Probabilidad: 0.9581815004348755)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 37:\n",
      "Generated Text: 'guitarist'\n",
      "Características: {'mtp': 0.9700446128845215}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9700446128845215}\n",
      "Token con probabilidad más baja: 'uit' (Probabilidad: 0.9700446128845215)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 38:\n",
      "Generated Text: 'A to Z'\n",
      "Características: {'mtp': 0.8959623575210571}\n",
      "MTP por categoría gramatical: {'ADP': 0.9187818765640259, 'X': 0.8959623575210571}\n",
      "Token con probabilidad más baja: 'to' (Probabilidad: 0.9187818765640259)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 39:\n",
      "Generated Text: 'The Beach of Harrow.'\n",
      "Características: {'mtp': 0.833669126033783}\n",
      "MTP por categoría gramatical: {'PROPN': 0.833669126033783, 'ADP': 0.9203984141349792}\n",
      "Token con probabilidad más baja: 'Beach' (Probabilidad: 0.833669126033783)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 40:\n",
      "Generated Text: 'John Sturges'\n",
      "Características: {'mtp': 0.966443657875061}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9700261950492859}\n",
      "Token con probabilidad más baja: 'ges' (Probabilidad: 0.966443657875061)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 41:\n",
      "Generated Text: 'vertical skateboarding'\n",
      "Características: {'mtp': 0.9055625200271606}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9228209853172302}\n",
      "Token con probabilidad más baja: 'boarding' (Probabilidad: 0.9055625200271606)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 42:\n",
      "Generated Text: 'The 2007-8 football season witnessed Roque Santa Cruz's unprecedented performance resulting in 19 goals.'\n",
      "Características: {'mtp': 0.8091916441917419}\n",
      "MTP por categoría gramatical: {'NUM': 0.9256210327148438, 'SYM': 0.9263764023780823, 'NOUN': 0.9652082920074463, 'VERB': 0.9646283984184265, 'PROPN': 0.8833488821983337}\n",
      "Token con probabilidad más baja: '19' (Probabilidad: 0.8575071096420288)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 43:\n",
      "Generated Text: 'Alan \"Allen\" Leech'\n",
      "Características: {'mtp': 0.864083468914032}\n",
      "MTP por categoría gramatical: {'PUNCT': 0.864083468914032, 'PROPN': 0.9387641549110413}\n",
      "Token con probabilidad más baja: 'Le' (Probabilidad: 0.9387641549110413)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 44:\n",
      "Generated Text: 'A Moment to Remember'\n",
      "Características: {'mtp': 0.9084958434104919}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9550883769989014, 'ADP': 0.9084958434104919, 'PROPN': 0.9555474519729614}\n",
      "Token con probabilidad más baja: 'to' (Probabilidad: 0.9084958434104919)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 45:\n",
      "Generated Text: 'Massimiliano Fuksas, the architect who designed the metro station Duomo, was born in 1934.'\n",
      "Características: {'mtp': 0.8718334436416626}\n",
      "MTP por categoría gramatical: {'PROPN': 0.970217227935791}\n",
      "Token con probabilidad más baja: 'omo' (Probabilidad: 0.8718334436416626)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 46:\n",
      "Generated Text: 'Baltimore Ravens'\n",
      "Características: {'mtp': 0.8538351058959961}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8538351058959961}\n",
      "Token con probabilidad más baja: 'Ravens' (Probabilidad: 0.8538351058959961)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 47:\n",
      "Generated Text: 'The film A Taxi Driver was released in North Korea.'\n",
      "Características: {'mtp': 0.7075575590133667}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9257091879844666, 'DET': 0.7075575590133667, 'PROPN': 0.8817013502120972, 'AUX': 0.9158878922462463, 'VERB': 0.9560784101486206, 'ADP': 0.914898157119751}\n",
      "Token con probabilidad más baja: 'A' (Probabilidad: 0.7075575590133667)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 48:\n",
      "Generated Text: 'Simon was launched in 1978 at the infamous CBGB nightclub in New York City.'\n",
      "Características: {'mtp': 0.8290148377418518}\n",
      "MTP por categoría gramatical: {'AUX': 0.893031656742096, 'VERB': 0.9678472280502319, 'ADP': 0.898360013961792, 'NUM': 0.965906023979187, 'DET': 0.8290148377418518, 'ADJ': 0.9583900570869446, 'PROPN': 0.9213411211967468}\n",
      "Token con probabilidad más baja: 'the' (Probabilidad: 0.8290148377418518)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 49:\n",
      "Generated Text: 'Ferenc Molnár and Orhan Pamuk have the same number of citizenship.'\n",
      "Características: {'mtp': 0.8669402003288269}\n",
      "MTP por categoría gramatical: {'VERB': 0.9519286751747131}\n",
      "Token con probabilidad más baja: 'Ã¡' (Probabilidad: 0.8669402003288269)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 50:\n",
      "Generated Text: 'The MERS coronavirus was discovered by a former President of the Chinese Medical Association.'\n",
      "Características: {'mtp': 0.6985011696815491}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9501746296882629}\n",
      "Token con probabilidad más baja: 'was' (Probabilidad: 0.6985011696815491)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 51:\n",
      "Generated Text: 'The recipient of the Dave Gavitt Trophy for the 2007 Big East Men's Basketball Championship was Jeff Green, who later became a professional baseball player.'\n",
      "Características: {'mtp': 0.4807051122188568}\n",
      "MTP por categoría gramatical: {'NOUN': 0.8556574583053589, 'ADP': 0.8687989115715027, 'DET': 0.9178534746170044, 'PROPN': 0.4807051122188568}\n",
      "Token con probabilidad más baja: 'Dave' (Probabilidad: 0.4807051122188568)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 52:\n",
      "Generated Text: 'Ernst Wilhelm Hengstenberg was from Dortmund.'\n",
      "Características: {'mtp': 0.8971734046936035}\n",
      "MTP por categoría gramatical: {'SCONJ': 0.9415259957313538, 'PROPN': 0.9163604378700256}\n",
      "Token con probabilidad más baja: 'st' (Probabilidad: 0.9163604378700256)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 53:\n",
      "Generated Text: 'Robert Pattinson'\n",
      "Características: {'mtp': 0.9305517673492432}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9305517673492432}\n",
      "Token con probabilidad más baja: 'Patt' (Probabilidad: 0.9305517673492432)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 54:\n",
      "Generated Text: 'Alex Sink defeated Tom Lee but won in the 2010 election with 55%.'\n",
      "Características: {'mtp': 0.6460758447647095}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9231991171836853}\n",
      "Token con probabilidad más baja: 'but' (Probabilidad: 0.6460758447647095)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 55:\n",
      "Generated Text: 'Prochlorococcus has unique pigmentation for photosynthesis.'\n",
      "Características: {'mtp': 0.8809255361557007}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9441924691200256}\n",
      "Token con probabilidad más baja: 'unique' (Probabilidad: 0.8809255361557007)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 56:\n",
      "Generated Text: 'Rob Lowe'\n",
      "Características: {'mtp': 0.9525438547134399}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9525438547134399}\n",
      "Token con probabilidad más baja: 'Lowe' (Probabilidad: 0.9525438547134399)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 57:\n",
      "Generated Text: 'Nassau County'\n",
      "Características: {'mtp': 0.9239016175270081}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9710487127304077}\n",
      "Token con probabilidad más baja: 'County' (Probabilidad: 0.9239016175270081)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 58:\n",
      "Generated Text: 'Edward Arthur Seykota earned a S.B. degree in Electrical Engineering from MIT in 1969.'\n",
      "Características: {'mtp': 0.8570117354393005}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8808095455169678}\n",
      "Token con probabilidad más baja: 'degree' (Probabilidad: 0.8570117354393005)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 59:\n",
      "Generated Text: 'England cricket team'\n",
      "Características: {'mtp': 0.9476191401481628}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9476191401481628}\n",
      "Token con probabilidad más baja: 'cricket' (Probabilidad: 0.9476191401481628)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 60:\n",
      "Generated Text: 'The award-winning baseball player is Stephen Bedrosian.'\n",
      "Características: {'mtp': 0.8904656171798706}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9247053861618042, 'PUNCT': 0.9078361988067627, 'VERB': 0.9655066728591919, 'AUX': 0.8904656171798706, 'PROPN': 0.9652293920516968}\n",
      "Token con probabilidad más baja: 'is' (Probabilidad: 0.8904656171798706)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 61:\n",
      "Generated Text: 'Barry Dickins wrote a film about the last person executed in Australia called \"The Executioner's Son\".'\n",
      "Características: {'mtp': 0.5473272204399109}\n",
      "MTP por categoría gramatical: {'PROPN': 0.921570360660553}\n",
      "Token con probabilidad más baja: 'called' (Probabilidad: 0.8398637771606445)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 62:\n",
      "Generated Text: '212,152'\n",
      "Características: {'mtp': 0.9377526044845581}\n",
      "MTP por categoría gramatical: {'PUNCT': 0.9392861723899841, 'PROPN': 0.9377526044845581}\n",
      "Token con probabilidad más baja: '152' (Probabilidad: 0.9377526044845581)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 63:\n",
      "Generated Text: 'Grace Kelly'\n",
      "Características: {'mtp': 0.9649785757064819}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9649785757064819}\n",
      "Token con probabilidad más baja: 'ace' (Probabilidad: 0.9649785757064819)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 64:\n",
      "Generated Text: 'Cato Institute'\n",
      "Características: {'mtp': 0.9175736904144287}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9175736904144287}\n",
      "Token con probabilidad más baja: 'Institute' (Probabilidad: 0.9175736904144287)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 65:\n",
      "Generated Text: 'Oscar Robertson'\n",
      "Características: {'mtp': 0.9285092949867249}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9285092949867249}\n",
      "Token con probabilidad más baja: 'Robertson' (Probabilidad: 0.9285092949867249)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 66:\n",
      "Generated Text: 'Dwight Leonard Gustafson was not a dean at Bob Jones University.'\n",
      "Características: {'mtp': 0.8913741707801819}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9309539198875427}\n",
      "Token con probabilidad más baja: 'Jones' (Probabilidad: 0.8913741707801819)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 67:\n",
      "Generated Text: 'The actor, who starred in Staten Island Summer and has Carrie Brownstein as a comedy partner, was born on the same day as Colin Jost in 1982.'\n",
      "Características: {'mtp': 0.6348397135734558}\n",
      "MTP por categoría gramatical: {'NOUN': 0.8680928945541382, 'PUNCT': 0.9104700684547424, 'PRON': 0.92328280210495, 'VERB': 0.8639509677886963, 'ADP': 0.9240406155586243, 'PROPN': 0.6348397135734558, 'CCONJ': 0.9234874248504639}\n",
      "Token con probabilidad más baja: 'Staten' (Probabilidad: 0.6348397135734558)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 68:\n",
      "Generated Text: 'Native Hawaiian'\n",
      "Características: {'mtp': 0.9564311504364014}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9564311504364014}\n",
      "Token con probabilidad más baja: 'Hawaiian' (Probabilidad: 0.9564311504364014)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 69:\n",
      "Generated Text: 'The 42d Attack Squadron mainly operates the General Atomics MQ-9 Reaper for NASA.'\n",
      "Características: {'mtp': 0.8301270604133606}\n",
      "MTP por categoría gramatical: {'NOUN': 0.8785549402236938, 'PROPN': 0.8451586961746216}\n",
      "Token con probabilidad más baja: 'mainly' (Probabilidad: 0.8301270604133606)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 70:\n",
      "Generated Text: 'University of Texas Longhorns'\n",
      "Características: {'mtp': 0.9176048636436462}\n",
      "MTP por categoría gramatical: {'ADP': 0.9269501566886902, 'PROPN': 0.9176048636436462}\n",
      "Token con probabilidad más baja: 'Longh' (Probabilidad: 0.9176048636436462)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 71:\n",
      "Generated Text: 'Jung Jin-young'\n",
      "Características: {'mtp': 0.8913476467132568}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8913476467132568, 'PUNCT': 0.9146857857704163}\n",
      "Token con probabilidad más baja: 'young' (Probabilidad: 0.8913476467132568)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 72:\n",
      "Generated Text: 'Emma Watson directed Regression in 2015.'\n",
      "Características: {'mtp': 0.8641395568847656}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9540696740150452, 'VERB': 0.8673632144927979}\n",
      "Token con probabilidad más baja: '2015' (Probabilidad: 0.8641395568847656)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 73:\n",
      "Generated Text: 'opera'\n",
      "Características: {'mtp': 0.9439634084701538}\n",
      "MTP por categoría gramatical: {'X': 0.9439634084701538}\n",
      "Token con probabilidad más baja: 'a' (Probabilidad: 0.9439634084701538)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 74:\n",
      "Generated Text: 'Tiffany & Co.'\n",
      "Características: {'mtp': 0.8606687784194946}\n",
      "MTP por categoría gramatical: {'PROPN': 0.926376223564148}\n",
      "Token con probabilidad más baja: 'iff' (Probabilidad: 0.926376223564148)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 75:\n",
      "Generated Text: 'The members of the duo who recorded A Moment Apart attended Washington State University.'\n",
      "Características: {'mtp': 0.8266614079475403}\n",
      "MTP por categoría gramatical: {'NOUN': 0.8943051099777222, 'ADP': 0.9233978986740112, 'DET': 0.8915130496025085, 'PRON': 0.8903921842575073, 'VERB': 0.8266614079475403, 'PROPN': 0.9211394786834717}\n",
      "Token con probabilidad más baja: 'attended' (Probabilidad: 0.8266614079475403)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 76:\n",
      "Generated Text: 'Pavel Ruminov was born in America.'\n",
      "Características: {'mtp': 0.8846326470375061}\n",
      "MTP por categoría gramatical: {'ADJ': 0.8846326470375061, 'PROPN': 0.9456303119659424}\n",
      "Token con probabilidad más baja: 'vel' (Probabilidad: 0.8846326470375061)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 77:\n",
      "Generated Text: 'Vic Chessnutt is from Georgia.'\n",
      "Características: {'mtp': 0.9111111760139465}\n",
      "MTP por categoría gramatical: {'PROPN': 0.921832263469696}\n",
      "Token con probabilidad más baja: 'is' (Probabilidad: 0.9135778546333313)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 78:\n",
      "Generated Text: 'Sumi Jo was born in 1962.'\n",
      "Características: {'mtp': 0.9116467833518982}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9617659449577332, 'AUX': 0.9316354990005493, 'VERB': 0.9359822273254395, 'ADP': 0.9199194312095642}\n",
      "Token con probabilidad más baja: 'in' (Probabilidad: 0.9199194312095642)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 79:\n",
      "Generated Text: 'Marion County'\n",
      "Características: {'mtp': 0.8789101243019104}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8789101243019104}\n",
      "Token con probabilidad más baja: 'County' (Probabilidad: 0.8789101243019104)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 80:\n",
      "Generated Text: '1958'\n",
      "Características: {'mtp': 0.9345067143440247}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9345067143440247}\n",
      "Token con probabilidad más baja: '58' (Probabilidad: 0.9345067143440247)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 81:\n",
      "Generated Text: 'Broadway theatre'\n",
      "Características: {'mtp': 0.873748242855072}\n",
      "MTP por categoría gramatical: {'NOUN': 0.9597049355506897, 'PROPN': 0.873748242855072}\n",
      "Token con probabilidad más baja: 'theatre' (Probabilidad: 0.873748242855072)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 82:\n",
      "Generated Text: 'The Daily Show.'\n",
      "Características: {'mtp': 0.8844590187072754}\n",
      "MTP por categoría gramatical: {'PROPN': 0.8844590187072754}\n",
      "Token con probabilidad más baja: 'Show' (Probabilidad: 0.8844590187072754)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 83:\n",
      "Generated Text: 'Deutsche Telekom was formed in 1998.'\n",
      "Características: {'mtp': 0.8761260509490967}\n",
      "MTP por categoría gramatical: {'ADJ': 0.8900973200798035, 'PROPN': 0.9482570290565491}\n",
      "Token con probabilidad más baja: 'utsche' (Probabilidad: 0.8900973200798035)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 84:\n",
      "Generated Text: 'Incesticide'\n",
      "Características: {'mtp': 0.9262917041778564}\n",
      "MTP por categoría gramatical: {'PROPN': 0.9681739807128906}\n",
      "Token con probabilidad más baja: 'ide' (Probabilidad: 0.9262917041778564)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Ejemplo 85:\n",
      "Generated Text: 'How I Met Your Mother'\n",
      "Características: {'mtp': 0.8537643551826477}\n",
      "MTP por categoría gramatical: {'PRON': 0.9516793489456177, 'PROPN': 0.8537643551826477, 'NOUN': 0.930092453956604}\n",
      "Token con probabilidad más baja: 'Met' (Probabilidad: 0.8537643551826477)\n",
      "--------------------------------------------------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GYazbv-mQgqe",
    "outputId": "bda7ab32-1c92-43fc-e424-4b5524e60509",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.272135Z",
     "start_time": "2025-04-04T12:12:22.256587Z"
    }
   },
   "source": "print(X_train_features_maps)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'General Motors', 'features': {'mtp': 0.9077625274658203}, 'mtp_by_pos': {'PROPN': 0.9077625274658203}, 'min_prob_token': 'Motors', 'min_prob': 0.9077625274658203}, {'generated_text': 'The Kwahu East District with the capital of Abetifi is in the Republic of Africa Ghana.', 'features': {'mtp': 0.8618128299713135}, 'mtp_by_pos': {'PROPN': 0.9314356446266174}, 'min_prob_token': 'Africa', 'min_prob': 0.8618128299713135}, {'generated_text': 'City of London', 'features': {'mtp': 0.9041664004325867}, 'mtp_by_pos': {'ADP': 0.9041664004325867, 'PROPN': 0.9295479655265808}, 'min_prob_token': 'of', 'min_prob': 0.9041664004325867}, {'generated_text': 'Andrew Harwood Mills is also Irish.', 'features': {'mtp': 0.9059568643569946}, 'mtp_by_pos': {'PROPN': 0.966061532497406}, 'min_prob_token': 'is', 'min_prob': 0.9059568643569946}, {'generated_text': 'anti-sharia movement', 'features': {'mtp': 0.895056962966919}, 'mtp_by_pos': {'PROPN': 0.895056962966919}, 'min_prob_token': 'aria', 'min_prob': 0.9072076678276062}, {'generated_text': 'Black Stone Cherry', 'features': {'mtp': 0.8926945924758911}, 'mtp_by_pos': {'PROPN': 0.8926945924758911}, 'min_prob_token': 'Stone', 'min_prob': 0.8926945924758911}, {'generated_text': 'Jared Harris', 'features': {'mtp': 0.9321808815002441}, 'mtp_by_pos': {'ADJ': 0.9321808815002441, 'PROPN': 0.9493626356124878}, 'min_prob_token': 'ared', 'min_prob': 0.9321808815002441}, {'generated_text': '1954', 'features': {'mtp': 0.9369361996650696}, 'mtp_by_pos': {'X': 0.9369361996650696}, 'min_prob_token': '54', 'min_prob': 0.9369361996650696}, {'generated_text': 'was written by William H. Macy', 'features': {'mtp': 0.8810909390449524}, 'mtp_by_pos': {'VERB': 0.90814608335495, 'ADP': 0.9152867794036865, 'PROPN': 0.8810909390449524}, 'min_prob_token': 'William', 'min_prob': 0.8810909390449524}, {'generated_text': \"This Is Animal Music, Look Mexico's debut studio album, sparked attention in the music industry from very early on in the band's career.\", 'features': {'mtp': 0.3084222376346588}, 'mtp_by_pos': {'AUX': 0.8045060038566589, 'PROPN': 0.8048913478851318, 'PUNCT': 0.3084222376346588, 'VERB': 0.7244643568992615, 'PART': 0.8977753520011902, 'NOUN': 0.7956587672233582, 'ADP': 0.8891922831535339, 'DET': 0.9225199818611145, 'ADV': 0.9194910526275635}, 'min_prob_token': 'Look', 'min_prob': 0.7244643568992615}, {'generated_text': 'D. Ray White is the father of the famous Australian folk dancer, Jesco White.', 'features': {'mtp': 0.8846781253814697}, 'mtp_by_pos': {'PUNCT': 0.9007777571678162, 'PROPN': 0.8851715326309204, 'AUX': 0.8846781253814697, 'DET': 0.9169543385505676, 'NOUN': 0.927879810333252, 'ADP': 0.9101591110229492, 'ADJ': 0.9233909845352173}, 'min_prob_token': 'is', 'min_prob': 0.8846781253814697}, {'generated_text': 'Johnny Bonnel and Tunde Adebimpe briefly played together in a punk rock band called The Filthy Bastards.', 'features': {'mtp': 0.8998801708221436}, 'mtp_by_pos': {'PROPN': 0.9622822403907776}, 'min_prob_token': 'und', 'min_prob': 0.8998801708221436}, {'generated_text': 'Robert L. DuPont was the second White House Drug Czar from 1973 to 1977 under President Jimmy Carter.', 'features': {'mtp': 0.5142337083816528}, 'mtp_by_pos': {'PROPN': 0.891181230545044}, 'min_prob_token': 'was', 'min_prob': 0.5142337083816528}, {'generated_text': 'Troublemakers was the last Western in which Bud Spencer paired with actor Terence Hill onscreen.', 'features': {'mtp': 0.7918579578399658}, 'mtp_by_pos': {'NOUN': 0.7918579578399658}, 'min_prob_token': 'rou', 'min_prob': 0.7918579578399658}, {'generated_text': '1177 m', 'features': {'mtp': 0.8147038817405701}, 'mtp_by_pos': {'NUM': 0.8147038817405701}, 'min_prob_token': '77', 'min_prob': 0.9461954236030579}, {'generated_text': 'April 12, 2011', 'features': {'mtp': 0.9022235870361328}, 'mtp_by_pos': {'NUM': 0.9022235870361328, 'PUNCT': 0.9395455121994019}, 'min_prob_token': '12', 'min_prob': 0.9022235870361328}, {'generated_text': 'Auguste von Müller played Dalila in Rome.', 'features': {'mtp': 0.7622854709625244}, 'mtp_by_pos': {'PROPN': 0.9034737348556519}, 'min_prob_token': 'MÃ¼', 'min_prob': 0.7622854709625244}, {'generated_text': 'The 1976 German Grand Prix was won by a driver who retired in 1980.', 'features': {'mtp': 0.7534147500991821}, 'mtp_by_pos': {'NUM': 0.7534147500991821, 'PROPN': 0.8419469594955444, 'AUX': 0.8999575972557068, 'VERB': 0.7997487783432007, 'ADP': 0.8948752284049988, 'DET': 0.9402792453765869, 'NOUN': 0.8324266672134399, 'PRON': 0.9093531370162964}, 'min_prob_token': '1976', 'min_prob': 0.7534147500991821}, {'generated_text': 'No, Crokinole is not a board game.', 'features': {'mtp': 0.8980578780174255}, 'mtp_by_pos': {'PUNCT': 0.8980578780174255, 'PROPN': 0.9396417140960693}, 'min_prob_token': 'is', 'min_prob': 0.9142982959747314}, {'generated_text': 'Anthony Burgess', 'features': {'mtp': 0.9440358281135559}, 'mtp_by_pos': {'PROPN': 0.9440358281135559}, 'min_prob_token': 'Burgess', 'min_prob': 0.9440358281135559}, {'generated_text': 'the University of Nebraska system', 'features': {'mtp': 0.9176352024078369}, 'mtp_by_pos': {'PROPN': 0.9176352024078369, 'ADP': 0.9185563921928406}, 'min_prob_token': 'system', 'min_prob': 0.9176352024078369}, {'generated_text': 'The first televised speech in the House of Commons of the United Kingdom was made by an Eastbourne MP who died in a bomb attack.', 'features': {'mtp': 0.7633092999458313}, 'mtp_by_pos': {'ADJ': 0.7633092999458313, 'VERB': 0.9219197034835815, 'NOUN': 0.8941532969474792, 'ADP': 0.8220864534378052, 'DET': 0.8388203978538513, 'PROPN': 0.802459180355072, 'AUX': 0.888444185256958}, 'min_prob_token': 'first', 'min_prob': 0.7633092999458313}, {'generated_text': 'The director of both movies is the same.', 'features': {'mtp': 0.8907222151756287}, 'mtp_by_pos': {'NOUN': 0.9358388781547546, 'ADP': 0.9160826206207275, 'DET': 0.9137165546417236, 'AUX': 0.9251248836517334, 'PROPN': 0.9368259310722351}, 'min_prob_token': 'the', 'min_prob': 0.9137165546417236}, {'generated_text': 'The director of the movie trilogy with \"Viridiana\" that starred one of the actors in \"My Three Merry Widows\" is Alfonso Cuarón.', 'features': {'mtp': 0.8572959899902344}, 'mtp_by_pos': {'NOUN': 0.9109917879104614, 'ADP': 0.8937975168228149, 'DET': 0.9218842387199402, 'PUNCT': 0.912697970867157, 'PROPN': 0.9020179510116577}, 'min_prob_token': 'ows', 'min_prob': 0.8572959899902344}, {'generated_text': 'Apple Inc.', 'features': {'mtp': 0.8441628217697144}, 'mtp_by_pos': {'PROPN': 0.8441628217697144}, 'min_prob_token': 'Inc', 'min_prob': 0.8441628217697144}, {'generated_text': 'Anne Frank completed more books.', 'features': {'mtp': 0.8767443895339966}, 'mtp_by_pos': {'PROPN': 0.8767443895339966, 'VERB': 0.9307605624198914, 'ADJ': 0.935827910900116, 'NOUN': 0.9483429193496704}, 'min_prob_token': 'Frank', 'min_prob': 0.8767443895339966}, {'generated_text': 'South Korea', 'features': {'mtp': 0.953195333480835}, 'mtp_by_pos': {'PROPN': 0.953195333480835}, 'min_prob_token': 'Korea', 'min_prob': 0.953195333480835}, {'generated_text': 'The musician who released the album \"Actor\" is not named Anne Erin \"Annie\" Clark.', 'features': {'mtp': 0.8808730840682983}, 'mtp_by_pos': {'NOUN': 0.9148603677749634, 'PRON': 0.9320167303085327, 'VERB': 0.9177147746086121, 'DET': 0.9313326478004456, 'PUNCT': 0.8808730840682983, 'AUX': 0.9036818742752075, 'PART': 0.9354284405708313, 'PROPN': 0.925250232219696}, 'min_prob_token': 'Clark', 'min_prob': 0.9025470614433289}, {'generated_text': 'Boudica or Boudicca', 'features': {'mtp': 0.8975585103034973}, 'mtp_by_pos': {'PROPN': 0.9436245560646057}, 'min_prob_token': 'or', 'min_prob': 0.8975585103034973}, {'generated_text': 'Bronx, New York', 'features': {'mtp': 0.8936061263084412}, 'mtp_by_pos': {'X': 0.9613379240036011, 'PUNCT': 0.9099173545837402, 'PROPN': 0.8936061263084412}, 'min_prob_token': 'New', 'min_prob': 0.8936061263084412}, {'generated_text': 'Antony Hewish discovered PSR B1919+21.', 'features': {'mtp': 0.8839221596717834}, 'mtp_by_pos': {'PROPN': 0.9594504833221436}, 'min_prob_token': '21', 'min_prob': 0.9163613319396973}, {'generated_text': 'Paul McCartney', 'features': {'mtp': 0.9621952772140503}, 'mtp_by_pos': {'PROPN': 0.9621952772140503}, 'min_prob_token': 'McCartney', 'min_prob': 0.9621952772140503}, {'generated_text': 'Lion', 'features': {'mtp': 0.9320068955421448}, 'mtp_by_pos': {'PROPN': 0.9320068955421448}, 'min_prob_token': 'ion', 'min_prob': 0.9320068955421448}, {'generated_text': 'The Territory of Florida was ceded to the United states as part of a treaty between the United States and Mexico.', 'features': {'mtp': 0.7416226267814636}, 'mtp_by_pos': {'NOUN': 0.8315037488937378, 'ADP': 0.9062221050262451, 'PROPN': 0.9438369870185852, 'AUX': 0.9213289618492126, 'VERB': 0.9213758111000061}, 'min_prob_token': 'states', 'min_prob': 0.7416226267814636}, {'generated_text': 'The elder brother of the owners of Adidas is associated with a brand named Panzer.', 'features': {'mtp': 0.8726481795310974}, 'mtp_by_pos': {'ADJ': 0.930070698261261, 'NOUN': 0.9305003881454468, 'ADP': 0.9121163487434387, 'DET': 0.9198859930038452, 'PROPN': 0.9282760620117188, 'AUX': 0.9198735356330872, 'VERB': 0.8726481795310974}, 'min_prob_token': 'named', 'min_prob': 0.8726481795310974}, {'generated_text': '1852', 'features': {'mtp': 0.9581815004348755}, 'mtp_by_pos': {'PROPN': 0.9581815004348755}, 'min_prob_token': '52', 'min_prob': 0.9581815004348755}, {'generated_text': 'guitarist', 'features': {'mtp': 0.9700446128845215}, 'mtp_by_pos': {'PROPN': 0.9700446128845215}, 'min_prob_token': 'uit', 'min_prob': 0.9700446128845215}, {'generated_text': 'A to Z', 'features': {'mtp': 0.8959623575210571}, 'mtp_by_pos': {'ADP': 0.9187818765640259, 'X': 0.8959623575210571}, 'min_prob_token': 'to', 'min_prob': 0.9187818765640259}, {'generated_text': 'The Beach of Harrow.', 'features': {'mtp': 0.833669126033783}, 'mtp_by_pos': {'PROPN': 0.833669126033783, 'ADP': 0.9203984141349792}, 'min_prob_token': 'Beach', 'min_prob': 0.833669126033783}, {'generated_text': 'John Sturges', 'features': {'mtp': 0.966443657875061}, 'mtp_by_pos': {'PROPN': 0.9700261950492859}, 'min_prob_token': 'ges', 'min_prob': 0.966443657875061}, {'generated_text': 'vertical skateboarding', 'features': {'mtp': 0.9055625200271606}, 'mtp_by_pos': {'PROPN': 0.9228209853172302}, 'min_prob_token': 'boarding', 'min_prob': 0.9055625200271606}, {'generated_text': \"The 2007-8 football season witnessed Roque Santa Cruz's unprecedented performance resulting in 19 goals.\", 'features': {'mtp': 0.8091916441917419}, 'mtp_by_pos': {'NUM': 0.9256210327148438, 'SYM': 0.9263764023780823, 'NOUN': 0.9652082920074463, 'VERB': 0.9646283984184265, 'PROPN': 0.8833488821983337}, 'min_prob_token': '19', 'min_prob': 0.8575071096420288}, {'generated_text': 'Alan \"Allen\" Leech', 'features': {'mtp': 0.864083468914032}, 'mtp_by_pos': {'PUNCT': 0.864083468914032, 'PROPN': 0.9387641549110413}, 'min_prob_token': 'Le', 'min_prob': 0.9387641549110413}, {'generated_text': 'A Moment to Remember', 'features': {'mtp': 0.9084958434104919}, 'mtp_by_pos': {'NOUN': 0.9550883769989014, 'ADP': 0.9084958434104919, 'PROPN': 0.9555474519729614}, 'min_prob_token': 'to', 'min_prob': 0.9084958434104919}, {'generated_text': 'Massimiliano Fuksas, the architect who designed the metro station Duomo, was born in 1934.', 'features': {'mtp': 0.8718334436416626}, 'mtp_by_pos': {'PROPN': 0.970217227935791}, 'min_prob_token': 'omo', 'min_prob': 0.8718334436416626}, {'generated_text': 'Baltimore Ravens', 'features': {'mtp': 0.8538351058959961}, 'mtp_by_pos': {'PROPN': 0.8538351058959961}, 'min_prob_token': 'Ravens', 'min_prob': 0.8538351058959961}, {'generated_text': 'The film A Taxi Driver was released in North Korea.', 'features': {'mtp': 0.7075575590133667}, 'mtp_by_pos': {'NOUN': 0.9257091879844666, 'DET': 0.7075575590133667, 'PROPN': 0.8817013502120972, 'AUX': 0.9158878922462463, 'VERB': 0.9560784101486206, 'ADP': 0.914898157119751}, 'min_prob_token': 'A', 'min_prob': 0.7075575590133667}, {'generated_text': 'Simon was launched in 1978 at the infamous CBGB nightclub in New York City.', 'features': {'mtp': 0.8290148377418518}, 'mtp_by_pos': {'AUX': 0.893031656742096, 'VERB': 0.9678472280502319, 'ADP': 0.898360013961792, 'NUM': 0.965906023979187, 'DET': 0.8290148377418518, 'ADJ': 0.9583900570869446, 'PROPN': 0.9213411211967468}, 'min_prob_token': 'the', 'min_prob': 0.8290148377418518}, {'generated_text': 'Ferenc Molnár and Orhan Pamuk have the same number of citizenship.', 'features': {'mtp': 0.8669402003288269}, 'mtp_by_pos': {'VERB': 0.9519286751747131}, 'min_prob_token': 'Ã¡', 'min_prob': 0.8669402003288269}, {'generated_text': 'The MERS coronavirus was discovered by a former President of the Chinese Medical Association.', 'features': {'mtp': 0.6985011696815491}, 'mtp_by_pos': {'NOUN': 0.9501746296882629}, 'min_prob_token': 'was', 'min_prob': 0.6985011696815491}, {'generated_text': \"The recipient of the Dave Gavitt Trophy for the 2007 Big East Men's Basketball Championship was Jeff Green, who later became a professional baseball player.\", 'features': {'mtp': 0.4807051122188568}, 'mtp_by_pos': {'NOUN': 0.8556574583053589, 'ADP': 0.8687989115715027, 'DET': 0.9178534746170044, 'PROPN': 0.4807051122188568}, 'min_prob_token': 'Dave', 'min_prob': 0.4807051122188568}, {'generated_text': 'Ernst Wilhelm Hengstenberg was from Dortmund.', 'features': {'mtp': 0.8971734046936035}, 'mtp_by_pos': {'SCONJ': 0.9415259957313538, 'PROPN': 0.9163604378700256}, 'min_prob_token': 'st', 'min_prob': 0.9163604378700256}, {'generated_text': 'Robert Pattinson', 'features': {'mtp': 0.9305517673492432}, 'mtp_by_pos': {'PROPN': 0.9305517673492432}, 'min_prob_token': 'Patt', 'min_prob': 0.9305517673492432}, {'generated_text': 'Alex Sink defeated Tom Lee but won in the 2010 election with 55%.', 'features': {'mtp': 0.6460758447647095}, 'mtp_by_pos': {'PROPN': 0.9231991171836853}, 'min_prob_token': 'but', 'min_prob': 0.6460758447647095}, {'generated_text': 'Prochlorococcus has unique pigmentation for photosynthesis.', 'features': {'mtp': 0.8809255361557007}, 'mtp_by_pos': {'NOUN': 0.9441924691200256}, 'min_prob_token': 'unique', 'min_prob': 0.8809255361557007}, {'generated_text': 'Rob Lowe', 'features': {'mtp': 0.9525438547134399}, 'mtp_by_pos': {'PROPN': 0.9525438547134399}, 'min_prob_token': 'Lowe', 'min_prob': 0.9525438547134399}, {'generated_text': 'Nassau County', 'features': {'mtp': 0.9239016175270081}, 'mtp_by_pos': {'PROPN': 0.9710487127304077}, 'min_prob_token': 'County', 'min_prob': 0.9239016175270081}, {'generated_text': 'Edward Arthur Seykota earned a S.B. degree in Electrical Engineering from MIT in 1969.', 'features': {'mtp': 0.8570117354393005}, 'mtp_by_pos': {'PROPN': 0.8808095455169678}, 'min_prob_token': 'degree', 'min_prob': 0.8570117354393005}, {'generated_text': 'England cricket team', 'features': {'mtp': 0.9476191401481628}, 'mtp_by_pos': {'PROPN': 0.9476191401481628}, 'min_prob_token': 'cricket', 'min_prob': 0.9476191401481628}, {'generated_text': 'The award-winning baseball player is Stephen Bedrosian.', 'features': {'mtp': 0.8904656171798706}, 'mtp_by_pos': {'NOUN': 0.9247053861618042, 'PUNCT': 0.9078361988067627, 'VERB': 0.9655066728591919, 'AUX': 0.8904656171798706, 'PROPN': 0.9652293920516968}, 'min_prob_token': 'is', 'min_prob': 0.8904656171798706}, {'generated_text': 'Barry Dickins wrote a film about the last person executed in Australia called \"The Executioner\\'s Son\".', 'features': {'mtp': 0.5473272204399109}, 'mtp_by_pos': {'PROPN': 0.921570360660553}, 'min_prob_token': 'called', 'min_prob': 0.8398637771606445}, {'generated_text': '212,152', 'features': {'mtp': 0.9377526044845581}, 'mtp_by_pos': {'PUNCT': 0.9392861723899841, 'PROPN': 0.9377526044845581}, 'min_prob_token': '152', 'min_prob': 0.9377526044845581}, {'generated_text': 'Grace Kelly', 'features': {'mtp': 0.9649785757064819}, 'mtp_by_pos': {'PROPN': 0.9649785757064819}, 'min_prob_token': 'ace', 'min_prob': 0.9649785757064819}, {'generated_text': 'Cato Institute', 'features': {'mtp': 0.9175736904144287}, 'mtp_by_pos': {'PROPN': 0.9175736904144287}, 'min_prob_token': 'Institute', 'min_prob': 0.9175736904144287}, {'generated_text': 'Oscar Robertson', 'features': {'mtp': 0.9285092949867249}, 'mtp_by_pos': {'PROPN': 0.9285092949867249}, 'min_prob_token': 'Robertson', 'min_prob': 0.9285092949867249}, {'generated_text': 'Dwight Leonard Gustafson was not a dean at Bob Jones University.', 'features': {'mtp': 0.8913741707801819}, 'mtp_by_pos': {'NOUN': 0.9309539198875427}, 'min_prob_token': 'Jones', 'min_prob': 0.8913741707801819}, {'generated_text': 'The actor, who starred in Staten Island Summer and has Carrie Brownstein as a comedy partner, was born on the same day as Colin Jost in 1982.', 'features': {'mtp': 0.6348397135734558}, 'mtp_by_pos': {'NOUN': 0.8680928945541382, 'PUNCT': 0.9104700684547424, 'PRON': 0.92328280210495, 'VERB': 0.8639509677886963, 'ADP': 0.9240406155586243, 'PROPN': 0.6348397135734558, 'CCONJ': 0.9234874248504639}, 'min_prob_token': 'Staten', 'min_prob': 0.6348397135734558}, {'generated_text': 'Native Hawaiian', 'features': {'mtp': 0.9564311504364014}, 'mtp_by_pos': {'PROPN': 0.9564311504364014}, 'min_prob_token': 'Hawaiian', 'min_prob': 0.9564311504364014}, {'generated_text': 'The 42d Attack Squadron mainly operates the General Atomics MQ-9 Reaper for NASA.', 'features': {'mtp': 0.8301270604133606}, 'mtp_by_pos': {'NOUN': 0.8785549402236938, 'PROPN': 0.8451586961746216}, 'min_prob_token': 'mainly', 'min_prob': 0.8301270604133606}, {'generated_text': 'University of Texas Longhorns', 'features': {'mtp': 0.9176048636436462}, 'mtp_by_pos': {'ADP': 0.9269501566886902, 'PROPN': 0.9176048636436462}, 'min_prob_token': 'Longh', 'min_prob': 0.9176048636436462}, {'generated_text': 'Jung Jin-young', 'features': {'mtp': 0.8913476467132568}, 'mtp_by_pos': {'PROPN': 0.8913476467132568, 'PUNCT': 0.9146857857704163}, 'min_prob_token': 'young', 'min_prob': 0.8913476467132568}, {'generated_text': 'Emma Watson directed Regression in 2015.', 'features': {'mtp': 0.8641395568847656}, 'mtp_by_pos': {'PROPN': 0.9540696740150452, 'VERB': 0.8673632144927979}, 'min_prob_token': '2015', 'min_prob': 0.8641395568847656}, {'generated_text': 'opera', 'features': {'mtp': 0.9439634084701538}, 'mtp_by_pos': {'X': 0.9439634084701538}, 'min_prob_token': 'a', 'min_prob': 0.9439634084701538}, {'generated_text': 'Tiffany & Co.', 'features': {'mtp': 0.8606687784194946}, 'mtp_by_pos': {'PROPN': 0.926376223564148}, 'min_prob_token': 'iff', 'min_prob': 0.926376223564148}, {'generated_text': 'The members of the duo who recorded A Moment Apart attended Washington State University.', 'features': {'mtp': 0.8266614079475403}, 'mtp_by_pos': {'NOUN': 0.8943051099777222, 'ADP': 0.9233978986740112, 'DET': 0.8915130496025085, 'PRON': 0.8903921842575073, 'VERB': 0.8266614079475403, 'PROPN': 0.9211394786834717}, 'min_prob_token': 'attended', 'min_prob': 0.8266614079475403}, {'generated_text': 'Pavel Ruminov was born in America.', 'features': {'mtp': 0.8846326470375061}, 'mtp_by_pos': {'ADJ': 0.8846326470375061, 'PROPN': 0.9456303119659424}, 'min_prob_token': 'vel', 'min_prob': 0.8846326470375061}, {'generated_text': 'Vic Chessnutt is from Georgia.', 'features': {'mtp': 0.9111111760139465}, 'mtp_by_pos': {'PROPN': 0.921832263469696}, 'min_prob_token': 'is', 'min_prob': 0.9135778546333313}, {'generated_text': 'Sumi Jo was born in 1962.', 'features': {'mtp': 0.9116467833518982}, 'mtp_by_pos': {'PROPN': 0.9617659449577332, 'AUX': 0.9316354990005493, 'VERB': 0.9359822273254395, 'ADP': 0.9199194312095642}, 'min_prob_token': 'in', 'min_prob': 0.9199194312095642}, {'generated_text': 'Marion County', 'features': {'mtp': 0.8789101243019104}, 'mtp_by_pos': {'PROPN': 0.8789101243019104}, 'min_prob_token': 'County', 'min_prob': 0.8789101243019104}, {'generated_text': '1958', 'features': {'mtp': 0.9345067143440247}, 'mtp_by_pos': {'PROPN': 0.9345067143440247}, 'min_prob_token': '58', 'min_prob': 0.9345067143440247}, {'generated_text': 'Broadway theatre', 'features': {'mtp': 0.873748242855072}, 'mtp_by_pos': {'NOUN': 0.9597049355506897, 'PROPN': 0.873748242855072}, 'min_prob_token': 'theatre', 'min_prob': 0.873748242855072}, {'generated_text': 'The Daily Show.', 'features': {'mtp': 0.8844590187072754}, 'mtp_by_pos': {'PROPN': 0.8844590187072754}, 'min_prob_token': 'Show', 'min_prob': 0.8844590187072754}, {'generated_text': 'Deutsche Telekom was formed in 1998.', 'features': {'mtp': 0.8761260509490967}, 'mtp_by_pos': {'ADJ': 0.8900973200798035, 'PROPN': 0.9482570290565491}, 'min_prob_token': 'utsche', 'min_prob': 0.8900973200798035}, {'generated_text': 'Incesticide', 'features': {'mtp': 0.9262917041778564}, 'mtp_by_pos': {'PROPN': 0.9681739807128906}, 'min_prob_token': 'ide', 'min_prob': 0.9262917041778564}, {'generated_text': 'How I Met Your Mother', 'features': {'mtp': 0.8537643551826477}, 'mtp_by_pos': {'PRON': 0.9516793489456177, 'PROPN': 0.8537643551826477, 'NOUN': 0.930092453956604}, 'min_prob_token': 'Met', 'min_prob': 0.8537643551826477}]\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCLqbQsU9rjT",
    "outputId": "55bc39e3-890c-4265-fb16-879203e47b53",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.327416Z",
     "start_time": "2025-04-04T12:12:22.322078Z"
    }
   },
   "source": [
    "X_train_features_maps[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generated_text': 'General Motors',\n",
       " 'features': {'mtp': 0.9077625274658203},\n",
       " 'mtp_by_pos': {'PROPN': 0.9077625274658203},\n",
       " 'min_prob_token': 'Motors',\n",
       " 'min_prob': 0.9077625274658203}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:15:17.123179Z",
     "start_time": "2025-04-04T12:15:17.119027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_data = []\n",
    "all_pos_keys = set()\n",
    "\n",
    "for item in X_train_features_maps:\n",
    "    if 'mtp_by_pos' in item:\n",
    "        all_pos_keys.update(item['mtp_by_pos'].keys())\n",
    "\n",
    "sorted_pos_keys = sorted(list(all_pos_keys))\n",
    "print(f\"Claves POS encontradas en mtp_by_pos: {sorted_pos_keys}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves POS encontradas en mtp_by_pos: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:16:07.364796Z",
     "start_time": "2025-04-04T12:16:07.332906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for item in X_train_features_maps:\n",
    "    flat_features = {}\n",
    "\n",
    "    # 1. Añadir características generales de 'features'\n",
    "    if 'features' in item:\n",
    "        flat_features.update(item['features']) # Copia todas las claves/valores de features\n",
    "\n",
    "    # 2. Añadir características de 'mtp_by_pos' como columnas separadas\n",
    "    # Inicializa todas las claves POS a 0 para esta muestra\n",
    "    for pos_key in sorted_pos_keys:\n",
    "        flat_features[f'mtp_{pos_key}'] = 0.0\n",
    "\n",
    "    # Actualiza con los valores reales que sí existen para esta muestra\n",
    "    if 'mtp_by_pos' in item:\n",
    "        for pos_key, value in item['mtp_by_pos'].items():\n",
    "            flat_features[f'mtp_{pos_key}'] = value\n",
    "\n",
    "    processed_data.append(flat_features)\n",
    "\n",
    "processed_data"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mtp': 0.9077625274658203,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9077625274658203,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8618128299713135,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9314356446266174,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9041664004325867,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9041664004325867,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9295479655265808,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9059568643569946,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.966061532497406,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.895056962966919,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.895056962966919,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8926945924758911,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8926945924758911,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9321808815002441,\n",
       "  'mtp_ADJ': 0.9321808815002441,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9493626356124878,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9369361996650696,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.9369361996650696},\n",
       " {'mtp': 0.8810909390449524,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9152867794036865,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8810909390449524,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.90814608335495,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.3084222376346588,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8891922831535339,\n",
       "  'mtp_ADV': 0.9194910526275635,\n",
       "  'mtp_AUX': 0.8045060038566589,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9225199818611145,\n",
       "  'mtp_NOUN': 0.7956587672233582,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.8977753520011902,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8048913478851318,\n",
       "  'mtp_PUNCT': 0.3084222376346588,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.7244643568992615,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8846781253814697,\n",
       "  'mtp_ADJ': 0.9233909845352173,\n",
       "  'mtp_ADP': 0.9101591110229492,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.8846781253814697,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9169543385505676,\n",
       "  'mtp_NOUN': 0.927879810333252,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8851715326309204,\n",
       "  'mtp_PUNCT': 0.9007777571678162,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8998801708221436,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9622822403907776,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.5142337083816528,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.891181230545044,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7918579578399658,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.7918579578399658,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8147038817405701,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.8147038817405701,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9022235870361328,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.9022235870361328,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.9395455121994019,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7622854709625244,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9034737348556519,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7534147500991821,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8948752284049988,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.8999575972557068,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9402792453765869,\n",
       "  'mtp_NOUN': 0.8324266672134399,\n",
       "  'mtp_NUM': 0.7534147500991821,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.9093531370162964,\n",
       "  'mtp_PROPN': 0.8419469594955444,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.7997487783432007,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8980578780174255,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9396417140960693,\n",
       "  'mtp_PUNCT': 0.8980578780174255,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9440358281135559,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9440358281135559,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9176352024078369,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9185563921928406,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9176352024078369,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7633092999458313,\n",
       "  'mtp_ADJ': 0.7633092999458313,\n",
       "  'mtp_ADP': 0.8220864534378052,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.888444185256958,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.8388203978538513,\n",
       "  'mtp_NOUN': 0.8941532969474792,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.802459180355072,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9219197034835815,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8907222151756287,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9160826206207275,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9251248836517334,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9137165546417236,\n",
       "  'mtp_NOUN': 0.9358388781547546,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9368259310722351,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8572959899902344,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8937975168228149,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9218842387199402,\n",
       "  'mtp_NOUN': 0.9109917879104614,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9020179510116577,\n",
       "  'mtp_PUNCT': 0.912697970867157,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8441628217697144,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8441628217697144,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8767443895339966,\n",
       "  'mtp_ADJ': 0.935827910900116,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9483429193496704,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8767443895339966,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9307605624198914,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.953195333480835,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.953195333480835,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8808730840682983,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9036818742752075,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9313326478004456,\n",
       "  'mtp_NOUN': 0.9148603677749634,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.9354284405708313,\n",
       "  'mtp_PRON': 0.9320167303085327,\n",
       "  'mtp_PROPN': 0.925250232219696,\n",
       "  'mtp_PUNCT': 0.8808730840682983,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9177147746086121,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8975585103034973,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9436245560646057,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8936061263084412,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8936061263084412,\n",
       "  'mtp_PUNCT': 0.9099173545837402,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.9613379240036011},\n",
       " {'mtp': 0.8839221596717834,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9594504833221436,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9621952772140503,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9621952772140503,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9320068955421448,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9320068955421448,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7416226267814636,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9062221050262451,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9213289618492126,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.8315037488937378,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9438369870185852,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9213758111000061,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8726481795310974,\n",
       "  'mtp_ADJ': 0.930070698261261,\n",
       "  'mtp_ADP': 0.9121163487434387,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9198735356330872,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9198859930038452,\n",
       "  'mtp_NOUN': 0.9305003881454468,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9282760620117188,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8726481795310974,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9581815004348755,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9581815004348755,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9700446128845215,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9700446128845215,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8959623575210571,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9187818765640259,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.8959623575210571},\n",
       " {'mtp': 0.833669126033783,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9203984141349792,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.833669126033783,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.966443657875061,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9700261950492859,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9055625200271606,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9228209853172302,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8091916441917419,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9652082920074463,\n",
       "  'mtp_NUM': 0.9256210327148438,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8833488821983337,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.9263764023780823,\n",
       "  'mtp_VERB': 0.9646283984184265,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.864083468914032,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9387641549110413,\n",
       "  'mtp_PUNCT': 0.864083468914032,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9084958434104919,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9084958434104919,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9550883769989014,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9555474519729614,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8718334436416626,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.970217227935791,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8538351058959961,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8538351058959961,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7075575590133667,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.914898157119751,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9158878922462463,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.7075575590133667,\n",
       "  'mtp_NOUN': 0.9257091879844666,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8817013502120972,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9560784101486206,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8290148377418518,\n",
       "  'mtp_ADJ': 0.9583900570869446,\n",
       "  'mtp_ADP': 0.898360013961792,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.893031656742096,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.8290148377418518,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.965906023979187,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9213411211967468,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9678472280502319,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8669402003288269,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9519286751747131,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.6985011696815491,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9501746296882629,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.4807051122188568,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8687989115715027,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9178534746170044,\n",
       "  'mtp_NOUN': 0.8556574583053589,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.4807051122188568,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8971734046936035,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9163604378700256,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.9415259957313538,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9305517673492432,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9305517673492432,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.6460758447647095,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9231991171836853,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8809255361557007,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9441924691200256,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9525438547134399,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9525438547134399,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9239016175270081,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9710487127304077,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8570117354393005,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8808095455169678,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9476191401481628,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9476191401481628,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8904656171798706,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.8904656171798706,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9247053861618042,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9652293920516968,\n",
       "  'mtp_PUNCT': 0.9078361988067627,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9655066728591919,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.5473272204399109,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.921570360660553,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9377526044845581,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9377526044845581,\n",
       "  'mtp_PUNCT': 0.9392861723899841,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9649785757064819,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9649785757064819,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9175736904144287,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9175736904144287,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9285092949867249,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9285092949867249,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8913741707801819,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9309539198875427,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.6348397135734558,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9240406155586243,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.9234874248504639,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.8680928945541382,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.92328280210495,\n",
       "  'mtp_PROPN': 0.6348397135734558,\n",
       "  'mtp_PUNCT': 0.9104700684547424,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8639509677886963,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9564311504364014,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9564311504364014,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8301270604133606,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.8785549402236938,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8451586961746216,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9176048636436462,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9269501566886902,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9176048636436462,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8913476467132568,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8913476467132568,\n",
       "  'mtp_PUNCT': 0.9146857857704163,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8641395568847656,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9540696740150452,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8673632144927979,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9439634084701538,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.9439634084701538},\n",
       " {'mtp': 0.8606687784194946,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.926376223564148,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8266614079475403,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9233978986740112,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.8915130496025085,\n",
       "  'mtp_NOUN': 0.8943051099777222,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.8903921842575073,\n",
       "  'mtp_PROPN': 0.9211394786834717,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8266614079475403,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8846326470375061,\n",
       "  'mtp_ADJ': 0.8846326470375061,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9456303119659424,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9111111760139465,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.921832263469696,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9116467833518982,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9199194312095642,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9316354990005493,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9617659449577332,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9359822273254395,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8789101243019104,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8789101243019104,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9345067143440247,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9345067143440247,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.873748242855072,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9597049355506897,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.873748242855072,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8844590187072754,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8844590187072754,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8761260509490967,\n",
       "  'mtp_ADJ': 0.8900973200798035,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9482570290565491,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9262917041778564,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9681739807128906,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8537643551826477,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.930092453956604,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.9516793489456177,\n",
       "  'mtp_PROPN': 0.8537643551826477,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9077625274658203,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9077625274658203,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8618128299713135,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9314356446266174,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9041664004325867,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9041664004325867,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9295479655265808,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9059568643569946,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.966061532497406,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.895056962966919,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.895056962966919,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8926945924758911,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8926945924758911,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9321808815002441,\n",
       "  'mtp_ADJ': 0.9321808815002441,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9493626356124878,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9369361996650696,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.9369361996650696},\n",
       " {'mtp': 0.8810909390449524,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9152867794036865,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8810909390449524,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.90814608335495,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.3084222376346588,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8891922831535339,\n",
       "  'mtp_ADV': 0.9194910526275635,\n",
       "  'mtp_AUX': 0.8045060038566589,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9225199818611145,\n",
       "  'mtp_NOUN': 0.7956587672233582,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.8977753520011902,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8048913478851318,\n",
       "  'mtp_PUNCT': 0.3084222376346588,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.7244643568992615,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8846781253814697,\n",
       "  'mtp_ADJ': 0.9233909845352173,\n",
       "  'mtp_ADP': 0.9101591110229492,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.8846781253814697,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9169543385505676,\n",
       "  'mtp_NOUN': 0.927879810333252,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8851715326309204,\n",
       "  'mtp_PUNCT': 0.9007777571678162,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8998801708221436,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9622822403907776,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.5142337083816528,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.891181230545044,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7918579578399658,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.7918579578399658,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8147038817405701,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.8147038817405701,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9022235870361328,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.9022235870361328,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.9395455121994019,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7622854709625244,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9034737348556519,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7534147500991821,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8948752284049988,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.8999575972557068,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9402792453765869,\n",
       "  'mtp_NOUN': 0.8324266672134399,\n",
       "  'mtp_NUM': 0.7534147500991821,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.9093531370162964,\n",
       "  'mtp_PROPN': 0.8419469594955444,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.7997487783432007,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8980578780174255,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9396417140960693,\n",
       "  'mtp_PUNCT': 0.8980578780174255,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9440358281135559,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9440358281135559,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9176352024078369,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9185563921928406,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9176352024078369,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7633092999458313,\n",
       "  'mtp_ADJ': 0.7633092999458313,\n",
       "  'mtp_ADP': 0.8220864534378052,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.888444185256958,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.8388203978538513,\n",
       "  'mtp_NOUN': 0.8941532969474792,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.802459180355072,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9219197034835815,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8907222151756287,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9160826206207275,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9251248836517334,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9137165546417236,\n",
       "  'mtp_NOUN': 0.9358388781547546,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9368259310722351,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8572959899902344,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8937975168228149,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9218842387199402,\n",
       "  'mtp_NOUN': 0.9109917879104614,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9020179510116577,\n",
       "  'mtp_PUNCT': 0.912697970867157,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8441628217697144,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8441628217697144,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8767443895339966,\n",
       "  'mtp_ADJ': 0.935827910900116,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9483429193496704,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8767443895339966,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9307605624198914,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.953195333480835,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.953195333480835,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8808730840682983,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9036818742752075,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9313326478004456,\n",
       "  'mtp_NOUN': 0.9148603677749634,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.9354284405708313,\n",
       "  'mtp_PRON': 0.9320167303085327,\n",
       "  'mtp_PROPN': 0.925250232219696,\n",
       "  'mtp_PUNCT': 0.8808730840682983,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9177147746086121,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8975585103034973,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9436245560646057,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8936061263084412,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8936061263084412,\n",
       "  'mtp_PUNCT': 0.9099173545837402,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.9613379240036011},\n",
       " {'mtp': 0.8839221596717834,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9594504833221436,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9621952772140503,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9621952772140503,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9320068955421448,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9320068955421448,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7416226267814636,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9062221050262451,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9213289618492126,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.8315037488937378,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9438369870185852,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9213758111000061,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8726481795310974,\n",
       "  'mtp_ADJ': 0.930070698261261,\n",
       "  'mtp_ADP': 0.9121163487434387,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9198735356330872,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9198859930038452,\n",
       "  'mtp_NOUN': 0.9305003881454468,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9282760620117188,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8726481795310974,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9581815004348755,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9581815004348755,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9700446128845215,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9700446128845215,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8959623575210571,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9187818765640259,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.8959623575210571},\n",
       " {'mtp': 0.833669126033783,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9203984141349792,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.833669126033783,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.966443657875061,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9700261950492859,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9055625200271606,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9228209853172302,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8091916441917419,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9652082920074463,\n",
       "  'mtp_NUM': 0.9256210327148438,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8833488821983337,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.9263764023780823,\n",
       "  'mtp_VERB': 0.9646283984184265,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.864083468914032,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9387641549110413,\n",
       "  'mtp_PUNCT': 0.864083468914032,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9084958434104919,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9084958434104919,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9550883769989014,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9555474519729614,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8718334436416626,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.970217227935791,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8538351058959961,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8538351058959961,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.7075575590133667,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.914898157119751,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9158878922462463,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.7075575590133667,\n",
       "  'mtp_NOUN': 0.9257091879844666,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8817013502120972,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9560784101486206,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8290148377418518,\n",
       "  'mtp_ADJ': 0.9583900570869446,\n",
       "  'mtp_ADP': 0.898360013961792,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.893031656742096,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.8290148377418518,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.965906023979187,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9213411211967468,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9678472280502319,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8669402003288269,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9519286751747131,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.6985011696815491,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9501746296882629,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.4807051122188568,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.8687989115715027,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.9178534746170044,\n",
       "  'mtp_NOUN': 0.8556574583053589,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.4807051122188568,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8971734046936035,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9163604378700256,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.9415259957313538,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9305517673492432,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9305517673492432,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.6460758447647095,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9231991171836853,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8809255361557007,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9441924691200256,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9525438547134399,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9525438547134399,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9239016175270081,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9710487127304077,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8570117354393005,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8808095455169678,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9476191401481628,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9476191401481628,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8904656171798706,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.8904656171798706,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9247053861618042,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9652293920516968,\n",
       "  'mtp_PUNCT': 0.9078361988067627,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9655066728591919,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.5473272204399109,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.921570360660553,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9377526044845581,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9377526044845581,\n",
       "  'mtp_PUNCT': 0.9392861723899841,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9649785757064819,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9649785757064819,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9175736904144287,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9175736904144287,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9285092949867249,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9285092949867249,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8913741707801819,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9309539198875427,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.6348397135734558,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9240406155586243,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.9234874248504639,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.8680928945541382,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.92328280210495,\n",
       "  'mtp_PROPN': 0.6348397135734558,\n",
       "  'mtp_PUNCT': 0.9104700684547424,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8639509677886963,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9564311504364014,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9564311504364014,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8301270604133606,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.8785549402236938,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8451586961746216,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9176048636436462,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9269501566886902,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9176048636436462,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8913476467132568,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8913476467132568,\n",
       "  'mtp_PUNCT': 0.9146857857704163,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8641395568847656,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9540696740150452,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8673632144927979,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9439634084701538,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.0,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.9439634084701538},\n",
       " {'mtp': 0.8606687784194946,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.926376223564148,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8266614079475403,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9233978986740112,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.8915130496025085,\n",
       "  'mtp_NOUN': 0.8943051099777222,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.8903921842575073,\n",
       "  'mtp_PROPN': 0.9211394786834717,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.8266614079475403,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8846326470375061,\n",
       "  'mtp_ADJ': 0.8846326470375061,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9456303119659424,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9111111760139465,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.921832263469696,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9116467833518982,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.9199194312095642,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.9316354990005493,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9617659449577332,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.9359822273254395,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8789101243019104,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8789101243019104,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9345067143440247,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9345067143440247,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.873748242855072,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.9597049355506897,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.873748242855072,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8844590187072754,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.8844590187072754,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8761260509490967,\n",
       "  'mtp_ADJ': 0.8900973200798035,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9482570290565491,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.9262917041778564,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.0,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.0,\n",
       "  'mtp_PROPN': 0.9681739807128906,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0},\n",
       " {'mtp': 0.8537643551826477,\n",
       "  'mtp_ADJ': 0.0,\n",
       "  'mtp_ADP': 0.0,\n",
       "  'mtp_ADV': 0.0,\n",
       "  'mtp_AUX': 0.0,\n",
       "  'mtp_CCONJ': 0.0,\n",
       "  'mtp_DET': 0.0,\n",
       "  'mtp_NOUN': 0.930092453956604,\n",
       "  'mtp_NUM': 0.0,\n",
       "  'mtp_PART': 0.0,\n",
       "  'mtp_PRON': 0.9516793489456177,\n",
       "  'mtp_PROPN': 0.8537643551826477,\n",
       "  'mtp_PUNCT': 0.0,\n",
       "  'mtp_SCONJ': 0.0,\n",
       "  'mtp_SYM': 0.0,\n",
       "  'mtp_VERB': 0.0,\n",
       "  'mtp_X': 0.0}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:16:47.840819Z",
     "start_time": "2025-04-04T12:16:47.816002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train_df = pd.DataFrame(processed_data)\n",
    "\n",
    "# Rellenar posibles valores NaN que puedan quedar (si alguna 'features' faltara en algún item) con 0\n",
    "X_train_df = X_train_df.fillna(0)\n",
    "\n",
    "print(\"\\nDataFrame de características procesado (X_train_df):\")\n",
    "print(X_train_df)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame de características procesado (X_train_df):\n",
      "          mtp   mtp_ADJ   mtp_ADP  mtp_ADV  mtp_AUX  mtp_CCONJ  mtp_DET  \\\n",
      "0    0.907763  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "1    0.861813  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "2    0.904166  0.000000  0.904166      0.0      0.0        0.0      0.0   \n",
      "3    0.905957  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "4    0.895057  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "..        ...       ...       ...      ...      ...        ...      ...   \n",
      "165  0.873748  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "166  0.884459  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "167  0.876126  0.890097  0.000000      0.0      0.0        0.0      0.0   \n",
      "168  0.926292  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "169  0.853764  0.000000  0.000000      0.0      0.0        0.0      0.0   \n",
      "\n",
      "     mtp_NOUN  mtp_NUM  mtp_PART  mtp_PRON  mtp_PROPN  mtp_PUNCT  mtp_SCONJ  \\\n",
      "0    0.000000      0.0       0.0  0.000000   0.907763        0.0        0.0   \n",
      "1    0.000000      0.0       0.0  0.000000   0.931436        0.0        0.0   \n",
      "2    0.000000      0.0       0.0  0.000000   0.929548        0.0        0.0   \n",
      "3    0.000000      0.0       0.0  0.000000   0.966062        0.0        0.0   \n",
      "4    0.000000      0.0       0.0  0.000000   0.895057        0.0        0.0   \n",
      "..        ...      ...       ...       ...        ...        ...        ...   \n",
      "165  0.959705      0.0       0.0  0.000000   0.873748        0.0        0.0   \n",
      "166  0.000000      0.0       0.0  0.000000   0.884459        0.0        0.0   \n",
      "167  0.000000      0.0       0.0  0.000000   0.948257        0.0        0.0   \n",
      "168  0.000000      0.0       0.0  0.000000   0.968174        0.0        0.0   \n",
      "169  0.930092      0.0       0.0  0.951679   0.853764        0.0        0.0   \n",
      "\n",
      "     mtp_SYM  mtp_VERB  mtp_X  \n",
      "0        0.0       0.0    0.0  \n",
      "1        0.0       0.0    0.0  \n",
      "2        0.0       0.0    0.0  \n",
      "3        0.0       0.0    0.0  \n",
      "4        0.0       0.0    0.0  \n",
      "..       ...       ...    ...  \n",
      "165      0.0       0.0    0.0  \n",
      "166      0.0       0.0    0.0  \n",
      "167      0.0       0.0    0.0  \n",
      "168      0.0       0.0    0.0  \n",
      "169      0.0       0.0    0.0  \n",
      "\n",
      "[170 rows x 17 columns]\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:22:12.908653Z",
     "start_time": "2025-04-04T12:22:12.906097Z"
    }
   },
   "cell_type": "code",
   "source": "Y_train_subset = Y_train[:100]\n",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:22:39.265003Z",
     "start_time": "2025-04-04T12:22:38.741313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FRHI34IE9uMQ",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.384127Z",
     "start_time": "2025-04-04T12:12:22.380485Z"
    }
   },
   "source": [
    "X_train_features = [list(dic.values()) for dic in X_train_features_maps]"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eWIgjAU-UW9",
    "outputId": "2e9e6ec1-86a2-4391-d72c-a79b9a57f3c1",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.435381Z",
     "start_time": "2025-04-04T12:12:22.430639Z"
    }
   },
   "source": [
    "len(X_train_features)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFXzlfwd-UYy",
    "outputId": "060eced8-05e9-4ff5-d770-bd58e8821234",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:22.490558Z",
     "start_time": "2025-04-04T12:12:22.487044Z"
    }
   },
   "source": [
    "X_train_features[0]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['General Motors',\n",
       " {'mtp': 0.9077625274658203},\n",
       " {'PROPN': 0.9077625274658203},\n",
       " 'Motors',\n",
       " 0.9077625274658203]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLgd5gk9SjjK"
   },
   "source": [
    "## Training Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "UdbK2IsvQgsp",
    "outputId": "aa9807b7-0f17-4039-acf8-30ddfe0f3b63",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.744684Z",
     "start_time": "2025-04-04T12:12:22.522693Z"
    }
   },
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(verbose=1)\n",
    "clf.fit(X_train_features, Y_train)"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'General Motors'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[31]\u001B[39m\u001B[32m, line 4\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01msklearn\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mlinear_model\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m LogisticRegression\n\u001B[32m      3\u001B[39m clf = LogisticRegression(verbose=\u001B[32m1\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m4\u001B[39m \u001B[43mclf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_features\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\HalluDetect-main\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\HalluDetect-main\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1222\u001B[39m, in \u001B[36mLogisticRegression.fit\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m   1219\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1220\u001B[39m     _dtype = [np.float64, np.float32]\n\u001B[32m-> \u001B[39m\u001B[32m1222\u001B[39m X, y = \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1224\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1225\u001B[39m \u001B[43m    \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1226\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1227\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1228\u001B[39m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mC\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1229\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43msolver\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mliblinear\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msag\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msaga\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1230\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1231\u001B[39m check_classification_targets(y)\n\u001B[32m   1232\u001B[39m \u001B[38;5;28mself\u001B[39m.classes_ = np.unique(y)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\HalluDetect-main\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2959\u001B[39m         y = check_array(y, input_name=\u001B[33m\"\u001B[39m\u001B[33my\u001B[39m\u001B[33m\"\u001B[39m, **check_y_params)\n\u001B[32m   2960\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2961\u001B[39m         X, y = \u001B[43mcheck_X_y\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2962\u001B[39m     out = X, y\n\u001B[32m   2964\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m check_params.get(\u001B[33m\"\u001B[39m\u001B[33mensure_2d\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mTrue\u001B[39;00m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\HalluDetect-main\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001B[39m, in \u001B[36mcheck_X_y\u001B[39m\u001B[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[39m\n\u001B[32m   1364\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1365\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m requires y to be passed, but the target y is None\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1366\u001B[39m     )\n\u001B[32m   1368\u001B[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001B[32m-> \u001B[39m\u001B[32m1370\u001B[39m X = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1371\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1372\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1373\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43maccept_large_sparse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1374\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1375\u001B[39m \u001B[43m    \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1376\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1377\u001B[39m \u001B[43m    \u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m=\u001B[49m\u001B[43mforce_writeable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1378\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1379\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_2d\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1380\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_nd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1381\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_min_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1382\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_min_features\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1383\u001B[39m \u001B[43m    \u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1384\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1385\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1387\u001B[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001B[32m   1389\u001B[39m check_consistent_length(X, y)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\HalluDetect-main\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1055\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1053\u001B[39m         array = xp.astype(array, dtype, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1054\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1055\u001B[39m         array = \u001B[43m_asarray_with_order\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1056\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[32m   1057\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1058\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.format(array)\n\u001B[32m   1059\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcomplex_warning\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\HalluDetect-main\\.venv\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:839\u001B[39m, in \u001B[36m_asarray_with_order\u001B[39m\u001B[34m(array, dtype, order, copy, xp, device)\u001B[39m\n\u001B[32m    837\u001B[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001B[32m    838\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m839\u001B[39m     array = \u001B[43mnumpy\u001B[49m\u001B[43m.\u001B[49m\u001B[43masarray\u001B[49m\u001B[43m(\u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43morder\u001B[49m\u001B[43m=\u001B[49m\u001B[43morder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    841\u001B[39m \u001B[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001B[39;00m\n\u001B[32m    842\u001B[39m \u001B[38;5;66;03m# container that is consistent with the input's namespace.\u001B[39;00m\n\u001B[32m    843\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m xp.asarray(array)\n",
      "\u001B[31mValueError\u001B[39m: could not convert string to float: 'General Motors'"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O69UE4MUS5_v"
   },
   "source": [
    "## Evaluate accuracy of Logistic Regression on the training set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KA6ovXb2S5kQ",
    "outputId": "d3764fb9-914e-4fff-d6e5-e3f4145d98bf",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.749592Z",
     "start_time": "2025-03-18T11:41:21.537209Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_Pred = clf.predict(X_train_features)\n",
    "\n",
    "accuracy = accuracy_score(Y_train, Y_Pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 89.40%\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.753148800Z",
     "start_time": "2025-03-18T11:42:21.666194Z"
    }
   },
   "source": [
    "log_odds = clf.coef_[0]\n",
    "odds = np.exp(clf.coef_[0])\n",
    "lr_features_log = {k: v for k, v in zip(X_train_features_maps[0].keys(), log_odds)}\n",
    "lr_features_no_log = {k: v for k, v in zip(X_train_features_maps[0].keys(), odds)}\n",
    "\n",
    "print(\"log\", lr_features_log)\n",
    "print(\"no_log\", lr_features_no_log)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log {'mtp': -7.111009509658728}\n",
      "no_log {'mtp': 0.0008160707448331231}\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCtQNHq9zVDv"
   },
   "source": [
    "## Extracting the Features of the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITx_0w7BTCce"
   },
   "source": [
    "## Extracting the Features of the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Yg60b4WcS5Nw",
    "outputId": "6e9e5b2c-c33e-4bca-c18d-974c30c55b69",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.754391900Z",
     "start_time": "2025-03-18T11:45:26.521709Z"
    }
   },
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "X_test_features_map = []\n",
    "\n",
    "for knowledge, conditioned_text, generated_text in tqdm(X_test, desc=\"Processing\"):\n",
    "    X_test_features_map.append(\n",
    "        extract_features(\n",
    "            knowledge, conditioned_text, generated_text, features_to_extract\n",
    "        )\n",
    "    )\n",
    "    torch.cuda.empty_cache()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 18000/18000 [16:26<00:00, 18.25it/s]\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kOmEOFRY-fci",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.764105300Z",
     "start_time": "2025-03-18T12:02:15.068685Z"
    }
   },
   "source": [
    "X_test_features = [list(dic.values()) for dic in X_test_features_map]"
   ],
   "outputs": [],
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaW4-c3XSzfP"
   },
   "source": [
    "## Evaluate accuracy of the LogisticRegression on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6_VRaKTQgu8",
    "outputId": "6e836007-5968-4930-b19e-c039adc19866",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.765317200Z",
     "start_time": "2025-03-18T12:02:18.499685Z"
    }
   },
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Y_Pred = clf.predict(X_test_features)\n",
    "\n",
    "lr_accuracy = accuracy_score(Y_test, Y_Pred)\n",
    "print(f\"Accuracy: {lr_accuracy * 100:.2f}%\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.33%\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "8pMh03rcrs6m",
    "outputId": "f43bd567-d3a9-4401-c2c3-9429c8c2645f",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.769082800Z",
     "start_time": "2025-03-18T12:02:22.202980Z"
    }
   },
   "source": [
    "log_odds = clf.coef_[0]\n",
    "pd.DataFrame(log_odds, X_train_features_maps[0].keys(), columns=[\"coef\"]).sort_values(\n",
    "    by=\"coef\", ascending=False\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        coef\n",
       "mtp -7.11101"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mtp</th>\n",
       "      <td>-7.11101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "id": "E4b53IT53_B7",
    "outputId": "f0d14aa0-3991-4f8d-cd95-5e35380d8e5e",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.770097800Z",
     "start_time": "2025-03-18T12:02:26.057614Z"
    }
   },
   "source": [
    "odds = np.exp(clf.coef_[0])\n",
    "pd.DataFrame(odds, X_train_features_maps[0].keys(), columns=[\"coef\"]).sort_values(\n",
    "    by=\"coef\", ascending=False\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         coef\n",
       "mtp  0.000816"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mtp</th>\n",
       "      <td>0.000816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5FFCFqGLTMez",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.773249Z",
     "start_time": "2025-03-18T12:02:32.816543Z"
    }
   },
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SimpleDenseNet(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim=1, dropout_prob=0.3):\n",
    "        super(SimpleDenseNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.774258600Z",
     "start_time": "2025-03-18T12:02:35.029543Z"
    }
   },
   "source": [
    "denseModel = SimpleDenseNet(\n",
    "    input_dim=np.array([v for v in features_to_extract.values()]).sum(), hidden_dim=512\n",
    ").to(device)"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FeJQkPQQjFM"
   },
   "source": [
    "# Code declaring and computing all the metrics to measure"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zb_0po_9QiKs",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.775259500Z",
     "start_time": "2025-03-18T12:02:38.775758Z"
    }
   },
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    precision_recall_curve,\n",
    "    auc\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(model, input_tensor, true_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        predicted_probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "        predicted = (outputs > 0.5).float().cpu().numpy()\n",
    "\n",
    "        true_labels = true_labels.cpu().numpy()\n",
    "\n",
    "        acc = accuracy_score(true_labels, predicted)\n",
    "        precision = precision_score(true_labels, predicted)\n",
    "        recall = recall_score(true_labels, predicted)\n",
    "        f1 = f1_score(true_labels, predicted)\n",
    "\n",
    "        precision_negative = precision_score(true_labels, predicted, pos_label=0)\n",
    "        recall_negative = recall_score(true_labels, predicted, pos_label=0)\n",
    "        f1_negative = f1_score(true_labels, predicted, pos_label=0)\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(true_labels, predicted).ravel()\n",
    "        roc_auc = roc_auc_score(true_labels, predicted_probs)\n",
    "\n",
    "        P, R, thre = precision_recall_curve(true_labels, predicted, pos_label=1)\n",
    "        pr_auc = auc(R, P)\n",
    "\n",
    "        roc_auc_negative = roc_auc_score(\n",
    "            true_labels, 1 - predicted_probs\n",
    "        )  # If predicted_probs is the probability of the positive class\n",
    "        P_neg, R_neg, _ = precision_recall_curve(true_labels, predicted, pos_label=0)\n",
    "        pr_auc_negative = auc(R_neg, P_neg)\n",
    "\n",
    "        return {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1\": f1,\n",
    "            \"TP\": tp,\n",
    "            \"TN\": tn,\n",
    "            \"FP\": fp,\n",
    "            \"FN\": fn,\n",
    "            \"ROC AUC\": roc_auc,\n",
    "            \"PR AUC\": pr_auc,\n",
    "            \"Precision-Negative\": precision_negative,\n",
    "            \"Recall-Negative\": recall_negative,\n",
    "            \"F1-Negative\": f1_negative,\n",
    "            \"ROC AUC-Negative\": roc_auc_negative,\n",
    "            \"PR AUC-Negative\": pr_auc_negative,\n",
    "        }"
   ],
   "outputs": [],
   "execution_count": 66
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68Fi6-lJUP11"
   },
   "source": [
    "## Code for training the Dense Model and getting the result of all metrics corresponding to the Testing Set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eNtGdq4b-8wo",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.777258Z",
     "start_time": "2025-03-18T12:02:41.529799Z"
    }
   },
   "source": [
    "def compute_accuracy(model, input_tensor, true_labels):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        correct = (predicted == true_labels).float().sum()\n",
    "        accuracy = correct / len(true_labels)\n",
    "        return accuracy.item()\n",
    "\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_features, dtype=torch.float32).to(device)\n",
    "Y_train_tensor = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "print(X_train_tensor.shape, Y_train_tensor.shape)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(denseModel.parameters(), lr=0.001)\n",
    "\n",
    "bestValAcc = 0\n",
    "# Training loop\n",
    "num_epochs = 20000\n",
    "for epoch in range(num_epochs):\n",
    "    denseModel.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = denseModel(X_train_tensor)\n",
    "    loss = criterion(outputs, Y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute training accuracy\n",
    "    train_accuracy = compute_accuracy(denseModel, X_train_tensor, Y_train_tensor)\n",
    "\n",
    "    # Uncomment this if you want to see how the accuracy of testing improves during the training process.\n",
    "    ##Compute testing accuracy\n",
    "    # X_val_tensor = torch.tensor(X_val_features, dtype=torch.float32).to(device)\n",
    "    # Y_val_tensor = torch.tensor(Y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "    # val_accuracy = compute_accuracy(denseModel, X_val_tensor, Y_val_tensor)\n",
    "\n",
    "    # if bestValAcc < val_accuracy:\n",
    "    #     bestValAcc = val_accuracy\n",
    "    #     print(f'Saving model with best validation accuracy ...')\n",
    "    #     torch.save(denseModel.state_dict(), 'llama-' + task + '-best-model')\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Training Accuracy: {train_accuracy:.4f}\"\n",
    "        )  # , \"Validation Accuracy\": {val_accuracy:.4f}')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 1]) torch.Size([2000, 1])\n",
      "Epoch [10/20000], Loss: 0.3119, Training Accuracy: 0.8910\n",
      "Epoch [20/20000], Loss: 0.2958, Training Accuracy: 0.9050\n",
      "Epoch [30/20000], Loss: 0.2859, Training Accuracy: 0.9055\n",
      "Epoch [40/20000], Loss: 0.2829, Training Accuracy: 0.9025\n",
      "Epoch [50/20000], Loss: 0.2816, Training Accuracy: 0.9035\n",
      "Epoch [60/20000], Loss: 0.2812, Training Accuracy: 0.9040\n",
      "Epoch [70/20000], Loss: 0.2810, Training Accuracy: 0.9040\n",
      "Epoch [80/20000], Loss: 0.2810, Training Accuracy: 0.9040\n",
      "Epoch [90/20000], Loss: 0.2809, Training Accuracy: 0.9035\n",
      "Epoch [100/20000], Loss: 0.2809, Training Accuracy: 0.9035\n",
      "Epoch [110/20000], Loss: 0.2808, Training Accuracy: 0.9035\n",
      "Epoch [120/20000], Loss: 0.2808, Training Accuracy: 0.9035\n",
      "Epoch [130/20000], Loss: 0.2808, Training Accuracy: 0.9035\n",
      "Epoch [140/20000], Loss: 0.2808, Training Accuracy: 0.9035\n",
      "Epoch [150/20000], Loss: 0.2807, Training Accuracy: 0.9035\n",
      "Epoch [160/20000], Loss: 0.2807, Training Accuracy: 0.9035\n",
      "Epoch [170/20000], Loss: 0.2807, Training Accuracy: 0.9035\n",
      "Epoch [180/20000], Loss: 0.2807, Training Accuracy: 0.9035\n",
      "Epoch [190/20000], Loss: 0.2806, Training Accuracy: 0.9035\n",
      "Epoch [200/20000], Loss: 0.2806, Training Accuracy: 0.9035\n",
      "Epoch [210/20000], Loss: 0.2806, Training Accuracy: 0.9035\n",
      "Epoch [220/20000], Loss: 0.2806, Training Accuracy: 0.9035\n",
      "Epoch [230/20000], Loss: 0.2806, Training Accuracy: 0.9030\n",
      "Epoch [240/20000], Loss: 0.2805, Training Accuracy: 0.9030\n",
      "Epoch [250/20000], Loss: 0.2805, Training Accuracy: 0.9030\n",
      "Epoch [260/20000], Loss: 0.2805, Training Accuracy: 0.9030\n",
      "Epoch [270/20000], Loss: 0.2805, Training Accuracy: 0.9030\n",
      "Epoch [280/20000], Loss: 0.2804, Training Accuracy: 0.9030\n",
      "Epoch [290/20000], Loss: 0.2804, Training Accuracy: 0.9030\n",
      "Epoch [300/20000], Loss: 0.2804, Training Accuracy: 0.9030\n",
      "Epoch [310/20000], Loss: 0.2804, Training Accuracy: 0.9030\n",
      "Epoch [320/20000], Loss: 0.2803, Training Accuracy: 0.9030\n",
      "Epoch [330/20000], Loss: 0.2803, Training Accuracy: 0.9030\n",
      "Epoch [340/20000], Loss: 0.2803, Training Accuracy: 0.9030\n",
      "Epoch [350/20000], Loss: 0.2803, Training Accuracy: 0.9030\n",
      "Epoch [360/20000], Loss: 0.2802, Training Accuracy: 0.9030\n",
      "Epoch [370/20000], Loss: 0.2802, Training Accuracy: 0.9030\n",
      "Epoch [380/20000], Loss: 0.2802, Training Accuracy: 0.9030\n",
      "Epoch [390/20000], Loss: 0.2802, Training Accuracy: 0.9030\n",
      "Epoch [400/20000], Loss: 0.2801, Training Accuracy: 0.9030\n",
      "Epoch [410/20000], Loss: 0.2801, Training Accuracy: 0.9025\n",
      "Epoch [420/20000], Loss: 0.2801, Training Accuracy: 0.9025\n",
      "Epoch [430/20000], Loss: 0.2801, Training Accuracy: 0.9025\n",
      "Epoch [440/20000], Loss: 0.2801, Training Accuracy: 0.9025\n",
      "Epoch [450/20000], Loss: 0.2800, Training Accuracy: 0.9025\n",
      "Epoch [460/20000], Loss: 0.2800, Training Accuracy: 0.9025\n",
      "Epoch [470/20000], Loss: 0.2800, Training Accuracy: 0.9025\n",
      "Epoch [480/20000], Loss: 0.2800, Training Accuracy: 0.9025\n",
      "Epoch [490/20000], Loss: 0.2799, Training Accuracy: 0.9025\n",
      "Epoch [500/20000], Loss: 0.2799, Training Accuracy: 0.9025\n",
      "Epoch [510/20000], Loss: 0.2799, Training Accuracy: 0.9025\n",
      "Epoch [520/20000], Loss: 0.2799, Training Accuracy: 0.9030\n",
      "Epoch [530/20000], Loss: 0.2798, Training Accuracy: 0.9030\n",
      "Epoch [540/20000], Loss: 0.2798, Training Accuracy: 0.9030\n",
      "Epoch [550/20000], Loss: 0.2798, Training Accuracy: 0.9030\n",
      "Epoch [560/20000], Loss: 0.2798, Training Accuracy: 0.9030\n",
      "Epoch [570/20000], Loss: 0.2798, Training Accuracy: 0.9030\n",
      "Epoch [580/20000], Loss: 0.2797, Training Accuracy: 0.9030\n",
      "Epoch [590/20000], Loss: 0.2797, Training Accuracy: 0.9030\n",
      "Epoch [600/20000], Loss: 0.2797, Training Accuracy: 0.9030\n",
      "Epoch [610/20000], Loss: 0.2797, Training Accuracy: 0.9030\n",
      "Epoch [620/20000], Loss: 0.2796, Training Accuracy: 0.9030\n",
      "Epoch [630/20000], Loss: 0.2796, Training Accuracy: 0.9035\n",
      "Epoch [640/20000], Loss: 0.2796, Training Accuracy: 0.9035\n",
      "Epoch [650/20000], Loss: 0.2796, Training Accuracy: 0.9035\n",
      "Epoch [660/20000], Loss: 0.2796, Training Accuracy: 0.9035\n",
      "Epoch [670/20000], Loss: 0.2795, Training Accuracy: 0.9035\n",
      "Epoch [680/20000], Loss: 0.2795, Training Accuracy: 0.9035\n",
      "Epoch [690/20000], Loss: 0.2795, Training Accuracy: 0.9035\n",
      "Epoch [700/20000], Loss: 0.2795, Training Accuracy: 0.9035\n",
      "Epoch [710/20000], Loss: 0.2795, Training Accuracy: 0.9035\n",
      "Epoch [720/20000], Loss: 0.2795, Training Accuracy: 0.9035\n",
      "Epoch [730/20000], Loss: 0.2794, Training Accuracy: 0.9035\n",
      "Epoch [740/20000], Loss: 0.2794, Training Accuracy: 0.9035\n",
      "Epoch [750/20000], Loss: 0.2794, Training Accuracy: 0.9035\n",
      "Epoch [760/20000], Loss: 0.2794, Training Accuracy: 0.9035\n",
      "Epoch [770/20000], Loss: 0.2794, Training Accuracy: 0.9035\n",
      "Epoch [780/20000], Loss: 0.2794, Training Accuracy: 0.9035\n",
      "Epoch [790/20000], Loss: 0.2793, Training Accuracy: 0.9035\n",
      "Epoch [800/20000], Loss: 0.2793, Training Accuracy: 0.9035\n",
      "Epoch [810/20000], Loss: 0.2793, Training Accuracy: 0.9030\n",
      "Epoch [820/20000], Loss: 0.2793, Training Accuracy: 0.9035\n",
      "Epoch [830/20000], Loss: 0.2793, Training Accuracy: 0.9040\n",
      "Epoch [840/20000], Loss: 0.2793, Training Accuracy: 0.9025\n",
      "Epoch [850/20000], Loss: 0.2792, Training Accuracy: 0.9035\n",
      "Epoch [860/20000], Loss: 0.2791, Training Accuracy: 0.9035\n",
      "Epoch [870/20000], Loss: 0.2791, Training Accuracy: 0.9035\n",
      "Epoch [880/20000], Loss: 0.2790, Training Accuracy: 0.9025\n",
      "Epoch [890/20000], Loss: 0.2791, Training Accuracy: 0.9030\n",
      "Epoch [900/20000], Loss: 0.2790, Training Accuracy: 0.9030\n",
      "Epoch [910/20000], Loss: 0.2790, Training Accuracy: 0.9040\n",
      "Epoch [920/20000], Loss: 0.2789, Training Accuracy: 0.9025\n",
      "Epoch [930/20000], Loss: 0.2789, Training Accuracy: 0.9025\n",
      "Epoch [940/20000], Loss: 0.2789, Training Accuracy: 0.9035\n",
      "Epoch [950/20000], Loss: 0.2790, Training Accuracy: 0.9040\n",
      "Epoch [960/20000], Loss: 0.2788, Training Accuracy: 0.9035\n",
      "Epoch [970/20000], Loss: 0.2788, Training Accuracy: 0.9025\n",
      "Epoch [980/20000], Loss: 0.2787, Training Accuracy: 0.9030\n",
      "Epoch [990/20000], Loss: 0.2789, Training Accuracy: 0.9040\n",
      "Epoch [1000/20000], Loss: 0.2788, Training Accuracy: 0.9050\n",
      "Epoch [1010/20000], Loss: 0.2788, Training Accuracy: 0.9035\n",
      "Epoch [1020/20000], Loss: 0.2787, Training Accuracy: 0.9045\n",
      "Epoch [1030/20000], Loss: 0.2786, Training Accuracy: 0.9025\n",
      "Epoch [1040/20000], Loss: 0.2788, Training Accuracy: 0.9040\n",
      "Epoch [1050/20000], Loss: 0.2787, Training Accuracy: 0.9050\n",
      "Epoch [1060/20000], Loss: 0.2785, Training Accuracy: 0.9030\n",
      "Epoch [1070/20000], Loss: 0.2785, Training Accuracy: 0.9035\n",
      "Epoch [1080/20000], Loss: 0.2787, Training Accuracy: 0.9045\n",
      "Epoch [1090/20000], Loss: 0.2785, Training Accuracy: 0.9035\n",
      "Epoch [1100/20000], Loss: 0.2784, Training Accuracy: 0.9035\n",
      "Epoch [1110/20000], Loss: 0.2787, Training Accuracy: 0.9040\n",
      "Epoch [1120/20000], Loss: 0.2786, Training Accuracy: 0.9045\n",
      "Epoch [1130/20000], Loss: 0.2784, Training Accuracy: 0.9050\n",
      "Epoch [1140/20000], Loss: 0.2783, Training Accuracy: 0.9035\n",
      "Epoch [1150/20000], Loss: 0.2786, Training Accuracy: 0.9045\n",
      "Epoch [1160/20000], Loss: 0.2782, Training Accuracy: 0.9035\n",
      "Epoch [1170/20000], Loss: 0.2785, Training Accuracy: 0.9050\n",
      "Epoch [1180/20000], Loss: 0.2784, Training Accuracy: 0.9040\n",
      "Epoch [1190/20000], Loss: 0.2782, Training Accuracy: 0.9050\n",
      "Epoch [1200/20000], Loss: 0.2782, Training Accuracy: 0.9035\n",
      "Epoch [1210/20000], Loss: 0.2785, Training Accuracy: 0.9045\n",
      "Epoch [1220/20000], Loss: 0.2782, Training Accuracy: 0.9050\n",
      "Epoch [1230/20000], Loss: 0.2780, Training Accuracy: 0.9035\n",
      "Epoch [1240/20000], Loss: 0.2783, Training Accuracy: 0.9045\n",
      "Epoch [1250/20000], Loss: 0.2780, Training Accuracy: 0.9035\n",
      "Epoch [1260/20000], Loss: 0.2785, Training Accuracy: 0.9030\n",
      "Epoch [1270/20000], Loss: 0.2782, Training Accuracy: 0.9045\n",
      "Epoch [1280/20000], Loss: 0.2781, Training Accuracy: 0.9050\n",
      "Epoch [1290/20000], Loss: 0.2779, Training Accuracy: 0.9035\n",
      "Epoch [1300/20000], Loss: 0.2783, Training Accuracy: 0.9030\n",
      "Epoch [1310/20000], Loss: 0.2781, Training Accuracy: 0.9045\n",
      "Epoch [1320/20000], Loss: 0.2780, Training Accuracy: 0.9045\n",
      "Epoch [1330/20000], Loss: 0.2778, Training Accuracy: 0.9035\n",
      "Epoch [1340/20000], Loss: 0.2779, Training Accuracy: 0.9035\n",
      "Epoch [1350/20000], Loss: 0.2777, Training Accuracy: 0.9030\n",
      "Epoch [1360/20000], Loss: 0.2789, Training Accuracy: 0.9025\n",
      "Epoch [1370/20000], Loss: 0.2777, Training Accuracy: 0.9040\n",
      "Epoch [1380/20000], Loss: 0.2780, Training Accuracy: 0.9050\n",
      "Epoch [1390/20000], Loss: 0.2776, Training Accuracy: 0.9030\n",
      "Epoch [1400/20000], Loss: 0.2776, Training Accuracy: 0.9030\n",
      "Epoch [1410/20000], Loss: 0.2777, Training Accuracy: 0.9040\n",
      "Epoch [1420/20000], Loss: 0.2777, Training Accuracy: 0.9045\n",
      "Epoch [1430/20000], Loss: 0.2776, Training Accuracy: 0.9040\n",
      "Epoch [1440/20000], Loss: 0.2779, Training Accuracy: 0.9040\n",
      "Epoch [1450/20000], Loss: 0.2775, Training Accuracy: 0.9045\n",
      "Epoch [1460/20000], Loss: 0.2774, Training Accuracy: 0.9030\n",
      "Epoch [1470/20000], Loss: 0.2776, Training Accuracy: 0.9045\n",
      "Epoch [1480/20000], Loss: 0.2774, Training Accuracy: 0.9040\n",
      "Epoch [1490/20000], Loss: 0.2778, Training Accuracy: 0.9045\n",
      "Epoch [1500/20000], Loss: 0.2773, Training Accuracy: 0.9025\n",
      "Epoch [1510/20000], Loss: 0.2772, Training Accuracy: 0.9035\n",
      "Epoch [1520/20000], Loss: 0.2774, Training Accuracy: 0.9035\n",
      "Epoch [1530/20000], Loss: 0.2772, Training Accuracy: 0.9030\n",
      "Epoch [1540/20000], Loss: 0.2780, Training Accuracy: 0.9025\n",
      "Epoch [1550/20000], Loss: 0.2781, Training Accuracy: 0.9030\n",
      "Epoch [1560/20000], Loss: 0.2771, Training Accuracy: 0.9035\n",
      "Epoch [1570/20000], Loss: 0.2773, Training Accuracy: 0.9045\n",
      "Epoch [1580/20000], Loss: 0.2771, Training Accuracy: 0.9025\n",
      "Epoch [1590/20000], Loss: 0.2770, Training Accuracy: 0.9035\n",
      "Epoch [1600/20000], Loss: 0.2770, Training Accuracy: 0.9035\n",
      "Epoch [1610/20000], Loss: 0.2780, Training Accuracy: 0.9045\n",
      "Epoch [1620/20000], Loss: 0.2772, Training Accuracy: 0.9040\n",
      "Epoch [1630/20000], Loss: 0.2770, Training Accuracy: 0.9030\n",
      "Epoch [1640/20000], Loss: 0.2771, Training Accuracy: 0.9045\n",
      "Epoch [1650/20000], Loss: 0.2770, Training Accuracy: 0.9030\n",
      "Epoch [1660/20000], Loss: 0.2771, Training Accuracy: 0.9035\n",
      "Epoch [1670/20000], Loss: 0.2774, Training Accuracy: 0.9025\n",
      "Epoch [1680/20000], Loss: 0.2769, Training Accuracy: 0.9030\n",
      "Epoch [1690/20000], Loss: 0.2768, Training Accuracy: 0.9030\n",
      "Epoch [1700/20000], Loss: 0.2771, Training Accuracy: 0.9025\n",
      "Epoch [1710/20000], Loss: 0.2767, Training Accuracy: 0.9030\n",
      "Epoch [1720/20000], Loss: 0.2772, Training Accuracy: 0.9040\n",
      "Epoch [1730/20000], Loss: 0.2770, Training Accuracy: 0.9025\n",
      "Epoch [1740/20000], Loss: 0.2766, Training Accuracy: 0.9025\n",
      "Epoch [1750/20000], Loss: 0.2772, Training Accuracy: 0.9045\n",
      "Epoch [1760/20000], Loss: 0.2769, Training Accuracy: 0.9025\n",
      "Epoch [1770/20000], Loss: 0.2765, Training Accuracy: 0.9030\n",
      "Epoch [1780/20000], Loss: 0.2768, Training Accuracy: 0.9035\n",
      "Epoch [1790/20000], Loss: 0.2768, Training Accuracy: 0.9025\n",
      "Epoch [1800/20000], Loss: 0.2767, Training Accuracy: 0.9050\n",
      "Epoch [1810/20000], Loss: 0.2768, Training Accuracy: 0.9025\n",
      "Epoch [1820/20000], Loss: 0.2765, Training Accuracy: 0.9025\n",
      "Epoch [1830/20000], Loss: 0.2765, Training Accuracy: 0.9035\n",
      "Epoch [1840/20000], Loss: 0.2764, Training Accuracy: 0.9035\n",
      "Epoch [1850/20000], Loss: 0.2770, Training Accuracy: 0.9025\n",
      "Epoch [1860/20000], Loss: 0.2765, Training Accuracy: 0.9035\n",
      "Epoch [1870/20000], Loss: 0.2762, Training Accuracy: 0.9045\n",
      "Epoch [1880/20000], Loss: 0.2763, Training Accuracy: 0.9045\n",
      "Epoch [1890/20000], Loss: 0.2763, Training Accuracy: 0.9035\n",
      "Epoch [1900/20000], Loss: 0.2767, Training Accuracy: 0.9035\n",
      "Epoch [1910/20000], Loss: 0.2762, Training Accuracy: 0.9040\n",
      "Epoch [1920/20000], Loss: 0.2762, Training Accuracy: 0.9035\n",
      "Epoch [1930/20000], Loss: 0.2763, Training Accuracy: 0.9025\n",
      "Epoch [1940/20000], Loss: 0.2762, Training Accuracy: 0.9045\n",
      "Epoch [1950/20000], Loss: 0.2768, Training Accuracy: 0.9050\n",
      "Epoch [1960/20000], Loss: 0.2762, Training Accuracy: 0.9025\n",
      "Epoch [1970/20000], Loss: 0.2760, Training Accuracy: 0.9030\n",
      "Epoch [1980/20000], Loss: 0.2760, Training Accuracy: 0.9025\n",
      "Epoch [1990/20000], Loss: 0.2770, Training Accuracy: 0.9050\n",
      "Epoch [2000/20000], Loss: 0.2759, Training Accuracy: 0.9040\n",
      "Epoch [2010/20000], Loss: 0.2759, Training Accuracy: 0.9030\n",
      "Epoch [2020/20000], Loss: 0.2759, Training Accuracy: 0.9035\n",
      "Epoch [2030/20000], Loss: 0.2758, Training Accuracy: 0.9035\n",
      "Epoch [2040/20000], Loss: 0.2758, Training Accuracy: 0.9035\n",
      "Epoch [2050/20000], Loss: 0.2761, Training Accuracy: 0.9020\n",
      "Epoch [2060/20000], Loss: 0.2757, Training Accuracy: 0.9030\n",
      "Epoch [2070/20000], Loss: 0.2758, Training Accuracy: 0.9025\n",
      "Epoch [2080/20000], Loss: 0.2758, Training Accuracy: 0.9030\n",
      "Epoch [2090/20000], Loss: 0.2756, Training Accuracy: 0.9045\n",
      "Epoch [2100/20000], Loss: 0.2758, Training Accuracy: 0.9025\n",
      "Epoch [2110/20000], Loss: 0.2759, Training Accuracy: 0.9025\n",
      "Epoch [2120/20000], Loss: 0.2763, Training Accuracy: 0.9040\n",
      "Epoch [2130/20000], Loss: 0.2759, Training Accuracy: 0.9025\n",
      "Epoch [2140/20000], Loss: 0.2757, Training Accuracy: 0.9025\n",
      "Epoch [2150/20000], Loss: 0.2755, Training Accuracy: 0.9035\n",
      "Epoch [2160/20000], Loss: 0.2759, Training Accuracy: 0.9020\n",
      "Epoch [2170/20000], Loss: 0.2754, Training Accuracy: 0.9040\n",
      "Epoch [2180/20000], Loss: 0.2766, Training Accuracy: 0.9050\n",
      "Epoch [2190/20000], Loss: 0.2761, Training Accuracy: 0.9025\n",
      "Epoch [2200/20000], Loss: 0.2758, Training Accuracy: 0.9025\n",
      "Epoch [2210/20000], Loss: 0.2755, Training Accuracy: 0.9025\n",
      "Epoch [2220/20000], Loss: 0.2753, Training Accuracy: 0.9035\n",
      "Epoch [2230/20000], Loss: 0.2756, Training Accuracy: 0.9030\n",
      "Epoch [2240/20000], Loss: 0.2754, Training Accuracy: 0.9045\n",
      "Epoch [2250/20000], Loss: 0.2753, Training Accuracy: 0.9045\n",
      "Epoch [2260/20000], Loss: 0.2752, Training Accuracy: 0.9035\n",
      "Epoch [2270/20000], Loss: 0.2753, Training Accuracy: 0.9035\n",
      "Epoch [2280/20000], Loss: 0.2754, Training Accuracy: 0.9030\n",
      "Epoch [2290/20000], Loss: 0.2752, Training Accuracy: 0.9045\n",
      "Epoch [2300/20000], Loss: 0.2752, Training Accuracy: 0.9025\n",
      "Epoch [2310/20000], Loss: 0.2777, Training Accuracy: 0.9020\n",
      "Epoch [2320/20000], Loss: 0.2757, Training Accuracy: 0.9040\n",
      "Epoch [2330/20000], Loss: 0.2752, Training Accuracy: 0.9035\n",
      "Epoch [2340/20000], Loss: 0.2752, Training Accuracy: 0.9025\n",
      "Epoch [2350/20000], Loss: 0.2751, Training Accuracy: 0.9045\n",
      "Epoch [2360/20000], Loss: 0.2750, Training Accuracy: 0.9030\n",
      "Epoch [2370/20000], Loss: 0.2750, Training Accuracy: 0.9045\n",
      "Epoch [2380/20000], Loss: 0.2755, Training Accuracy: 0.9035\n",
      "Epoch [2390/20000], Loss: 0.2750, Training Accuracy: 0.9025\n",
      "Epoch [2400/20000], Loss: 0.2751, Training Accuracy: 0.9030\n",
      "Epoch [2410/20000], Loss: 0.2751, Training Accuracy: 0.9025\n",
      "Epoch [2420/20000], Loss: 0.2749, Training Accuracy: 0.9045\n",
      "Epoch [2430/20000], Loss: 0.2749, Training Accuracy: 0.9045\n",
      "Epoch [2440/20000], Loss: 0.2749, Training Accuracy: 0.9025\n",
      "Epoch [2450/20000], Loss: 0.2762, Training Accuracy: 0.9020\n",
      "Epoch [2460/20000], Loss: 0.2758, Training Accuracy: 0.9035\n",
      "Epoch [2470/20000], Loss: 0.2752, Training Accuracy: 0.9025\n",
      "Epoch [2480/20000], Loss: 0.2749, Training Accuracy: 0.9040\n",
      "Epoch [2490/20000], Loss: 0.2747, Training Accuracy: 0.9035\n",
      "Epoch [2500/20000], Loss: 0.2749, Training Accuracy: 0.9025\n",
      "Epoch [2510/20000], Loss: 0.2752, Training Accuracy: 0.9025\n",
      "Epoch [2520/20000], Loss: 0.2748, Training Accuracy: 0.9030\n",
      "Epoch [2530/20000], Loss: 0.2749, Training Accuracy: 0.9035\n",
      "Epoch [2540/20000], Loss: 0.2750, Training Accuracy: 0.9045\n",
      "Epoch [2550/20000], Loss: 0.2746, Training Accuracy: 0.9045\n",
      "Epoch [2560/20000], Loss: 0.2746, Training Accuracy: 0.9025\n",
      "Epoch [2570/20000], Loss: 0.2746, Training Accuracy: 0.9035\n",
      "Epoch [2580/20000], Loss: 0.2747, Training Accuracy: 0.9045\n",
      "Epoch [2590/20000], Loss: 0.2746, Training Accuracy: 0.9025\n",
      "Epoch [2600/20000], Loss: 0.2753, Training Accuracy: 0.9020\n",
      "Epoch [2610/20000], Loss: 0.2746, Training Accuracy: 0.9040\n",
      "Epoch [2620/20000], Loss: 0.2747, Training Accuracy: 0.9045\n",
      "Epoch [2630/20000], Loss: 0.2747, Training Accuracy: 0.9025\n",
      "Epoch [2640/20000], Loss: 0.2745, Training Accuracy: 0.9035\n",
      "Epoch [2650/20000], Loss: 0.2745, Training Accuracy: 0.9040\n",
      "Epoch [2660/20000], Loss: 0.2745, Training Accuracy: 0.9045\n",
      "Epoch [2670/20000], Loss: 0.2779, Training Accuracy: 0.9030\n",
      "Epoch [2680/20000], Loss: 0.2748, Training Accuracy: 0.9035\n",
      "Epoch [2690/20000], Loss: 0.2747, Training Accuracy: 0.9025\n",
      "Epoch [2700/20000], Loss: 0.2746, Training Accuracy: 0.9045\n",
      "Epoch [2710/20000], Loss: 0.2744, Training Accuracy: 0.9030\n",
      "Epoch [2720/20000], Loss: 0.2744, Training Accuracy: 0.9035\n",
      "Epoch [2730/20000], Loss: 0.2744, Training Accuracy: 0.9025\n",
      "Epoch [2740/20000], Loss: 0.2744, Training Accuracy: 0.9025\n",
      "Epoch [2750/20000], Loss: 0.2746, Training Accuracy: 0.9025\n",
      "Epoch [2760/20000], Loss: 0.2749, Training Accuracy: 0.9025\n",
      "Epoch [2770/20000], Loss: 0.2745, Training Accuracy: 0.9035\n",
      "Epoch [2780/20000], Loss: 0.2744, Training Accuracy: 0.9025\n",
      "Epoch [2790/20000], Loss: 0.2744, Training Accuracy: 0.9045\n",
      "Epoch [2800/20000], Loss: 0.2744, Training Accuracy: 0.9025\n",
      "Epoch [2810/20000], Loss: 0.2742, Training Accuracy: 0.9035\n",
      "Epoch [2820/20000], Loss: 0.2744, Training Accuracy: 0.9045\n",
      "Epoch [2830/20000], Loss: 0.2761, Training Accuracy: 0.9035\n",
      "Epoch [2840/20000], Loss: 0.2751, Training Accuracy: 0.9025\n",
      "Epoch [2850/20000], Loss: 0.2745, Training Accuracy: 0.9045\n",
      "Epoch [2860/20000], Loss: 0.2743, Training Accuracy: 0.9025\n",
      "Epoch [2870/20000], Loss: 0.2743, Training Accuracy: 0.9045\n",
      "Epoch [2880/20000], Loss: 0.2742, Training Accuracy: 0.9025\n",
      "Epoch [2890/20000], Loss: 0.2742, Training Accuracy: 0.9025\n",
      "Epoch [2900/20000], Loss: 0.2748, Training Accuracy: 0.9020\n",
      "Epoch [2910/20000], Loss: 0.2741, Training Accuracy: 0.9045\n",
      "Epoch [2920/20000], Loss: 0.2742, Training Accuracy: 0.9030\n",
      "Epoch [2930/20000], Loss: 0.2742, Training Accuracy: 0.9025\n",
      "Epoch [2940/20000], Loss: 0.2743, Training Accuracy: 0.9045\n",
      "Epoch [2950/20000], Loss: 0.2742, Training Accuracy: 0.9025\n",
      "Epoch [2960/20000], Loss: 0.2741, Training Accuracy: 0.9030\n",
      "Epoch [2970/20000], Loss: 0.2741, Training Accuracy: 0.9045\n",
      "Epoch [2980/20000], Loss: 0.2764, Training Accuracy: 0.9045\n",
      "Epoch [2990/20000], Loss: 0.2750, Training Accuracy: 0.9020\n",
      "Epoch [3000/20000], Loss: 0.2743, Training Accuracy: 0.9045\n",
      "Epoch [3010/20000], Loss: 0.2741, Training Accuracy: 0.9025\n",
      "Epoch [3020/20000], Loss: 0.2740, Training Accuracy: 0.9035\n",
      "Epoch [3030/20000], Loss: 0.2740, Training Accuracy: 0.9025\n",
      "Epoch [3040/20000], Loss: 0.2740, Training Accuracy: 0.9025\n",
      "Epoch [3050/20000], Loss: 0.2740, Training Accuracy: 0.9030\n",
      "Epoch [3060/20000], Loss: 0.2741, Training Accuracy: 0.9045\n",
      "Epoch [3070/20000], Loss: 0.2757, Training Accuracy: 0.9035\n",
      "Epoch [3080/20000], Loss: 0.2745, Training Accuracy: 0.9030\n",
      "Epoch [3090/20000], Loss: 0.2742, Training Accuracy: 0.9025\n",
      "Epoch [3100/20000], Loss: 0.2739, Training Accuracy: 0.9025\n",
      "Epoch [3110/20000], Loss: 0.2739, Training Accuracy: 0.9035\n",
      "Epoch [3120/20000], Loss: 0.2739, Training Accuracy: 0.9025\n",
      "Epoch [3130/20000], Loss: 0.2739, Training Accuracy: 0.9030\n",
      "Epoch [3140/20000], Loss: 0.2739, Training Accuracy: 0.9030\n",
      "Epoch [3150/20000], Loss: 0.2739, Training Accuracy: 0.9025\n",
      "Epoch [3160/20000], Loss: 0.2748, Training Accuracy: 0.9025\n",
      "Epoch [3170/20000], Loss: 0.2740, Training Accuracy: 0.9045\n",
      "Epoch [3180/20000], Loss: 0.2739, Training Accuracy: 0.9030\n",
      "Epoch [3190/20000], Loss: 0.2740, Training Accuracy: 0.9025\n",
      "Epoch [3200/20000], Loss: 0.2740, Training Accuracy: 0.9045\n",
      "Epoch [3210/20000], Loss: 0.2738, Training Accuracy: 0.9025\n",
      "Epoch [3220/20000], Loss: 0.2739, Training Accuracy: 0.9025\n",
      "Epoch [3230/20000], Loss: 0.2741, Training Accuracy: 0.9025\n",
      "Epoch [3240/20000], Loss: 0.2746, Training Accuracy: 0.9025\n",
      "Epoch [3250/20000], Loss: 0.2742, Training Accuracy: 0.9040\n",
      "Epoch [3260/20000], Loss: 0.2738, Training Accuracy: 0.9030\n",
      "Epoch [3270/20000], Loss: 0.2740, Training Accuracy: 0.9020\n",
      "Epoch [3280/20000], Loss: 0.2738, Training Accuracy: 0.9035\n",
      "Epoch [3290/20000], Loss: 0.2740, Training Accuracy: 0.9045\n",
      "Epoch [3300/20000], Loss: 0.2739, Training Accuracy: 0.9045\n",
      "Epoch [3310/20000], Loss: 0.2740, Training Accuracy: 0.9045\n",
      "Epoch [3320/20000], Loss: 0.2748, Training Accuracy: 0.9030\n",
      "Epoch [3330/20000], Loss: 0.2742, Training Accuracy: 0.9025\n",
      "Epoch [3340/20000], Loss: 0.2737, Training Accuracy: 0.9035\n",
      "Epoch [3350/20000], Loss: 0.2737, Training Accuracy: 0.9030\n",
      "Epoch [3360/20000], Loss: 0.2738, Training Accuracy: 0.9020\n",
      "Epoch [3370/20000], Loss: 0.2740, Training Accuracy: 0.9025\n",
      "Epoch [3380/20000], Loss: 0.2739, Training Accuracy: 0.9025\n",
      "Epoch [3390/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3400/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3410/20000], Loss: 0.2737, Training Accuracy: 0.9025\n",
      "Epoch [3420/20000], Loss: 0.2779, Training Accuracy: 0.9045\n",
      "Epoch [3430/20000], Loss: 0.2737, Training Accuracy: 0.9025\n",
      "Epoch [3440/20000], Loss: 0.2742, Training Accuracy: 0.9040\n",
      "Epoch [3450/20000], Loss: 0.2736, Training Accuracy: 0.9030\n",
      "Epoch [3460/20000], Loss: 0.2737, Training Accuracy: 0.9025\n",
      "Epoch [3470/20000], Loss: 0.2736, Training Accuracy: 0.9035\n",
      "Epoch [3480/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3490/20000], Loss: 0.2736, Training Accuracy: 0.9030\n",
      "Epoch [3500/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3510/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3520/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3530/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3540/20000], Loss: 0.2753, Training Accuracy: 0.9025\n",
      "Epoch [3550/20000], Loss: 0.2750, Training Accuracy: 0.9030\n",
      "Epoch [3560/20000], Loss: 0.2741, Training Accuracy: 0.9025\n",
      "Epoch [3570/20000], Loss: 0.2736, Training Accuracy: 0.9035\n",
      "Epoch [3580/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3590/20000], Loss: 0.2735, Training Accuracy: 0.9030\n",
      "Epoch [3600/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3610/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3620/20000], Loss: 0.2735, Training Accuracy: 0.9035\n",
      "Epoch [3630/20000], Loss: 0.2739, Training Accuracy: 0.9040\n",
      "Epoch [3640/20000], Loss: 0.2739, Training Accuracy: 0.9045\n",
      "Epoch [3650/20000], Loss: 0.2737, Training Accuracy: 0.9025\n",
      "Epoch [3660/20000], Loss: 0.2737, Training Accuracy: 0.9025\n",
      "Epoch [3670/20000], Loss: 0.2738, Training Accuracy: 0.9045\n",
      "Epoch [3680/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3690/20000], Loss: 0.2738, Training Accuracy: 0.9025\n",
      "Epoch [3700/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [3710/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [3720/20000], Loss: 0.2744, Training Accuracy: 0.9035\n",
      "Epoch [3730/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3740/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [3750/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [3760/20000], Loss: 0.2734, Training Accuracy: 0.9030\n",
      "Epoch [3770/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [3780/20000], Loss: 0.2748, Training Accuracy: 0.9020\n",
      "Epoch [3790/20000], Loss: 0.2740, Training Accuracy: 0.9040\n",
      "Epoch [3800/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3810/20000], Loss: 0.2734, Training Accuracy: 0.9035\n",
      "Epoch [3820/20000], Loss: 0.2734, Training Accuracy: 0.9030\n",
      "Epoch [3830/20000], Loss: 0.2735, Training Accuracy: 0.9020\n",
      "Epoch [3840/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3850/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [3860/20000], Loss: 0.2738, Training Accuracy: 0.9025\n",
      "Epoch [3870/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [3880/20000], Loss: 0.2740, Training Accuracy: 0.9040\n",
      "Epoch [3890/20000], Loss: 0.2735, Training Accuracy: 0.9025\n",
      "Epoch [3900/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [3910/20000], Loss: 0.2735, Training Accuracy: 0.9035\n",
      "Epoch [3920/20000], Loss: 0.2733, Training Accuracy: 0.9030\n",
      "Epoch [3930/20000], Loss: 0.2733, Training Accuracy: 0.9030\n",
      "Epoch [3940/20000], Loss: 0.2743, Training Accuracy: 0.9025\n",
      "Epoch [3950/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [3960/20000], Loss: 0.2733, Training Accuracy: 0.9030\n",
      "Epoch [3970/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [3980/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [3990/20000], Loss: 0.2734, Training Accuracy: 0.9035\n",
      "Epoch [4000/20000], Loss: 0.2733, Training Accuracy: 0.9035\n",
      "Epoch [4010/20000], Loss: 0.2733, Training Accuracy: 0.9030\n",
      "Epoch [4020/20000], Loss: 0.2734, Training Accuracy: 0.9035\n",
      "Epoch [4030/20000], Loss: 0.2751, Training Accuracy: 0.9035\n",
      "Epoch [4040/20000], Loss: 0.2741, Training Accuracy: 0.9020\n",
      "Epoch [4050/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [4060/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [4070/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [4080/20000], Loss: 0.2733, Training Accuracy: 0.9030\n",
      "Epoch [4090/20000], Loss: 0.2732, Training Accuracy: 0.9025\n",
      "Epoch [4100/20000], Loss: 0.2733, Training Accuracy: 0.9020\n",
      "Epoch [4110/20000], Loss: 0.2741, Training Accuracy: 0.9020\n",
      "Epoch [4120/20000], Loss: 0.2732, Training Accuracy: 0.9030\n",
      "Epoch [4130/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [4140/20000], Loss: 0.2732, Training Accuracy: 0.9025\n",
      "Epoch [4150/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [4160/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [4170/20000], Loss: 0.2733, Training Accuracy: 0.9035\n",
      "Epoch [4180/20000], Loss: 0.2744, Training Accuracy: 0.9025\n",
      "Epoch [4190/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [4200/20000], Loss: 0.2732, Training Accuracy: 0.9025\n",
      "Epoch [4210/20000], Loss: 0.2734, Training Accuracy: 0.9035\n",
      "Epoch [4220/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [4230/20000], Loss: 0.2732, Training Accuracy: 0.9025\n",
      "Epoch [4240/20000], Loss: 0.2732, Training Accuracy: 0.9035\n",
      "Epoch [4250/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [4260/20000], Loss: 0.2734, Training Accuracy: 0.9035\n",
      "Epoch [4270/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [4280/20000], Loss: 0.2734, Training Accuracy: 0.9020\n",
      "Epoch [4290/20000], Loss: 0.2734, Training Accuracy: 0.9045\n",
      "Epoch [4300/20000], Loss: 0.2732, Training Accuracy: 0.9030\n",
      "Epoch [4310/20000], Loss: 0.2736, Training Accuracy: 0.9020\n",
      "Epoch [4320/20000], Loss: 0.2767, Training Accuracy: 0.9035\n",
      "Epoch [4330/20000], Loss: 0.2741, Training Accuracy: 0.9040\n",
      "Epoch [4340/20000], Loss: 0.2733, Training Accuracy: 0.9045\n",
      "Epoch [4350/20000], Loss: 0.2731, Training Accuracy: 0.9030\n",
      "Epoch [4360/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [4370/20000], Loss: 0.2733, Training Accuracy: 0.9020\n",
      "Epoch [4380/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4390/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4400/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4410/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4420/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4430/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4440/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4450/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4460/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4470/20000], Loss: 0.2731, Training Accuracy: 0.9030\n",
      "Epoch [4480/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4490/20000], Loss: 0.2731, Training Accuracy: 0.9030\n",
      "Epoch [4500/20000], Loss: 0.2739, Training Accuracy: 0.9035\n",
      "Epoch [4510/20000], Loss: 0.2731, Training Accuracy: 0.9020\n",
      "Epoch [4520/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [4530/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4540/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4550/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4560/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4570/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4580/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4590/20000], Loss: 0.2735, Training Accuracy: 0.9045\n",
      "Epoch [4600/20000], Loss: 0.2734, Training Accuracy: 0.9035\n",
      "Epoch [4610/20000], Loss: 0.2734, Training Accuracy: 0.9020\n",
      "Epoch [4620/20000], Loss: 0.2733, Training Accuracy: 0.9035\n",
      "Epoch [4630/20000], Loss: 0.2732, Training Accuracy: 0.9020\n",
      "Epoch [4640/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4650/20000], Loss: 0.2731, Training Accuracy: 0.9030\n",
      "Epoch [4660/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4670/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4680/20000], Loss: 0.2733, Training Accuracy: 0.9045\n",
      "Epoch [4690/20000], Loss: 0.2740, Training Accuracy: 0.9045\n",
      "Epoch [4700/20000], Loss: 0.2735, Training Accuracy: 0.9020\n",
      "Epoch [4710/20000], Loss: 0.2732, Training Accuracy: 0.9030\n",
      "Epoch [4720/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4730/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4740/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4750/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4760/20000], Loss: 0.2730, Training Accuracy: 0.9030\n",
      "Epoch [4770/20000], Loss: 0.2732, Training Accuracy: 0.9035\n",
      "Epoch [4780/20000], Loss: 0.2741, Training Accuracy: 0.9040\n",
      "Epoch [4790/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [4800/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4810/20000], Loss: 0.2732, Training Accuracy: 0.9035\n",
      "Epoch [4820/20000], Loss: 0.2731, Training Accuracy: 0.9020\n",
      "Epoch [4830/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4840/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4850/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4860/20000], Loss: 0.2736, Training Accuracy: 0.9040\n",
      "Epoch [4870/20000], Loss: 0.2733, Training Accuracy: 0.9030\n",
      "Epoch [4880/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4890/20000], Loss: 0.2731, Training Accuracy: 0.9030\n",
      "Epoch [4900/20000], Loss: 0.2730, Training Accuracy: 0.9020\n",
      "Epoch [4910/20000], Loss: 0.2730, Training Accuracy: 0.9030\n",
      "Epoch [4920/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [4930/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [4940/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [4950/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4960/20000], Loss: 0.2744, Training Accuracy: 0.9020\n",
      "Epoch [4970/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [4980/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [4990/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5000/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [5010/20000], Loss: 0.2730, Training Accuracy: 0.9020\n",
      "Epoch [5020/20000], Loss: 0.2730, Training Accuracy: 0.9020\n",
      "Epoch [5030/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5040/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5050/20000], Loss: 0.2734, Training Accuracy: 0.9045\n",
      "Epoch [5060/20000], Loss: 0.2732, Training Accuracy: 0.9030\n",
      "Epoch [5070/20000], Loss: 0.2732, Training Accuracy: 0.9020\n",
      "Epoch [5080/20000], Loss: 0.2731, Training Accuracy: 0.9030\n",
      "Epoch [5090/20000], Loss: 0.2730, Training Accuracy: 0.9020\n",
      "Epoch [5100/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5110/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5120/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5130/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [5140/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [5150/20000], Loss: 0.2729, Training Accuracy: 0.9030\n",
      "Epoch [5160/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [5170/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5180/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [5190/20000], Loss: 0.2739, Training Accuracy: 0.9025\n",
      "Epoch [5200/20000], Loss: 0.2730, Training Accuracy: 0.9035\n",
      "Epoch [5210/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5220/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [5230/20000], Loss: 0.2729, Training Accuracy: 0.9030\n",
      "Epoch [5240/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5250/20000], Loss: 0.2729, Training Accuracy: 0.9020\n",
      "Epoch [5260/20000], Loss: 0.2733, Training Accuracy: 0.9020\n",
      "Epoch [5270/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5280/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [5290/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [5300/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5310/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5320/20000], Loss: 0.2738, Training Accuracy: 0.9025\n",
      "Epoch [5330/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5340/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [5350/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5360/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [5370/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5380/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5390/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5400/20000], Loss: 0.2729, Training Accuracy: 0.9035\n",
      "Epoch [5410/20000], Loss: 0.2753, Training Accuracy: 0.9030\n",
      "Epoch [5420/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [5430/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5440/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5450/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5460/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5470/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5480/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5490/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5500/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5510/20000], Loss: 0.2732, Training Accuracy: 0.9020\n",
      "Epoch [5520/20000], Loss: 0.2733, Training Accuracy: 0.9025\n",
      "Epoch [5530/20000], Loss: 0.2732, Training Accuracy: 0.9035\n",
      "Epoch [5540/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5550/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5560/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5570/20000], Loss: 0.2728, Training Accuracy: 0.9020\n",
      "Epoch [5580/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [5590/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [5600/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5610/20000], Loss: 0.2730, Training Accuracy: 0.9035\n",
      "Epoch [5620/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [5630/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5640/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [5650/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5660/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5670/20000], Loss: 0.2733, Training Accuracy: 0.9045\n",
      "Epoch [5680/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5690/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [5700/20000], Loss: 0.2730, Training Accuracy: 0.9035\n",
      "Epoch [5710/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5720/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5730/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5740/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5750/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5760/20000], Loss: 0.2741, Training Accuracy: 0.9025\n",
      "Epoch [5770/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [5780/20000], Loss: 0.2729, Training Accuracy: 0.9035\n",
      "Epoch [5790/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [5800/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5810/20000], Loss: 0.2727, Training Accuracy: 0.9020\n",
      "Epoch [5820/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5830/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5840/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5850/20000], Loss: 0.2729, Training Accuracy: 0.9035\n",
      "Epoch [5860/20000], Loss: 0.2734, Training Accuracy: 0.9045\n",
      "Epoch [5870/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [5880/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5890/20000], Loss: 0.2728, Training Accuracy: 0.9030\n",
      "Epoch [5900/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [5910/20000], Loss: 0.2730, Training Accuracy: 0.9020\n",
      "Epoch [5920/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5930/20000], Loss: 0.2727, Training Accuracy: 0.9030\n",
      "Epoch [5940/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [5950/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [5960/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [5970/20000], Loss: 0.2732, Training Accuracy: 0.9025\n",
      "Epoch [5980/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [5990/20000], Loss: 0.2729, Training Accuracy: 0.9035\n",
      "Epoch [6000/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6010/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [6020/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [6030/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6040/20000], Loss: 0.2728, Training Accuracy: 0.9020\n",
      "Epoch [6050/20000], Loss: 0.2734, Training Accuracy: 0.9020\n",
      "Epoch [6060/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [6070/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6080/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6090/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6100/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6110/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6120/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6130/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [6140/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6150/20000], Loss: 0.2728, Training Accuracy: 0.9020\n",
      "Epoch [6160/20000], Loss: 0.2729, Training Accuracy: 0.9020\n",
      "Epoch [6170/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6180/20000], Loss: 0.2730, Training Accuracy: 0.9040\n",
      "Epoch [6190/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6200/20000], Loss: 0.2729, Training Accuracy: 0.9020\n",
      "Epoch [6210/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6220/20000], Loss: 0.2728, Training Accuracy: 0.9030\n",
      "Epoch [6230/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [6240/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6250/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [6260/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6270/20000], Loss: 0.2728, Training Accuracy: 0.9035\n",
      "Epoch [6280/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6290/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6300/20000], Loss: 0.2728, Training Accuracy: 0.9020\n",
      "Epoch [6310/20000], Loss: 0.2731, Training Accuracy: 0.9020\n",
      "Epoch [6320/20000], Loss: 0.2729, Training Accuracy: 0.9035\n",
      "Epoch [6330/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6340/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6350/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6360/20000], Loss: 0.2728, Training Accuracy: 0.9030\n",
      "Epoch [6370/20000], Loss: 0.2729, Training Accuracy: 0.9030\n",
      "Epoch [6380/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6390/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6400/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6410/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6420/20000], Loss: 0.2736, Training Accuracy: 0.9025\n",
      "Epoch [6430/20000], Loss: 0.2726, Training Accuracy: 0.9030\n",
      "Epoch [6440/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6450/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6460/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6470/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6480/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6490/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6500/20000], Loss: 0.2728, Training Accuracy: 0.9035\n",
      "Epoch [6510/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6520/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6530/20000], Loss: 0.2727, Training Accuracy: 0.9030\n",
      "Epoch [6540/20000], Loss: 0.2736, Training Accuracy: 0.9045\n",
      "Epoch [6550/20000], Loss: 0.2727, Training Accuracy: 0.9020\n",
      "Epoch [6560/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6570/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6580/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6590/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6600/20000], Loss: 0.2730, Training Accuracy: 0.9035\n",
      "Epoch [6610/20000], Loss: 0.2737, Training Accuracy: 0.9040\n",
      "Epoch [6620/20000], Loss: 0.2727, Training Accuracy: 0.9020\n",
      "Epoch [6630/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6640/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6650/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6660/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6670/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6680/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6690/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6700/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [6710/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [6720/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6730/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [6740/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [6750/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6760/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6770/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6780/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [6790/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6800/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [6810/20000], Loss: 0.2731, Training Accuracy: 0.9020\n",
      "Epoch [6820/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [6830/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6840/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6850/20000], Loss: 0.2724, Training Accuracy: 0.9020\n",
      "Epoch [6860/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6870/20000], Loss: 0.2724, Training Accuracy: 0.9020\n",
      "Epoch [6880/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [6890/20000], Loss: 0.2738, Training Accuracy: 0.9045\n",
      "Epoch [6900/20000], Loss: 0.2730, Training Accuracy: 0.9020\n",
      "Epoch [6910/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [6920/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6930/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [6940/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [6950/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [6960/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [6970/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [6980/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [6990/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7000/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [7010/20000], Loss: 0.2731, Training Accuracy: 0.9025\n",
      "Epoch [7020/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7030/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7040/20000], Loss: 0.2724, Training Accuracy: 0.9020\n",
      "Epoch [7050/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7060/20000], Loss: 0.2724, Training Accuracy: 0.9020\n",
      "Epoch [7070/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [7080/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [7090/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7100/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7110/20000], Loss: 0.2727, Training Accuracy: 0.9020\n",
      "Epoch [7120/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7130/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7140/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [7150/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [7160/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [7170/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [7180/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7190/20000], Loss: 0.2724, Training Accuracy: 0.9020\n",
      "Epoch [7200/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7210/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7220/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7230/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [7240/20000], Loss: 0.2724, Training Accuracy: 0.9020\n",
      "Epoch [7250/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7260/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [7270/20000], Loss: 0.2734, Training Accuracy: 0.9025\n",
      "Epoch [7280/20000], Loss: 0.2730, Training Accuracy: 0.9030\n",
      "Epoch [7290/20000], Loss: 0.2726, Training Accuracy: 0.9020\n",
      "Epoch [7300/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7310/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7320/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7330/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7340/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7350/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [7360/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7370/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7380/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [7390/20000], Loss: 0.2730, Training Accuracy: 0.9025\n",
      "Epoch [7400/20000], Loss: 0.2727, Training Accuracy: 0.9025\n",
      "Epoch [7410/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7420/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7430/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7440/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7450/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7460/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7470/20000], Loss: 0.2731, Training Accuracy: 0.9035\n",
      "Epoch [7480/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7490/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7500/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7510/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7520/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7530/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7540/20000], Loss: 0.2728, Training Accuracy: 0.9030\n",
      "Epoch [7550/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [7560/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [7570/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7580/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7590/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7600/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7610/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7620/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7630/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7640/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7650/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7660/20000], Loss: 0.2730, Training Accuracy: 0.9035\n",
      "Epoch [7670/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7680/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7690/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7700/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7710/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7720/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7730/20000], Loss: 0.2737, Training Accuracy: 0.9045\n",
      "Epoch [7740/20000], Loss: 0.2729, Training Accuracy: 0.9025\n",
      "Epoch [7750/20000], Loss: 0.2725, Training Accuracy: 0.9020\n",
      "Epoch [7760/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7770/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7780/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7790/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7800/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7810/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7820/20000], Loss: 0.2731, Training Accuracy: 0.9045\n",
      "Epoch [7830/20000], Loss: 0.2727, Training Accuracy: 0.9045\n",
      "Epoch [7840/20000], Loss: 0.2728, Training Accuracy: 0.9025\n",
      "Epoch [7850/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7860/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7870/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7880/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7890/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7900/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7910/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [7920/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [7930/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7940/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [7950/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [7960/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7970/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [7980/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [7990/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [8000/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [8010/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [8020/20000], Loss: 0.2725, Training Accuracy: 0.9035\n",
      "Epoch [8030/20000], Loss: 0.2727, Training Accuracy: 0.9020\n",
      "Epoch [8040/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [8050/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [8060/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8070/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [8080/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [8090/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8100/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8110/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8120/20000], Loss: 0.2725, Training Accuracy: 0.9025\n",
      "Epoch [8130/20000], Loss: 0.2728, Training Accuracy: 0.9030\n",
      "Epoch [8140/20000], Loss: 0.2725, Training Accuracy: 0.9040\n",
      "Epoch [8150/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8160/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8170/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [8180/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [8190/20000], Loss: 0.2731, Training Accuracy: 0.9050\n",
      "Epoch [8200/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [8210/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [8220/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [8230/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8240/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8250/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8260/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8270/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [8280/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [8290/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [8300/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8310/20000], Loss: 0.2723, Training Accuracy: 0.9035\n",
      "Epoch [8320/20000], Loss: 0.2723, Training Accuracy: 0.9035\n",
      "Epoch [8330/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8340/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8350/20000], Loss: 0.2726, Training Accuracy: 0.9050\n",
      "Epoch [8360/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [8370/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [8380/20000], Loss: 0.2722, Training Accuracy: 0.9035\n",
      "Epoch [8390/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8400/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8410/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8420/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8430/20000], Loss: 0.2725, Training Accuracy: 0.9050\n",
      "Epoch [8440/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [8450/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [8460/20000], Loss: 0.2723, Training Accuracy: 0.9040\n",
      "Epoch [8470/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8480/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8490/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [8500/20000], Loss: 0.2724, Training Accuracy: 0.9045\n",
      "Epoch [8510/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8520/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [8530/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8540/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8550/20000], Loss: 0.2722, Training Accuracy: 0.9045\n",
      "Epoch [8560/20000], Loss: 0.2728, Training Accuracy: 0.9050\n",
      "Epoch [8570/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8580/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [8590/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [8600/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [8610/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8620/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [8630/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8640/20000], Loss: 0.2733, Training Accuracy: 0.9045\n",
      "Epoch [8650/20000], Loss: 0.2725, Training Accuracy: 0.9045\n",
      "Epoch [8660/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [8670/20000], Loss: 0.2721, Training Accuracy: 0.9030\n",
      "Epoch [8680/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [8690/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8700/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8710/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8720/20000], Loss: 0.2721, Training Accuracy: 0.9045\n",
      "Epoch [8730/20000], Loss: 0.2724, Training Accuracy: 0.9050\n",
      "Epoch [8740/20000], Loss: 0.2721, Training Accuracy: 0.9030\n",
      "Epoch [8750/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [8760/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [8770/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [8780/20000], Loss: 0.2722, Training Accuracy: 0.9045\n",
      "Epoch [8790/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8800/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8810/20000], Loss: 0.2723, Training Accuracy: 0.9025\n",
      "Epoch [8820/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [8830/20000], Loss: 0.2721, Training Accuracy: 0.9045\n",
      "Epoch [8840/20000], Loss: 0.2723, Training Accuracy: 0.9050\n",
      "Epoch [8850/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8860/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8870/20000], Loss: 0.2723, Training Accuracy: 0.9050\n",
      "Epoch [8880/20000], Loss: 0.2733, Training Accuracy: 0.9055\n",
      "Epoch [8890/20000], Loss: 0.2726, Training Accuracy: 0.9025\n",
      "Epoch [8900/20000], Loss: 0.2722, Training Accuracy: 0.9045\n",
      "Epoch [8910/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8920/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8930/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8940/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [8950/20000], Loss: 0.2720, Training Accuracy: 0.9040\n",
      "Epoch [8960/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [8970/20000], Loss: 0.2720, Training Accuracy: 0.9040\n",
      "Epoch [8980/20000], Loss: 0.2724, Training Accuracy: 0.9050\n",
      "Epoch [8990/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9000/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [9010/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9020/20000], Loss: 0.2719, Training Accuracy: 0.9035\n",
      "Epoch [9030/20000], Loss: 0.2726, Training Accuracy: 0.9050\n",
      "Epoch [9040/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [9050/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [9060/20000], Loss: 0.2722, Training Accuracy: 0.9050\n",
      "Epoch [9070/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9080/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9090/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9100/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9110/20000], Loss: 0.2719, Training Accuracy: 0.9035\n",
      "Epoch [9120/20000], Loss: 0.2720, Training Accuracy: 0.9045\n",
      "Epoch [9130/20000], Loss: 0.2719, Training Accuracy: 0.9035\n",
      "Epoch [9140/20000], Loss: 0.2721, Training Accuracy: 0.9050\n",
      "Epoch [9150/20000], Loss: 0.2720, Training Accuracy: 0.9045\n",
      "Epoch [9160/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9170/20000], Loss: 0.2721, Training Accuracy: 0.9050\n",
      "Epoch [9180/20000], Loss: 0.2721, Training Accuracy: 0.9045\n",
      "Epoch [9190/20000], Loss: 0.2722, Training Accuracy: 0.9020\n",
      "Epoch [9200/20000], Loss: 0.2719, Training Accuracy: 0.9040\n",
      "Epoch [9210/20000], Loss: 0.2721, Training Accuracy: 0.9045\n",
      "Epoch [9220/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9230/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [9240/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [9250/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9260/20000], Loss: 0.2724, Training Accuracy: 0.9025\n",
      "Epoch [9270/20000], Loss: 0.2719, Training Accuracy: 0.9030\n",
      "Epoch [9280/20000], Loss: 0.2721, Training Accuracy: 0.9045\n",
      "Epoch [9290/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9300/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [9310/20000], Loss: 0.2718, Training Accuracy: 0.9025\n",
      "Epoch [9320/20000], Loss: 0.2718, Training Accuracy: 0.9035\n",
      "Epoch [9330/20000], Loss: 0.2722, Training Accuracy: 0.9050\n",
      "Epoch [9340/20000], Loss: 0.2719, Training Accuracy: 0.9035\n",
      "Epoch [9350/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [9360/20000], Loss: 0.2721, Training Accuracy: 0.9050\n",
      "Epoch [9370/20000], Loss: 0.2718, Training Accuracy: 0.9025\n",
      "Epoch [9380/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9390/20000], Loss: 0.2719, Training Accuracy: 0.9050\n",
      "Epoch [9400/20000], Loss: 0.2719, Training Accuracy: 0.9045\n",
      "Epoch [9410/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9420/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [9430/20000], Loss: 0.2718, Training Accuracy: 0.9020\n",
      "Epoch [9440/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [9450/20000], Loss: 0.2723, Training Accuracy: 0.9020\n",
      "Epoch [9460/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [9470/20000], Loss: 0.2719, Training Accuracy: 0.9050\n",
      "Epoch [9480/20000], Loss: 0.2719, Training Accuracy: 0.9045\n",
      "Epoch [9490/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [9500/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [9510/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [9520/20000], Loss: 0.2721, Training Accuracy: 0.9050\n",
      "Epoch [9530/20000], Loss: 0.2719, Training Accuracy: 0.9045\n",
      "Epoch [9540/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [9550/20000], Loss: 0.2722, Training Accuracy: 0.9050\n",
      "Epoch [9560/20000], Loss: 0.2718, Training Accuracy: 0.9035\n",
      "Epoch [9570/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [9580/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [9590/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [9600/20000], Loss: 0.2718, Training Accuracy: 0.9040\n",
      "Epoch [9610/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [9620/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9630/20000], Loss: 0.2721, Training Accuracy: 0.9050\n",
      "Epoch [9640/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [9650/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [9660/20000], Loss: 0.2718, Training Accuracy: 0.9025\n",
      "Epoch [9670/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9680/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [9690/20000], Loss: 0.2719, Training Accuracy: 0.9050\n",
      "Epoch [9700/20000], Loss: 0.2721, Training Accuracy: 0.9020\n",
      "Epoch [9710/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9720/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [9730/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [9740/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [9750/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [9760/20000], Loss: 0.2722, Training Accuracy: 0.9050\n",
      "Epoch [9770/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9780/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [9790/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9800/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [9810/20000], Loss: 0.2723, Training Accuracy: 0.9055\n",
      "Epoch [9820/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [9830/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [9840/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [9850/20000], Loss: 0.2719, Training Accuracy: 0.9020\n",
      "Epoch [9860/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9870/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [9880/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [9890/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9900/20000], Loss: 0.2721, Training Accuracy: 0.9025\n",
      "Epoch [9910/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [9920/20000], Loss: 0.2719, Training Accuracy: 0.9050\n",
      "Epoch [9930/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [9940/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [9950/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [9960/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [9970/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [9980/20000], Loss: 0.2722, Training Accuracy: 0.9055\n",
      "Epoch [9990/20000], Loss: 0.2717, Training Accuracy: 0.9030\n",
      "Epoch [10000/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [10010/20000], Loss: 0.2719, Training Accuracy: 0.9050\n",
      "Epoch [10020/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10030/20000], Loss: 0.2718, Training Accuracy: 0.9025\n",
      "Epoch [10040/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [10050/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10060/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [10070/20000], Loss: 0.2721, Training Accuracy: 0.9055\n",
      "Epoch [10080/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10090/20000], Loss: 0.2720, Training Accuracy: 0.9020\n",
      "Epoch [10100/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10110/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [10120/20000], Loss: 0.2721, Training Accuracy: 0.9055\n",
      "Epoch [10130/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10140/20000], Loss: 0.2720, Training Accuracy: 0.9025\n",
      "Epoch [10150/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [10160/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [10170/20000], Loss: 0.2720, Training Accuracy: 0.9050\n",
      "Epoch [10180/20000], Loss: 0.2717, Training Accuracy: 0.9025\n",
      "Epoch [10190/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [10200/20000], Loss: 0.2718, Training Accuracy: 0.9050\n",
      "Epoch [10210/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10220/20000], Loss: 0.2718, Training Accuracy: 0.9025\n",
      "Epoch [10230/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [10240/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10250/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [10260/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10270/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [10280/20000], Loss: 0.2717, Training Accuracy: 0.9030\n",
      "Epoch [10290/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10300/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10310/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [10320/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10330/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [10340/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10350/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10360/20000], Loss: 0.2719, Training Accuracy: 0.9055\n",
      "Epoch [10370/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10380/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [10390/20000], Loss: 0.2720, Training Accuracy: 0.9055\n",
      "Epoch [10400/20000], Loss: 0.2716, Training Accuracy: 0.9040\n",
      "Epoch [10410/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10420/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10430/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10440/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10450/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [10460/20000], Loss: 0.2717, Training Accuracy: 0.9050\n",
      "Epoch [10470/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10480/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [10490/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10500/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10510/20000], Loss: 0.2720, Training Accuracy: 0.9055\n",
      "Epoch [10520/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10530/20000], Loss: 0.2718, Training Accuracy: 0.9040\n",
      "Epoch [10540/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10550/20000], Loss: 0.2717, Training Accuracy: 0.9035\n",
      "Epoch [10560/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [10570/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [10580/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10590/20000], Loss: 0.2718, Training Accuracy: 0.9030\n",
      "Epoch [10600/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10610/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10620/20000], Loss: 0.2721, Training Accuracy: 0.9055\n",
      "Epoch [10630/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10640/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10650/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10660/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10670/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10680/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [10690/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10700/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10710/20000], Loss: 0.2719, Training Accuracy: 0.9035\n",
      "Epoch [10720/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10730/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [10740/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10750/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10760/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [10770/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [10780/20000], Loss: 0.2717, Training Accuracy: 0.9040\n",
      "Epoch [10790/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10800/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10810/20000], Loss: 0.2719, Training Accuracy: 0.9025\n",
      "Epoch [10820/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10830/20000], Loss: 0.2716, Training Accuracy: 0.9055\n",
      "Epoch [10840/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10850/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [10860/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10870/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [10880/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [10890/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10900/20000], Loss: 0.2717, Training Accuracy: 0.9055\n",
      "Epoch [10910/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10920/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10930/20000], Loss: 0.2716, Training Accuracy: 0.9050\n",
      "Epoch [10940/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10950/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [10960/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [10970/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [10980/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [10990/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11000/20000], Loss: 0.2722, Training Accuracy: 0.9025\n",
      "Epoch [11010/20000], Loss: 0.2718, Training Accuracy: 0.9055\n",
      "Epoch [11020/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [11030/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11040/20000], Loss: 0.2714, Training Accuracy: 0.9055\n",
      "Epoch [11050/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [11060/20000], Loss: 0.2714, Training Accuracy: 0.9055\n",
      "Epoch [11070/20000], Loss: 0.2714, Training Accuracy: 0.9055\n",
      "Epoch [11080/20000], Loss: 0.2716, Training Accuracy: 0.9055\n",
      "Epoch [11090/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11100/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [11110/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [11120/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [11130/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11140/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [11150/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [11160/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11170/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [11180/20000], Loss: 0.2717, Training Accuracy: 0.9045\n",
      "Epoch [11190/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11200/20000], Loss: 0.2716, Training Accuracy: 0.9055\n",
      "Epoch [11210/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11220/20000], Loss: 0.2716, Training Accuracy: 0.9055\n",
      "Epoch [11230/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11240/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [11250/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11260/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11270/20000], Loss: 0.2717, Training Accuracy: 0.9055\n",
      "Epoch [11280/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11290/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [11300/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [11310/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11320/20000], Loss: 0.2719, Training Accuracy: 0.9040\n",
      "Epoch [11330/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11340/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [11350/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11360/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [11370/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11380/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11390/20000], Loss: 0.2718, Training Accuracy: 0.9045\n",
      "Epoch [11400/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11410/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11420/20000], Loss: 0.2713, Training Accuracy: 0.9045\n",
      "Epoch [11430/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11440/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11450/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [11460/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11470/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [11480/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11490/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11500/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [11510/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11520/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [11530/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11540/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [11550/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11560/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11570/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11580/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [11590/20000], Loss: 0.2715, Training Accuracy: 0.9055\n",
      "Epoch [11600/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [11610/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [11620/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [11630/20000], Loss: 0.2714, Training Accuracy: 0.9055\n",
      "Epoch [11640/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11650/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [11660/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11670/20000], Loss: 0.2716, Training Accuracy: 0.9055\n",
      "Epoch [11680/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11690/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [11700/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11710/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [11720/20000], Loss: 0.2716, Training Accuracy: 0.9055\n",
      "Epoch [11730/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11740/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11750/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11760/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [11770/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [11780/20000], Loss: 0.2714, Training Accuracy: 0.9055\n",
      "Epoch [11790/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [11800/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [11810/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [11820/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [11830/20000], Loss: 0.2713, Training Accuracy: 0.9045\n",
      "Epoch [11840/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11850/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11860/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [11870/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [11880/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [11890/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [11900/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [11910/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [11920/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11930/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [11940/20000], Loss: 0.2715, Training Accuracy: 0.9050\n",
      "Epoch [11950/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [11960/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [11970/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [11980/20000], Loss: 0.2715, Training Accuracy: 0.9045\n",
      "Epoch [11990/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12000/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [12010/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [12020/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [12030/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12040/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [12050/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12060/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [12070/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12080/20000], Loss: 0.2714, Training Accuracy: 0.9045\n",
      "Epoch [12090/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12100/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [12110/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12120/20000], Loss: 0.2713, Training Accuracy: 0.9045\n",
      "Epoch [12130/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12140/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [12150/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12160/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [12170/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12180/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [12190/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12200/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12210/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12220/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12230/20000], Loss: 0.2713, Training Accuracy: 0.9050\n",
      "Epoch [12240/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12250/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12260/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12270/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12280/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12290/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12300/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [12310/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12320/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [12330/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12340/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [12350/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12360/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [12370/20000], Loss: 0.2716, Training Accuracy: 0.9045\n",
      "Epoch [12380/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12390/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [12400/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12410/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12420/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12430/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12440/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12450/20000], Loss: 0.2712, Training Accuracy: 0.9050\n",
      "Epoch [12460/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [12470/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12480/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12490/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12500/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12510/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12520/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12530/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12540/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12550/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12560/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12570/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12580/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [12590/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12600/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [12610/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12620/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [12630/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [12640/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12650/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12660/20000], Loss: 0.2712, Training Accuracy: 0.9055\n",
      "Epoch [12670/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12680/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12690/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12700/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12710/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [12720/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [12730/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12740/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12750/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12760/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12770/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12780/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12790/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [12800/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [12810/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12820/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [12830/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [12840/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12850/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [12860/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [12870/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [12880/20000], Loss: 0.2706, Training Accuracy: 0.9050\n",
      "Epoch [12890/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [12900/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [12910/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [12920/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [12930/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12940/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [12950/20000], Loss: 0.2711, Training Accuracy: 0.9055\n",
      "Epoch [12960/20000], Loss: 0.2706, Training Accuracy: 0.9050\n",
      "Epoch [12970/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [12980/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [12990/20000], Loss: 0.2709, Training Accuracy: 0.9050\n",
      "Epoch [13000/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13010/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [13020/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [13030/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [13040/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13050/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13060/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13070/20000], Loss: 0.2713, Training Accuracy: 0.9055\n",
      "Epoch [13080/20000], Loss: 0.2725, Training Accuracy: 0.9070\n",
      "Epoch [13090/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [13100/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [13110/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13120/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13130/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13140/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13150/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13160/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13170/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13180/20000], Loss: 0.2709, Training Accuracy: 0.9060\n",
      "Epoch [13190/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13200/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13210/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [13220/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13230/20000], Loss: 0.2706, Training Accuracy: 0.9050\n",
      "Epoch [13240/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [13250/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13260/20000], Loss: 0.2710, Training Accuracy: 0.9050\n",
      "Epoch [13270/20000], Loss: 0.2705, Training Accuracy: 0.9050\n",
      "Epoch [13280/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13290/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13300/20000], Loss: 0.2708, Training Accuracy: 0.9050\n",
      "Epoch [13310/20000], Loss: 0.2705, Training Accuracy: 0.9050\n",
      "Epoch [13320/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [13330/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13340/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [13350/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [13360/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [13370/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [13380/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13390/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [13400/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [13410/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [13420/20000], Loss: 0.2711, Training Accuracy: 0.9050\n",
      "Epoch [13430/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [13440/20000], Loss: 0.2705, Training Accuracy: 0.9050\n",
      "Epoch [13450/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [13460/20000], Loss: 0.2707, Training Accuracy: 0.9050\n",
      "Epoch [13470/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13480/20000], Loss: 0.2708, Training Accuracy: 0.9045\n",
      "Epoch [13490/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [13500/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13510/20000], Loss: 0.2705, Training Accuracy: 0.9050\n",
      "Epoch [13520/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13530/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [13540/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13550/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13560/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13570/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [13580/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13590/20000], Loss: 0.2704, Training Accuracy: 0.9050\n",
      "Epoch [13600/20000], Loss: 0.2709, Training Accuracy: 0.9055\n",
      "Epoch [13610/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13620/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13630/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [13640/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13650/20000], Loss: 0.2707, Training Accuracy: 0.9045\n",
      "Epoch [13660/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13670/20000], Loss: 0.2708, Training Accuracy: 0.9055\n",
      "Epoch [13680/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13690/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13700/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13710/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13720/20000], Loss: 0.2704, Training Accuracy: 0.9050\n",
      "Epoch [13730/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13740/20000], Loss: 0.2707, Training Accuracy: 0.9055\n",
      "Epoch [13750/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13760/20000], Loss: 0.2705, Training Accuracy: 0.9045\n",
      "Epoch [13770/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [13780/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13790/20000], Loss: 0.2705, Training Accuracy: 0.9045\n",
      "Epoch [13800/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13810/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13820/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13830/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [13840/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13850/20000], Loss: 0.2706, Training Accuracy: 0.9045\n",
      "Epoch [13860/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13870/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13880/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13890/20000], Loss: 0.2708, Training Accuracy: 0.9040\n",
      "Epoch [13900/20000], Loss: 0.2702, Training Accuracy: 0.9050\n",
      "Epoch [13910/20000], Loss: 0.2704, Training Accuracy: 0.9050\n",
      "Epoch [13920/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13930/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13940/20000], Loss: 0.2710, Training Accuracy: 0.9055\n",
      "Epoch [13950/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [13960/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [13970/20000], Loss: 0.2704, Training Accuracy: 0.9065\n",
      "Epoch [13980/20000], Loss: 0.2702, Training Accuracy: 0.9050\n",
      "Epoch [13990/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [14000/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14010/20000], Loss: 0.2704, Training Accuracy: 0.9060\n",
      "Epoch [14020/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14030/20000], Loss: 0.2703, Training Accuracy: 0.9050\n",
      "Epoch [14040/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14050/20000], Loss: 0.2705, Training Accuracy: 0.9055\n",
      "Epoch [14060/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14070/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14080/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [14090/20000], Loss: 0.2704, Training Accuracy: 0.9040\n",
      "Epoch [14100/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14110/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [14120/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14130/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [14140/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14150/20000], Loss: 0.2704, Training Accuracy: 0.9045\n",
      "Epoch [14160/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14170/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14180/20000], Loss: 0.2706, Training Accuracy: 0.9070\n",
      "Epoch [14190/20000], Loss: 0.2701, Training Accuracy: 0.9050\n",
      "Epoch [14200/20000], Loss: 0.2700, Training Accuracy: 0.9060\n",
      "Epoch [14210/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14220/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [14230/20000], Loss: 0.2703, Training Accuracy: 0.9040\n",
      "Epoch [14240/20000], Loss: 0.2701, Training Accuracy: 0.9050\n",
      "Epoch [14250/20000], Loss: 0.2702, Training Accuracy: 0.9050\n",
      "Epoch [14260/20000], Loss: 0.2700, Training Accuracy: 0.9050\n",
      "Epoch [14270/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14280/20000], Loss: 0.2702, Training Accuracy: 0.9050\n",
      "Epoch [14290/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14300/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14310/20000], Loss: 0.2700, Training Accuracy: 0.9065\n",
      "Epoch [14320/20000], Loss: 0.2702, Training Accuracy: 0.9045\n",
      "Epoch [14330/20000], Loss: 0.2701, Training Accuracy: 0.9060\n",
      "Epoch [14340/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14350/20000], Loss: 0.2701, Training Accuracy: 0.9065\n",
      "Epoch [14360/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14370/20000], Loss: 0.2705, Training Accuracy: 0.9075\n",
      "Epoch [14380/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14390/20000], Loss: 0.2701, Training Accuracy: 0.9050\n",
      "Epoch [14400/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14410/20000], Loss: 0.2700, Training Accuracy: 0.9050\n",
      "Epoch [14420/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [14430/20000], Loss: 0.2700, Training Accuracy: 0.9060\n",
      "Epoch [14440/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14450/20000], Loss: 0.2699, Training Accuracy: 0.9060\n",
      "Epoch [14460/20000], Loss: 0.2702, Training Accuracy: 0.9060\n",
      "Epoch [14470/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14480/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [14490/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14500/20000], Loss: 0.2700, Training Accuracy: 0.9050\n",
      "Epoch [14510/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14520/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14530/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14540/20000], Loss: 0.2706, Training Accuracy: 0.9075\n",
      "Epoch [14550/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14560/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [14570/20000], Loss: 0.2702, Training Accuracy: 0.9065\n",
      "Epoch [14580/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14590/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14600/20000], Loss: 0.2699, Training Accuracy: 0.9050\n",
      "Epoch [14610/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14620/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14630/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14640/20000], Loss: 0.2699, Training Accuracy: 0.9060\n",
      "Epoch [14650/20000], Loss: 0.2704, Training Accuracy: 0.9075\n",
      "Epoch [14660/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14670/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14680/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [14690/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14700/20000], Loss: 0.2701, Training Accuracy: 0.9050\n",
      "Epoch [14710/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14720/20000], Loss: 0.2702, Training Accuracy: 0.9065\n",
      "Epoch [14730/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14740/20000], Loss: 0.2698, Training Accuracy: 0.9060\n",
      "Epoch [14750/20000], Loss: 0.2699, Training Accuracy: 0.9045\n",
      "Epoch [14760/20000], Loss: 0.2699, Training Accuracy: 0.9045\n",
      "Epoch [14770/20000], Loss: 0.2704, Training Accuracy: 0.9070\n",
      "Epoch [14780/20000], Loss: 0.2703, Training Accuracy: 0.9055\n",
      "Epoch [14790/20000], Loss: 0.2701, Training Accuracy: 0.9060\n",
      "Epoch [14800/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14810/20000], Loss: 0.2698, Training Accuracy: 0.9050\n",
      "Epoch [14820/20000], Loss: 0.2701, Training Accuracy: 0.9070\n",
      "Epoch [14830/20000], Loss: 0.2698, Training Accuracy: 0.9060\n",
      "Epoch [14840/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14850/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14860/20000], Loss: 0.2703, Training Accuracy: 0.9080\n",
      "Epoch [14870/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [14880/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14890/20000], Loss: 0.2698, Training Accuracy: 0.9060\n",
      "Epoch [14900/20000], Loss: 0.2699, Training Accuracy: 0.9050\n",
      "Epoch [14910/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [14920/20000], Loss: 0.2700, Training Accuracy: 0.9065\n",
      "Epoch [14930/20000], Loss: 0.2698, Training Accuracy: 0.9045\n",
      "Epoch [14940/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [14950/20000], Loss: 0.2704, Training Accuracy: 0.9075\n",
      "Epoch [14960/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [14970/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [14980/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [14990/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [15000/20000], Loss: 0.2698, Training Accuracy: 0.9050\n",
      "Epoch [15010/20000], Loss: 0.2702, Training Accuracy: 0.9080\n",
      "Epoch [15020/20000], Loss: 0.2697, Training Accuracy: 0.9050\n",
      "Epoch [15030/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [15040/20000], Loss: 0.2698, Training Accuracy: 0.9065\n",
      "Epoch [15050/20000], Loss: 0.2698, Training Accuracy: 0.9045\n",
      "Epoch [15060/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15070/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [15080/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15090/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [15100/20000], Loss: 0.2697, Training Accuracy: 0.9050\n",
      "Epoch [15110/20000], Loss: 0.2699, Training Accuracy: 0.9075\n",
      "Epoch [15120/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [15130/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [15140/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15150/20000], Loss: 0.2698, Training Accuracy: 0.9050\n",
      "Epoch [15160/20000], Loss: 0.2714, Training Accuracy: 0.9050\n",
      "Epoch [15170/20000], Loss: 0.2706, Training Accuracy: 0.9055\n",
      "Epoch [15180/20000], Loss: 0.2697, Training Accuracy: 0.9050\n",
      "Epoch [15190/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15200/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15210/20000], Loss: 0.2696, Training Accuracy: 0.9045\n",
      "Epoch [15220/20000], Loss: 0.2697, Training Accuracy: 0.9050\n",
      "Epoch [15230/20000], Loss: 0.2696, Training Accuracy: 0.9040\n",
      "Epoch [15240/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15250/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [15260/20000], Loss: 0.2702, Training Accuracy: 0.9080\n",
      "Epoch [15270/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [15280/20000], Loss: 0.2696, Training Accuracy: 0.9050\n",
      "Epoch [15290/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15300/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15310/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15320/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15330/20000], Loss: 0.2700, Training Accuracy: 0.9080\n",
      "Epoch [15340/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15350/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15360/20000], Loss: 0.2698, Training Accuracy: 0.9070\n",
      "Epoch [15370/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15380/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [15390/20000], Loss: 0.2696, Training Accuracy: 0.9065\n",
      "Epoch [15400/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15410/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [15420/20000], Loss: 0.2701, Training Accuracy: 0.9055\n",
      "Epoch [15430/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15440/20000], Loss: 0.2698, Training Accuracy: 0.9070\n",
      "Epoch [15450/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [15460/20000], Loss: 0.2696, Training Accuracy: 0.9060\n",
      "Epoch [15470/20000], Loss: 0.2697, Training Accuracy: 0.9070\n",
      "Epoch [15480/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15490/20000], Loss: 0.2695, Training Accuracy: 0.9045\n",
      "Epoch [15500/20000], Loss: 0.2699, Training Accuracy: 0.9080\n",
      "Epoch [15510/20000], Loss: 0.2695, Training Accuracy: 0.9050\n",
      "Epoch [15520/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15530/20000], Loss: 0.2695, Training Accuracy: 0.9050\n",
      "Epoch [15540/20000], Loss: 0.2695, Training Accuracy: 0.9050\n",
      "Epoch [15550/20000], Loss: 0.2709, Training Accuracy: 0.9065\n",
      "Epoch [15560/20000], Loss: 0.2700, Training Accuracy: 0.9055\n",
      "Epoch [15570/20000], Loss: 0.2697, Training Accuracy: 0.9080\n",
      "Epoch [15580/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [15590/20000], Loss: 0.2698, Training Accuracy: 0.9080\n",
      "Epoch [15600/20000], Loss: 0.2695, Training Accuracy: 0.9050\n",
      "Epoch [15610/20000], Loss: 0.2695, Training Accuracy: 0.9040\n",
      "Epoch [15620/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15630/20000], Loss: 0.2696, Training Accuracy: 0.9050\n",
      "Epoch [15640/20000], Loss: 0.2698, Training Accuracy: 0.9080\n",
      "Epoch [15650/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15660/20000], Loss: 0.2696, Training Accuracy: 0.9075\n",
      "Epoch [15670/20000], Loss: 0.2694, Training Accuracy: 0.9050\n",
      "Epoch [15680/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [15690/20000], Loss: 0.2695, Training Accuracy: 0.9060\n",
      "Epoch [15700/20000], Loss: 0.2695, Training Accuracy: 0.9060\n",
      "Epoch [15710/20000], Loss: 0.2704, Training Accuracy: 0.9055\n",
      "Epoch [15720/20000], Loss: 0.2701, Training Accuracy: 0.9075\n",
      "Epoch [15730/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15740/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [15750/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [15760/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [15770/20000], Loss: 0.2695, Training Accuracy: 0.9060\n",
      "Epoch [15780/20000], Loss: 0.2695, Training Accuracy: 0.9080\n",
      "Epoch [15790/20000], Loss: 0.2699, Training Accuracy: 0.9080\n",
      "Epoch [15800/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [15810/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [15820/20000], Loss: 0.2698, Training Accuracy: 0.9080\n",
      "Epoch [15830/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [15840/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15850/20000], Loss: 0.2694, Training Accuracy: 0.9040\n",
      "Epoch [15860/20000], Loss: 0.2694, Training Accuracy: 0.9060\n",
      "Epoch [15870/20000], Loss: 0.2702, Training Accuracy: 0.9055\n",
      "Epoch [15880/20000], Loss: 0.2701, Training Accuracy: 0.9080\n",
      "Epoch [15890/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15900/20000], Loss: 0.2694, Training Accuracy: 0.9065\n",
      "Epoch [15910/20000], Loss: 0.2693, Training Accuracy: 0.9045\n",
      "Epoch [15920/20000], Loss: 0.2697, Training Accuracy: 0.9055\n",
      "Epoch [15930/20000], Loss: 0.2693, Training Accuracy: 0.9050\n",
      "Epoch [15940/20000], Loss: 0.2699, Training Accuracy: 0.9075\n",
      "Epoch [15950/20000], Loss: 0.2693, Training Accuracy: 0.9050\n",
      "Epoch [15960/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [15970/20000], Loss: 0.2695, Training Accuracy: 0.9080\n",
      "Epoch [15980/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [15990/20000], Loss: 0.2694, Training Accuracy: 0.9060\n",
      "Epoch [16000/20000], Loss: 0.2702, Training Accuracy: 0.9070\n",
      "Epoch [16010/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [16020/20000], Loss: 0.2693, Training Accuracy: 0.9065\n",
      "Epoch [16030/20000], Loss: 0.2694, Training Accuracy: 0.9065\n",
      "Epoch [16040/20000], Loss: 0.2694, Training Accuracy: 0.9065\n",
      "Epoch [16050/20000], Loss: 0.2695, Training Accuracy: 0.9060\n",
      "Epoch [16060/20000], Loss: 0.2693, Training Accuracy: 0.9045\n",
      "Epoch [16070/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [16080/20000], Loss: 0.2694, Training Accuracy: 0.9045\n",
      "Epoch [16090/20000], Loss: 0.2696, Training Accuracy: 0.9080\n",
      "Epoch [16100/20000], Loss: 0.2693, Training Accuracy: 0.9060\n",
      "Epoch [16110/20000], Loss: 0.2693, Training Accuracy: 0.9055\n",
      "Epoch [16120/20000], Loss: 0.2699, Training Accuracy: 0.9055\n",
      "Epoch [16130/20000], Loss: 0.2694, Training Accuracy: 0.9080\n",
      "Epoch [16140/20000], Loss: 0.2695, Training Accuracy: 0.9075\n",
      "Epoch [16150/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [16160/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [16170/20000], Loss: 0.2693, Training Accuracy: 0.9045\n",
      "Epoch [16180/20000], Loss: 0.2702, Training Accuracy: 0.9050\n",
      "Epoch [16190/20000], Loss: 0.2697, Training Accuracy: 0.9075\n",
      "Epoch [16200/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [16210/20000], Loss: 0.2694, Training Accuracy: 0.9075\n",
      "Epoch [16220/20000], Loss: 0.2693, Training Accuracy: 0.9050\n",
      "Epoch [16230/20000], Loss: 0.2693, Training Accuracy: 0.9045\n",
      "Epoch [16240/20000], Loss: 0.2695, Training Accuracy: 0.9080\n",
      "Epoch [16250/20000], Loss: 0.2692, Training Accuracy: 0.9055\n",
      "Epoch [16260/20000], Loss: 0.2693, Training Accuracy: 0.9075\n",
      "Epoch [16270/20000], Loss: 0.2698, Training Accuracy: 0.9080\n",
      "Epoch [16280/20000], Loss: 0.2698, Training Accuracy: 0.9055\n",
      "Epoch [16290/20000], Loss: 0.2694, Training Accuracy: 0.9080\n",
      "Epoch [16300/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [16310/20000], Loss: 0.2693, Training Accuracy: 0.9055\n",
      "Epoch [16320/20000], Loss: 0.2695, Training Accuracy: 0.9060\n",
      "Epoch [16330/20000], Loss: 0.2692, Training Accuracy: 0.9065\n",
      "Epoch [16340/20000], Loss: 0.2700, Training Accuracy: 0.9065\n",
      "Epoch [16350/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [16360/20000], Loss: 0.2693, Training Accuracy: 0.9080\n",
      "Epoch [16370/20000], Loss: 0.2692, Training Accuracy: 0.9045\n",
      "Epoch [16380/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [16390/20000], Loss: 0.2692, Training Accuracy: 0.9065\n",
      "Epoch [16400/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [16410/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16420/20000], Loss: 0.2693, Training Accuracy: 0.9075\n",
      "Epoch [16430/20000], Loss: 0.2694, Training Accuracy: 0.9080\n",
      "Epoch [16440/20000], Loss: 0.2692, Training Accuracy: 0.9070\n",
      "Epoch [16450/20000], Loss: 0.2697, Training Accuracy: 0.9075\n",
      "Epoch [16460/20000], Loss: 0.2692, Training Accuracy: 0.9065\n",
      "Epoch [16470/20000], Loss: 0.2693, Training Accuracy: 0.9065\n",
      "Epoch [16480/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [16490/20000], Loss: 0.2691, Training Accuracy: 0.9050\n",
      "Epoch [16500/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [16510/20000], Loss: 0.2692, Training Accuracy: 0.9050\n",
      "Epoch [16520/20000], Loss: 0.2698, Training Accuracy: 0.9075\n",
      "Epoch [16530/20000], Loss: 0.2695, Training Accuracy: 0.9055\n",
      "Epoch [16540/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16550/20000], Loss: 0.2691, Training Accuracy: 0.9070\n",
      "Epoch [16560/20000], Loss: 0.2691, Training Accuracy: 0.9060\n",
      "Epoch [16570/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16580/20000], Loss: 0.2704, Training Accuracy: 0.9065\n",
      "Epoch [16590/20000], Loss: 0.2696, Training Accuracy: 0.9055\n",
      "Epoch [16600/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16610/20000], Loss: 0.2692, Training Accuracy: 0.9065\n",
      "Epoch [16620/20000], Loss: 0.2693, Training Accuracy: 0.9055\n",
      "Epoch [16630/20000], Loss: 0.2691, Training Accuracy: 0.9065\n",
      "Epoch [16640/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16650/20000], Loss: 0.2693, Training Accuracy: 0.9075\n",
      "Epoch [16660/20000], Loss: 0.2691, Training Accuracy: 0.9055\n",
      "Epoch [16670/20000], Loss: 0.2699, Training Accuracy: 0.9050\n",
      "Epoch [16680/20000], Loss: 0.2691, Training Accuracy: 0.9075\n",
      "Epoch [16690/20000], Loss: 0.2691, Training Accuracy: 0.9060\n",
      "Epoch [16700/20000], Loss: 0.2692, Training Accuracy: 0.9050\n",
      "Epoch [16710/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16720/20000], Loss: 0.2691, Training Accuracy: 0.9060\n",
      "Epoch [16730/20000], Loss: 0.2691, Training Accuracy: 0.9050\n",
      "Epoch [16740/20000], Loss: 0.2696, Training Accuracy: 0.9050\n",
      "Epoch [16750/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [16760/20000], Loss: 0.2698, Training Accuracy: 0.9065\n",
      "Epoch [16770/20000], Loss: 0.2691, Training Accuracy: 0.9045\n",
      "Epoch [16780/20000], Loss: 0.2694, Training Accuracy: 0.9060\n",
      "Epoch [16790/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16800/20000], Loss: 0.2691, Training Accuracy: 0.9080\n",
      "Epoch [16810/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [16820/20000], Loss: 0.2691, Training Accuracy: 0.9055\n",
      "Epoch [16830/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [16840/20000], Loss: 0.2695, Training Accuracy: 0.9050\n",
      "Epoch [16850/20000], Loss: 0.2691, Training Accuracy: 0.9060\n",
      "Epoch [16860/20000], Loss: 0.2698, Training Accuracy: 0.9070\n",
      "Epoch [16870/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [16880/20000], Loss: 0.2691, Training Accuracy: 0.9080\n",
      "Epoch [16890/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [16900/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [16910/20000], Loss: 0.2690, Training Accuracy: 0.9080\n",
      "Epoch [16920/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [16930/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [16940/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [16950/20000], Loss: 0.2702, Training Accuracy: 0.9050\n",
      "Epoch [16960/20000], Loss: 0.2695, Training Accuracy: 0.9070\n",
      "Epoch [16970/20000], Loss: 0.2691, Training Accuracy: 0.9050\n",
      "Epoch [16980/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [16990/20000], Loss: 0.2690, Training Accuracy: 0.9065\n",
      "Epoch [17000/20000], Loss: 0.2692, Training Accuracy: 0.9045\n",
      "Epoch [17010/20000], Loss: 0.2689, Training Accuracy: 0.9070\n",
      "Epoch [17020/20000], Loss: 0.2694, Training Accuracy: 0.9075\n",
      "Epoch [17030/20000], Loss: 0.2689, Training Accuracy: 0.9060\n",
      "Epoch [17040/20000], Loss: 0.2694, Training Accuracy: 0.9055\n",
      "Epoch [17050/20000], Loss: 0.2689, Training Accuracy: 0.9070\n",
      "Epoch [17060/20000], Loss: 0.2692, Training Accuracy: 0.9075\n",
      "Epoch [17070/20000], Loss: 0.2691, Training Accuracy: 0.9080\n",
      "Epoch [17080/20000], Loss: 0.2694, Training Accuracy: 0.9050\n",
      "Epoch [17090/20000], Loss: 0.2689, Training Accuracy: 0.9070\n",
      "Epoch [17100/20000], Loss: 0.2694, Training Accuracy: 0.9070\n",
      "Epoch [17110/20000], Loss: 0.2689, Training Accuracy: 0.9055\n",
      "Epoch [17120/20000], Loss: 0.2693, Training Accuracy: 0.9055\n",
      "Epoch [17130/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17140/20000], Loss: 0.2697, Training Accuracy: 0.9065\n",
      "Epoch [17150/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [17160/20000], Loss: 0.2690, Training Accuracy: 0.9055\n",
      "Epoch [17170/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17180/20000], Loss: 0.2693, Training Accuracy: 0.9075\n",
      "Epoch [17190/20000], Loss: 0.2689, Training Accuracy: 0.9065\n",
      "Epoch [17200/20000], Loss: 0.2694, Training Accuracy: 0.9050\n",
      "Epoch [17210/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17220/20000], Loss: 0.2693, Training Accuracy: 0.9070\n",
      "Epoch [17230/20000], Loss: 0.2689, Training Accuracy: 0.9065\n",
      "Epoch [17240/20000], Loss: 0.2696, Training Accuracy: 0.9050\n",
      "Epoch [17250/20000], Loss: 0.2688, Training Accuracy: 0.9075\n",
      "Epoch [17260/20000], Loss: 0.2697, Training Accuracy: 0.9065\n",
      "Epoch [17270/20000], Loss: 0.2695, Training Accuracy: 0.9050\n",
      "Epoch [17280/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [17290/20000], Loss: 0.2688, Training Accuracy: 0.9060\n",
      "Epoch [17300/20000], Loss: 0.2689, Training Accuracy: 0.9055\n",
      "Epoch [17310/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [17320/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17330/20000], Loss: 0.2692, Training Accuracy: 0.9055\n",
      "Epoch [17340/20000], Loss: 0.2688, Training Accuracy: 0.9070\n",
      "Epoch [17350/20000], Loss: 0.2697, Training Accuracy: 0.9065\n",
      "Epoch [17360/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [17370/20000], Loss: 0.2688, Training Accuracy: 0.9065\n",
      "Epoch [17380/20000], Loss: 0.2691, Training Accuracy: 0.9075\n",
      "Epoch [17390/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17400/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [17410/20000], Loss: 0.2690, Training Accuracy: 0.9080\n",
      "Epoch [17420/20000], Loss: 0.2692, Training Accuracy: 0.9055\n",
      "Epoch [17430/20000], Loss: 0.2688, Training Accuracy: 0.9060\n",
      "Epoch [17440/20000], Loss: 0.2688, Training Accuracy: 0.9080\n",
      "Epoch [17450/20000], Loss: 0.2695, Training Accuracy: 0.9065\n",
      "Epoch [17460/20000], Loss: 0.2688, Training Accuracy: 0.9055\n",
      "Epoch [17470/20000], Loss: 0.2692, Training Accuracy: 0.9050\n",
      "Epoch [17480/20000], Loss: 0.2689, Training Accuracy: 0.9075\n",
      "Epoch [17490/20000], Loss: 0.2688, Training Accuracy: 0.9070\n",
      "Epoch [17500/20000], Loss: 0.2689, Training Accuracy: 0.9045\n",
      "Epoch [17510/20000], Loss: 0.2690, Training Accuracy: 0.9055\n",
      "Epoch [17520/20000], Loss: 0.2690, Training Accuracy: 0.9070\n",
      "Epoch [17530/20000], Loss: 0.2688, Training Accuracy: 0.9070\n",
      "Epoch [17540/20000], Loss: 0.2696, Training Accuracy: 0.9050\n",
      "Epoch [17550/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17560/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [17570/20000], Loss: 0.2689, Training Accuracy: 0.9060\n",
      "Epoch [17580/20000], Loss: 0.2691, Training Accuracy: 0.9070\n",
      "Epoch [17590/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [17600/20000], Loss: 0.2690, Training Accuracy: 0.9055\n",
      "Epoch [17610/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [17620/20000], Loss: 0.2687, Training Accuracy: 0.9080\n",
      "Epoch [17630/20000], Loss: 0.2702, Training Accuracy: 0.9060\n",
      "Epoch [17640/20000], Loss: 0.2692, Training Accuracy: 0.9050\n",
      "Epoch [17650/20000], Loss: 0.2689, Training Accuracy: 0.9075\n",
      "Epoch [17660/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [17670/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [17680/20000], Loss: 0.2689, Training Accuracy: 0.9075\n",
      "Epoch [17690/20000], Loss: 0.2687, Training Accuracy: 0.9075\n",
      "Epoch [17700/20000], Loss: 0.2689, Training Accuracy: 0.9075\n",
      "Epoch [17710/20000], Loss: 0.2688, Training Accuracy: 0.9075\n",
      "Epoch [17720/20000], Loss: 0.2687, Training Accuracy: 0.9080\n",
      "Epoch [17730/20000], Loss: 0.2700, Training Accuracy: 0.9060\n",
      "Epoch [17740/20000], Loss: 0.2691, Training Accuracy: 0.9050\n",
      "Epoch [17750/20000], Loss: 0.2689, Training Accuracy: 0.9070\n",
      "Epoch [17760/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [17770/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [17780/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [17790/20000], Loss: 0.2686, Training Accuracy: 0.9065\n",
      "Epoch [17800/20000], Loss: 0.2691, Training Accuracy: 0.9045\n",
      "Epoch [17810/20000], Loss: 0.2688, Training Accuracy: 0.9080\n",
      "Epoch [17820/20000], Loss: 0.2687, Training Accuracy: 0.9080\n",
      "Epoch [17830/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [17840/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [17850/20000], Loss: 0.2688, Training Accuracy: 0.9075\n",
      "Epoch [17860/20000], Loss: 0.2689, Training Accuracy: 0.9080\n",
      "Epoch [17870/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [17880/20000], Loss: 0.2686, Training Accuracy: 0.9080\n",
      "Epoch [17890/20000], Loss: 0.2689, Training Accuracy: 0.9075\n",
      "Epoch [17900/20000], Loss: 0.2686, Training Accuracy: 0.9070\n",
      "Epoch [17910/20000], Loss: 0.2690, Training Accuracy: 0.9075\n",
      "Epoch [17920/20000], Loss: 0.2687, Training Accuracy: 0.9080\n",
      "Epoch [17930/20000], Loss: 0.2694, Training Accuracy: 0.9050\n",
      "Epoch [17940/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [17950/20000], Loss: 0.2688, Training Accuracy: 0.9045\n",
      "Epoch [17960/20000], Loss: 0.2686, Training Accuracy: 0.9080\n",
      "Epoch [17970/20000], Loss: 0.2687, Training Accuracy: 0.9080\n",
      "Epoch [17980/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [17990/20000], Loss: 0.2686, Training Accuracy: 0.9065\n",
      "Epoch [18000/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [18010/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [18020/20000], Loss: 0.2688, Training Accuracy: 0.9055\n",
      "Epoch [18030/20000], Loss: 0.2690, Training Accuracy: 0.9045\n",
      "Epoch [18040/20000], Loss: 0.2689, Training Accuracy: 0.9070\n",
      "Epoch [18050/20000], Loss: 0.2685, Training Accuracy: 0.9065\n",
      "Epoch [18060/20000], Loss: 0.2689, Training Accuracy: 0.9060\n",
      "Epoch [18070/20000], Loss: 0.2689, Training Accuracy: 0.9070\n",
      "Epoch [18080/20000], Loss: 0.2685, Training Accuracy: 0.9065\n",
      "Epoch [18090/20000], Loss: 0.2691, Training Accuracy: 0.9050\n",
      "Epoch [18100/20000], Loss: 0.2685, Training Accuracy: 0.9075\n",
      "Epoch [18110/20000], Loss: 0.2691, Training Accuracy: 0.9070\n",
      "Epoch [18120/20000], Loss: 0.2685, Training Accuracy: 0.9065\n",
      "Epoch [18130/20000], Loss: 0.2688, Training Accuracy: 0.9060\n",
      "Epoch [18140/20000], Loss: 0.2687, Training Accuracy: 0.9075\n",
      "Epoch [18150/20000], Loss: 0.2685, Training Accuracy: 0.9075\n",
      "Epoch [18160/20000], Loss: 0.2687, Training Accuracy: 0.9050\n",
      "Epoch [18170/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [18180/20000], Loss: 0.2690, Training Accuracy: 0.9070\n",
      "Epoch [18190/20000], Loss: 0.2685, Training Accuracy: 0.9065\n",
      "Epoch [18200/20000], Loss: 0.2690, Training Accuracy: 0.9045\n",
      "Epoch [18210/20000], Loss: 0.2685, Training Accuracy: 0.9075\n",
      "Epoch [18220/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [18230/20000], Loss: 0.2685, Training Accuracy: 0.9060\n",
      "Epoch [18240/20000], Loss: 0.2688, Training Accuracy: 0.9060\n",
      "Epoch [18250/20000], Loss: 0.2686, Training Accuracy: 0.9080\n",
      "Epoch [18260/20000], Loss: 0.2687, Training Accuracy: 0.9080\n",
      "Epoch [18270/20000], Loss: 0.2685, Training Accuracy: 0.9080\n",
      "Epoch [18280/20000], Loss: 0.2691, Training Accuracy: 0.9065\n",
      "Epoch [18290/20000], Loss: 0.2687, Training Accuracy: 0.9040\n",
      "Epoch [18300/20000], Loss: 0.2686, Training Accuracy: 0.9065\n",
      "Epoch [18310/20000], Loss: 0.2691, Training Accuracy: 0.9070\n",
      "Epoch [18320/20000], Loss: 0.2686, Training Accuracy: 0.9060\n",
      "Epoch [18330/20000], Loss: 0.2685, Training Accuracy: 0.9065\n",
      "Epoch [18340/20000], Loss: 0.2687, Training Accuracy: 0.9075\n",
      "Epoch [18350/20000], Loss: 0.2685, Training Accuracy: 0.9080\n",
      "Epoch [18360/20000], Loss: 0.2684, Training Accuracy: 0.9075\n",
      "Epoch [18370/20000], Loss: 0.2688, Training Accuracy: 0.9070\n",
      "Epoch [18380/20000], Loss: 0.2690, Training Accuracy: 0.9080\n",
      "Epoch [18390/20000], Loss: 0.2693, Training Accuracy: 0.9050\n",
      "Epoch [18400/20000], Loss: 0.2690, Training Accuracy: 0.9070\n",
      "Epoch [18410/20000], Loss: 0.2686, Training Accuracy: 0.9055\n",
      "Epoch [18420/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [18430/20000], Loss: 0.2685, Training Accuracy: 0.9080\n",
      "Epoch [18440/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [18450/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [18460/20000], Loss: 0.2689, Training Accuracy: 0.9075\n",
      "Epoch [18470/20000], Loss: 0.2689, Training Accuracy: 0.9055\n",
      "Epoch [18480/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [18490/20000], Loss: 0.2688, Training Accuracy: 0.9075\n",
      "Epoch [18500/20000], Loss: 0.2684, Training Accuracy: 0.9075\n",
      "Epoch [18510/20000], Loss: 0.2684, Training Accuracy: 0.9065\n",
      "Epoch [18520/20000], Loss: 0.2699, Training Accuracy: 0.9050\n",
      "Epoch [18530/20000], Loss: 0.2690, Training Accuracy: 0.9065\n",
      "Epoch [18540/20000], Loss: 0.2685, Training Accuracy: 0.9060\n",
      "Epoch [18550/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18560/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [18570/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [18580/20000], Loss: 0.2686, Training Accuracy: 0.9070\n",
      "Epoch [18590/20000], Loss: 0.2683, Training Accuracy: 0.9065\n",
      "Epoch [18600/20000], Loss: 0.2686, Training Accuracy: 0.9060\n",
      "Epoch [18610/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18620/20000], Loss: 0.2686, Training Accuracy: 0.9075\n",
      "Epoch [18630/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18640/20000], Loss: 0.2690, Training Accuracy: 0.9065\n",
      "Epoch [18650/20000], Loss: 0.2683, Training Accuracy: 0.9065\n",
      "Epoch [18660/20000], Loss: 0.2689, Training Accuracy: 0.9050\n",
      "Epoch [18670/20000], Loss: 0.2683, Training Accuracy: 0.9075\n",
      "Epoch [18680/20000], Loss: 0.2688, Training Accuracy: 0.9070\n",
      "Epoch [18690/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18700/20000], Loss: 0.2685, Training Accuracy: 0.9055\n",
      "Epoch [18710/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [18720/20000], Loss: 0.2692, Training Accuracy: 0.9055\n",
      "Epoch [18730/20000], Loss: 0.2684, Training Accuracy: 0.9055\n",
      "Epoch [18740/20000], Loss: 0.2683, Training Accuracy: 0.9075\n",
      "Epoch [18750/20000], Loss: 0.2687, Training Accuracy: 0.9070\n",
      "Epoch [18760/20000], Loss: 0.2684, Training Accuracy: 0.9065\n",
      "Epoch [18770/20000], Loss: 0.2684, Training Accuracy: 0.9065\n",
      "Epoch [18780/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18790/20000], Loss: 0.2687, Training Accuracy: 0.9075\n",
      "Epoch [18800/20000], Loss: 0.2683, Training Accuracy: 0.9065\n",
      "Epoch [18810/20000], Loss: 0.2690, Training Accuracy: 0.9050\n",
      "Epoch [18820/20000], Loss: 0.2684, Training Accuracy: 0.9075\n",
      "Epoch [18830/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18840/20000], Loss: 0.2691, Training Accuracy: 0.9050\n",
      "Epoch [18850/20000], Loss: 0.2688, Training Accuracy: 0.9070\n",
      "Epoch [18860/20000], Loss: 0.2682, Training Accuracy: 0.9065\n",
      "Epoch [18870/20000], Loss: 0.2685, Training Accuracy: 0.9060\n",
      "Epoch [18880/20000], Loss: 0.2682, Training Accuracy: 0.9075\n",
      "Epoch [18890/20000], Loss: 0.2688, Training Accuracy: 0.9075\n",
      "Epoch [18900/20000], Loss: 0.2682, Training Accuracy: 0.9065\n",
      "Epoch [18910/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [18920/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [18930/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [18940/20000], Loss: 0.2689, Training Accuracy: 0.9050\n",
      "Epoch [18950/20000], Loss: 0.2684, Training Accuracy: 0.9075\n",
      "Epoch [18960/20000], Loss: 0.2682, Training Accuracy: 0.9070\n",
      "Epoch [18970/20000], Loss: 0.2685, Training Accuracy: 0.9065\n",
      "Epoch [18980/20000], Loss: 0.2682, Training Accuracy: 0.9080\n",
      "Epoch [18990/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [19000/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19010/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [19020/20000], Loss: 0.2685, Training Accuracy: 0.9075\n",
      "Epoch [19030/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19040/20000], Loss: 0.2690, Training Accuracy: 0.9060\n",
      "Epoch [19050/20000], Loss: 0.2681, Training Accuracy: 0.9070\n",
      "Epoch [19060/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [19070/20000], Loss: 0.2681, Training Accuracy: 0.9065\n",
      "Epoch [19080/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19090/20000], Loss: 0.2694, Training Accuracy: 0.9065\n",
      "Epoch [19100/20000], Loss: 0.2686, Training Accuracy: 0.9065\n",
      "Epoch [19110/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19120/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [19130/20000], Loss: 0.2680, Training Accuracy: 0.9065\n",
      "Epoch [19140/20000], Loss: 0.2681, Training Accuracy: 0.9055\n",
      "Epoch [19150/20000], Loss: 0.2688, Training Accuracy: 0.9075\n",
      "Epoch [19160/20000], Loss: 0.2682, Training Accuracy: 0.9080\n",
      "Epoch [19170/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [19180/20000], Loss: 0.2689, Training Accuracy: 0.9055\n",
      "Epoch [19190/20000], Loss: 0.2682, Training Accuracy: 0.9080\n",
      "Epoch [19200/20000], Loss: 0.2680, Training Accuracy: 0.9070\n",
      "Epoch [19210/20000], Loss: 0.2684, Training Accuracy: 0.9060\n",
      "Epoch [19220/20000], Loss: 0.2680, Training Accuracy: 0.9065\n",
      "Epoch [19230/20000], Loss: 0.2684, Training Accuracy: 0.9060\n",
      "Epoch [19240/20000], Loss: 0.2683, Training Accuracy: 0.9065\n",
      "Epoch [19250/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19260/20000], Loss: 0.2692, Training Accuracy: 0.9060\n",
      "Epoch [19270/20000], Loss: 0.2681, Training Accuracy: 0.9065\n",
      "Epoch [19280/20000], Loss: 0.2681, Training Accuracy: 0.9060\n",
      "Epoch [19290/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19300/20000], Loss: 0.2685, Training Accuracy: 0.9075\n",
      "Epoch [19310/20000], Loss: 0.2682, Training Accuracy: 0.9060\n",
      "Epoch [19320/20000], Loss: 0.2681, Training Accuracy: 0.9065\n",
      "Epoch [19330/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [19340/20000], Loss: 0.2680, Training Accuracy: 0.9060\n",
      "Epoch [19350/20000], Loss: 0.2682, Training Accuracy: 0.9065\n",
      "Epoch [19360/20000], Loss: 0.2682, Training Accuracy: 0.9080\n",
      "Epoch [19370/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19380/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19390/20000], Loss: 0.2683, Training Accuracy: 0.9080\n",
      "Epoch [19400/20000], Loss: 0.2681, Training Accuracy: 0.9085\n",
      "Epoch [19410/20000], Loss: 0.2683, Training Accuracy: 0.9075\n",
      "Epoch [19420/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19430/20000], Loss: 0.2685, Training Accuracy: 0.9055\n",
      "Epoch [19440/20000], Loss: 0.2679, Training Accuracy: 0.9080\n",
      "Epoch [19450/20000], Loss: 0.2684, Training Accuracy: 0.9080\n",
      "Epoch [19460/20000], Loss: 0.2686, Training Accuracy: 0.9070\n",
      "Epoch [19470/20000], Loss: 0.2682, Training Accuracy: 0.9080\n",
      "Epoch [19480/20000], Loss: 0.2679, Training Accuracy: 0.9070\n",
      "Epoch [19490/20000], Loss: 0.2682, Training Accuracy: 0.9060\n",
      "Epoch [19500/20000], Loss: 0.2679, Training Accuracy: 0.9065\n",
      "Epoch [19510/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19520/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19530/20000], Loss: 0.2687, Training Accuracy: 0.9070\n",
      "Epoch [19540/20000], Loss: 0.2681, Training Accuracy: 0.9080\n",
      "Epoch [19550/20000], Loss: 0.2678, Training Accuracy: 0.9065\n",
      "Epoch [19560/20000], Loss: 0.2681, Training Accuracy: 0.9060\n",
      "Epoch [19570/20000], Loss: 0.2678, Training Accuracy: 0.9075\n",
      "Epoch [19580/20000], Loss: 0.2683, Training Accuracy: 0.9070\n",
      "Epoch [19590/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19600/20000], Loss: 0.2679, Training Accuracy: 0.9080\n",
      "Epoch [19610/20000], Loss: 0.2687, Training Accuracy: 0.9065\n",
      "Epoch [19620/20000], Loss: 0.2678, Training Accuracy: 0.9065\n",
      "Epoch [19630/20000], Loss: 0.2686, Training Accuracy: 0.9065\n",
      "Epoch [19640/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19650/20000], Loss: 0.2679, Training Accuracy: 0.9075\n",
      "Epoch [19660/20000], Loss: 0.2682, Training Accuracy: 0.9060\n",
      "Epoch [19670/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19680/20000], Loss: 0.2679, Training Accuracy: 0.9080\n",
      "Epoch [19690/20000], Loss: 0.2683, Training Accuracy: 0.9070\n",
      "Epoch [19700/20000], Loss: 0.2677, Training Accuracy: 0.9075\n",
      "Epoch [19710/20000], Loss: 0.2680, Training Accuracy: 0.9080\n",
      "Epoch [19720/20000], Loss: 0.2681, Training Accuracy: 0.9085\n",
      "Epoch [19730/20000], Loss: 0.2678, Training Accuracy: 0.9065\n",
      "Epoch [19740/20000], Loss: 0.2683, Training Accuracy: 0.9060\n",
      "Epoch [19750/20000], Loss: 0.2685, Training Accuracy: 0.9055\n",
      "Epoch [19760/20000], Loss: 0.2677, Training Accuracy: 0.9060\n",
      "Epoch [19770/20000], Loss: 0.2679, Training Accuracy: 0.9060\n",
      "Epoch [19780/20000], Loss: 0.2677, Training Accuracy: 0.9075\n",
      "Epoch [19790/20000], Loss: 0.2682, Training Accuracy: 0.9065\n",
      "Epoch [19800/20000], Loss: 0.2678, Training Accuracy: 0.9080\n",
      "Epoch [19810/20000], Loss: 0.2677, Training Accuracy: 0.9080\n",
      "Epoch [19820/20000], Loss: 0.2687, Training Accuracy: 0.9060\n",
      "Epoch [19830/20000], Loss: 0.2677, Training Accuracy: 0.9065\n",
      "Epoch [19840/20000], Loss: 0.2680, Training Accuracy: 0.9060\n",
      "Epoch [19850/20000], Loss: 0.2684, Training Accuracy: 0.9055\n",
      "Epoch [19860/20000], Loss: 0.2677, Training Accuracy: 0.9060\n",
      "Epoch [19870/20000], Loss: 0.2680, Training Accuracy: 0.9060\n",
      "Epoch [19880/20000], Loss: 0.2677, Training Accuracy: 0.9085\n",
      "Epoch [19890/20000], Loss: 0.2682, Training Accuracy: 0.9070\n",
      "Epoch [19900/20000], Loss: 0.2677, Training Accuracy: 0.9065\n",
      "Epoch [19910/20000], Loss: 0.2679, Training Accuracy: 0.9065\n",
      "Epoch [19920/20000], Loss: 0.2680, Training Accuracy: 0.9070\n",
      "Epoch [19930/20000], Loss: 0.2677, Training Accuracy: 0.9080\n",
      "Epoch [19940/20000], Loss: 0.2676, Training Accuracy: 0.9060\n",
      "Epoch [19950/20000], Loss: 0.2683, Training Accuracy: 0.9065\n",
      "Epoch [19960/20000], Loss: 0.2676, Training Accuracy: 0.9080\n",
      "Epoch [19970/20000], Loss: 0.2686, Training Accuracy: 0.9060\n",
      "Epoch [19980/20000], Loss: 0.2683, Training Accuracy: 0.9070\n",
      "Epoch [19990/20000], Loss: 0.2677, Training Accuracy: 0.9080\n",
      "Epoch [20000/20000], Loss: 0.2676, Training Accuracy: 0.9075\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncomment next cell if you want to load a particular model you already trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "bafIJlAtv5hT"
   },
   "outputs": [],
   "source": [
    "# loaded_model = SimpleDenseNet(input_dim=len(list(features_to_extract.keys())), hidden_dim=512).to(device)\n",
    "# loaded_model.load_state_dict(torch.load('llama-' + task + '-best-model'))\n",
    "\n",
    "# # Set the model to evaluation mode\"\n",
    "# loaded_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bE9X728RRHRK"
   },
   "source": [
    "#Compute the metrics using the model on the Test Set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ntxC26m--ma",
    "outputId": "fcb095e0-3d72-46e2-d887-05f73e09a4bd",
    "ExecuteTime": {
     "end_time": "2025-04-04T12:12:23.780739200Z",
     "start_time": "2025-03-18T12:03:48.981799Z"
    }
   },
   "source": [
    "X_test_tensor = torch.tensor(X_test_features, dtype=torch.float32).to(device)\n",
    "Y_test_tensor = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "test_metrics = compute_metrics(denseModel, X_test_tensor, Y_test_tensor)\n",
    "\n",
    "print(\n",
    "    f\"Testing - Accuracy: {test_metrics['Accuracy']:.4f}, Precision: {test_metrics['Precision']:.4f}, Recall: {test_metrics['Recall']:.4f}, F1: {test_metrics['F1']:.4f}, ROC AUC: {test_metrics['ROC AUC']:.4f}, PR AUC: {test_metrics['PR AUC']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Testing - Negative: {test_metrics['Accuracy']:.4f}, Precision-Negative: {test_metrics['Precision-Negative']:.4f}, Recall-Negative: {test_metrics['Recall-Negative']:.4f}, F1-Negative: {test_metrics['F1-Negative']:.4f}, ROC AUC-Negative: {test_metrics['ROC AUC-Negative']:.4f}, PR AUC-Negative: {test_metrics['PR AUC-Negative']:.4f}\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing - Accuracy: 0.8897, Precision: 0.8908, Recall: 0.8883, F1: 0.8896, ROC AUC: 0.9377, PR AUC: 0.9175\n",
      "Testing - Negative: 0.8897, Precision-Negative: 0.8886, Recall-Negative: 0.8911, F1-Negative: 0.8899, ROC AUC-Negative: 0.0623, PR AUC-Negative: 0.3318\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-RlivuLRN0H"
   },
   "source": [
    "## Save the results on a CSV if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boqJczJoL4lW"
   },
   "outputs": [],
   "source": [
    "model_dataframe = pd.DataFrame(\n",
    "    columns=[\n",
    "        \"features\",\n",
    "        \"model_name\",\n",
    "        \"feature_to_extract\",\n",
    "        \"method\",\n",
    "        \"accuracy\",\n",
    "        \"precision\",\n",
    "        \"recall\",\n",
    "        \"roc auc\",\n",
    "        \"pr auc\",\n",
    "        \"negative\",\n",
    "        \"precision-negative\",\n",
    "        \"recall-negative\",\n",
    "        \"negative f1\",\n",
    "        \"lr_accuracy\",\n",
    "        \"lr_features_log\",\n",
    "        \"lr_features_no_log\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJQ56NLzL5Tv"
   },
   "outputs": [],
   "source": [
    "d = {\n",
    "    \"features\": features_to_extract,\n",
    "    \"model_name\": str(model.getName()),\n",
    "    \"feature_to_extract\": feature_to_extract,\n",
    "    \"method\": \"TEST\",\n",
    "    \"accuracy\": test_metrics[\"Accuracy\"],\n",
    "    \"precision\": test_metrics[\"Precision\"],\n",
    "    \"recall\": test_metrics[\"Recall\"],\n",
    "    \"f1\": test_metrics[\"F1\"],\n",
    "    \"pr auc\": test_metrics[\"PR AUC\"],\n",
    "    \"precision-negative\": test_metrics[\"Precision-Negative\"],\n",
    "    \"recall-negative\": test_metrics[\"Recall-Negative\"],\n",
    "    \"negative-f1\": test_metrics[\"F1-Negative\"],\n",
    "    \"lr_accuracy\": lr_accuracy,\n",
    "    \"lr_features_log\": lr_features_log,\n",
    "    \"lr_features_no_log\": lr_features_no_log,\n",
    "}\n",
    "\n",
    "model_dataframe.loc[len(model_dataframe.index)] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XNMNRvxvMO_o"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "features",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "model_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "feature_to_extract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "method",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "roc auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pr auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "negative",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision-negative",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall-negative",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "negative f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lr_accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lr_features_log",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "lr_features_no_log",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "05340699-60e1-4a6a-ba2d-905e71e1f9eb",
       "rows": [
        [
         "0",
         "{'mtp': True, 'avgtp': False, 'MDVTP': False, 'MMDVP': False}",
         "facebook/bart-large-cnn",
         "mtp",
         "TEST",
         "0.8905555555555555",
         "0.88049361333622",
         "0.9037777777777778",
         null,
         "0.9161912511125545",
         null,
         "0.9011641177813284",
         "0.8773333333333333",
         null,
         "0.8842222222222222",
         "{'mtp': np.float64(-6.95953145273689)}",
         "{'mtp': np.float64(0.000949541375340262)}"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>model_name</th>\n",
       "      <th>feature_to_extract</th>\n",
       "      <th>method</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc auc</th>\n",
       "      <th>pr auc</th>\n",
       "      <th>negative</th>\n",
       "      <th>precision-negative</th>\n",
       "      <th>recall-negative</th>\n",
       "      <th>negative f1</th>\n",
       "      <th>lr_accuracy</th>\n",
       "      <th>lr_features_log</th>\n",
       "      <th>lr_features_no_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'mtp': True, 'avgtp': False, 'MDVTP': False, ...</td>\n",
       "      <td>facebook/bart-large-cnn</td>\n",
       "      <td>mtp</td>\n",
       "      <td>TEST</td>\n",
       "      <td>0.890556</td>\n",
       "      <td>0.880494</td>\n",
       "      <td>0.903778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.916191</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.901164</td>\n",
       "      <td>0.877333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.884222</td>\n",
       "      <td>{'mtp': -6.95953145273689}</td>\n",
       "      <td>{'mtp': 0.000949541375340262}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            features               model_name  \\\n",
       "0  {'mtp': True, 'avgtp': False, 'MDVTP': False, ...  facebook/bart-large-cnn   \n",
       "\n",
       "  feature_to_extract method  accuracy  precision    recall  roc auc    pr auc  \\\n",
       "0                mtp   TEST  0.890556   0.880494  0.903778      NaN  0.916191   \n",
       "\n",
       "   negative  precision-negative  recall-negative  negative f1  lr_accuracy  \\\n",
       "0       NaN            0.901164         0.877333          NaN     0.884222   \n",
       "\n",
       "              lr_features_log             lr_features_no_log  \n",
       "0  {'mtp': -6.95953145273689}  {'mtp': 0.000949541375340262}  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6tGhxo6SMRvc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook__bart-large-cnn_qa_includeKnowledge=True_includeConditioned=True_mtp=True_avgtp=False_MDVTP=False_MMDVP=False.csv\n"
     ]
    }
   ],
   "source": [
    "csv_name = f\"{model.getSanitizedName()}_{task}_{includeKnowledge=}_{includeConditioned=}_{'_'.join([f'{k}={v}' for k, v in features_to_extract.items()])}.csv\"\n",
    "print(csv_name)\n",
    "model_dataframe.to_csv(csv_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0006c2b3cdf04a0aa6df9a67f786e22c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0409f0470b6946eabe5b539b3589f521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "054ab9b730dd4f909e711338205950f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "076ee3f0e6f64a80a30ad7d1e42629d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1b9bd5dc03b84c0083b94943d1361d6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2232393916e349deb72bc20ac61c3bd7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "27ae621aa1fb46749e61098f2e78990e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2cbaee8118c44153997b1d251b350744": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2dd132e1bbdb4fb4ac2bfd7362320201": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec83dd4e6c3c461da065a91cc7adba45",
      "placeholder": "​",
      "style": "IPY_MODEL_884680a128e24097bd1451a0b1ac3e0a",
      "value": " 1.04M/1.04M [00:00&lt;00:00, 6.39MB/s]"
     }
    },
    "31eff2c794584fd9a187f26f23f070b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3895f9f23123442aa8625d0317f7d856": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3afbd438492c4873ba6c99d39b949752": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9435679a05a941a792e2474f75978385",
      "placeholder": "​",
      "style": "IPY_MODEL_d542096ea2d441ba91698399af48a2af",
      "value": "config.json: 100%"
     }
    },
    "3e5f509de6b24e209ac2a43d116ae064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de85041e44f7432b9c58de466a95968a",
      "max": 3247159078,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_076ee3f0e6f64a80a30ad7d1e42629d6",
      "value": 3247159078
     }
    },
    "4a5a39bfc9f0429a910120e8e3cabe8f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d614251c35b4ab3b9d7214909c80aae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "512f8fb2ca404adfa43933752b5b4990": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5222d4e05153479887f194d9f8bb7165": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "534f2d965c8548638293c8cce181eca3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bf6901510d1405ba1e11dbfae99e64f",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_31eff2c794584fd9a187f26f23f070b9",
      "value": 1355256
     }
    },
    "55370b5cee4b4db5bbab3ed306089fb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b9bd5dc03b84c0083b94943d1361d6f",
      "placeholder": "​",
      "style": "IPY_MODEL_9ec59d9d599241659315c51f1e2f557b",
      "value": "vocab.json: 100%"
     }
    },
    "580c6068c3ca40d88180a213d41e8fff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72d993653d3d4efea6ed47b6b9cf057a",
       "IPY_MODEL_973726c0f5e24b1cb48452c97534cb8f",
       "IPY_MODEL_ea6ab54e02f04d37993b2c4887d5c7ed"
      ],
      "layout": "IPY_MODEL_78c143f681e647ad94d19941c778988d"
     }
    },
    "59948c70bdb44c528e02ba81308a56cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5d9e549be6394f4b8b487cc8f51eb4a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63d2ce276f854602bb02f296857ba020": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cab34f5c61284ce3ad3010563bda7122",
       "IPY_MODEL_3e5f509de6b24e209ac2a43d116ae064",
       "IPY_MODEL_bd172de25a83456c901ef268642883ee"
      ],
      "layout": "IPY_MODEL_5d9e549be6394f4b8b487cc8f51eb4a8"
     }
    },
    "6486665290344283b467a2921dbaee61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_55370b5cee4b4db5bbab3ed306089fb0",
       "IPY_MODEL_b14964cdbe8a498480dfc3d304ba0d7b",
       "IPY_MODEL_2dd132e1bbdb4fb4ac2bfd7362320201"
      ],
      "layout": "IPY_MODEL_0006c2b3cdf04a0aa6df9a67f786e22c"
     }
    },
    "65ab7ee47b174265931b3fd91d4cc4a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6aec23935a144fceb51cf92d39d68f5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72369f95d12e44298dbf5634b74c13fa",
       "IPY_MODEL_ea54b3f282184db195845f1fdbc86f9b",
       "IPY_MODEL_6c15978810124ae59953508e54844f80"
      ],
      "layout": "IPY_MODEL_5222d4e05153479887f194d9f8bb7165"
     }
    },
    "6c15978810124ae59953508e54844f80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be85927df4874f98a966452726e296e8",
      "placeholder": "​",
      "style": "IPY_MODEL_b234c5707cb74b76a92507f1de368bfb",
      "value": " 456k/456k [00:00&lt;00:00, 2.82MB/s]"
     }
    },
    "70eaf260d9994bcab22ab1de76500107": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96085e02e4ab4402bae246fb1839c0d3",
      "placeholder": "​",
      "style": "IPY_MODEL_cd39a3042f2843c4916c2047ae6f43b1",
      "value": " 26.0/26.0 [00:00&lt;00:00, 2.18kB/s]"
     }
    },
    "72369f95d12e44298dbf5634b74c13fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9be1debef01a4c818d9272c26f199f52",
      "placeholder": "​",
      "style": "IPY_MODEL_2cbaee8118c44153997b1d251b350744",
      "value": "merges.txt: 100%"
     }
    },
    "72d993653d3d4efea6ed47b6b9cf057a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5550bc06ebe4a37b6789cdaa6329128",
      "placeholder": "​",
      "style": "IPY_MODEL_65ab7ee47b174265931b3fd91d4cc4a2",
      "value": "generation_config.json: 100%"
     }
    },
    "744b41f6f55847e59e4c2985897ebb3f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_054ab9b730dd4f909e711338205950f5",
      "max": 26,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_27ae621aa1fb46749e61098f2e78990e",
      "value": 26
     }
    },
    "78c143f681e647ad94d19941c778988d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a3ab257d1f74fbfad760e17a24b6de4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0409f0470b6946eabe5b539b3589f521",
      "placeholder": "​",
      "style": "IPY_MODEL_f5477dbf0abb4af2a50201ab646532c0",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 4.11MB/s]"
     }
    },
    "7edab73b97574a8b88cb6d2d8895c64b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "864d448419da4711936a0e584c96964b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "884680a128e24097bd1451a0b1ac3e0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8bf6901510d1405ba1e11dbfae99e64f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8c46c70b058947b488b4d4c9cdfa8b69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9435679a05a941a792e2474f75978385": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "96085e02e4ab4402bae246fb1839c0d3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "973726c0f5e24b1cb48452c97534cb8f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff237f2ab642427ba2c184031c6b2b5c",
      "max": 124,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_59948c70bdb44c528e02ba81308a56cc",
      "value": 124
     }
    },
    "9be1debef01a4c818d9272c26f199f52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ceb67edf8df4e64ae46e700b71072da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ec59d9d599241659315c51f1e2f557b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6a0fa0acdd9408c9ff2291cecd0981f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ec6fc5a0624039a6e50b0fef9c0a71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "abf71c210c314691b1bfcbc0491772fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af24fc423d1e44359a15565fe4d4aba1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3afbd438492c4873ba6c99d39b949752",
       "IPY_MODEL_de349f93b59c4cd289aa135d32d17b3e",
       "IPY_MODEL_b8ed2b38fb684eea814889609bf679fb"
      ],
      "layout": "IPY_MODEL_ebddae646de9494f872de523a517d130"
     }
    },
    "b0d397c5d5eb467e9dba76d983e24926": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b14964cdbe8a498480dfc3d304ba0d7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ceb67edf8df4e64ae46e700b71072da",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3895f9f23123442aa8625d0317f7d856",
      "value": 1042301
     }
    },
    "b234c5707cb74b76a92507f1de368bfb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b64a6d701ebd4802be30cda4ed10783e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b7d5fef6b5634fa2b33a0a2566d8ce27": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f5e6051d53584f1ba67af09b95f6bd50",
       "IPY_MODEL_534f2d965c8548638293c8cce181eca3",
       "IPY_MODEL_7a3ab257d1f74fbfad760e17a24b6de4"
      ],
      "layout": "IPY_MODEL_a6a0fa0acdd9408c9ff2291cecd0981f"
     }
    },
    "b85f192e77dc4bc8a8053289c1e9b8c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_512f8fb2ca404adfa43933752b5b4990",
      "placeholder": "​",
      "style": "IPY_MODEL_e5dfa372bace44d592dfb6174ede3566",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "b8ed2b38fb684eea814889609bf679fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7edab73b97574a8b88cb6d2d8895c64b",
      "placeholder": "​",
      "style": "IPY_MODEL_dab2171a31a04d35afa13f4be8e9c93a",
      "value": " 666/666 [00:00&lt;00:00, 56.1kB/s]"
     }
    },
    "bd172de25a83456c901ef268642883ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abf71c210c314691b1bfcbc0491772fe",
      "placeholder": "​",
      "style": "IPY_MODEL_a6ec6fc5a0624039a6e50b0fef9c0a71",
      "value": " 3.25G/3.25G [00:09&lt;00:00, 314MB/s]"
     }
    },
    "be85927df4874f98a966452726e296e8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c2f797a8a10f4f39aafef4c19bf106db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c5550bc06ebe4a37b6789cdaa6329128": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cab34f5c61284ce3ad3010563bda7122": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c46c70b058947b488b4d4c9cdfa8b69",
      "placeholder": "​",
      "style": "IPY_MODEL_c2f797a8a10f4f39aafef4c19bf106db",
      "value": "model.safetensors: 100%"
     }
    },
    "cb8a173015aa4fc9aebc31fd364529c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd39a3042f2843c4916c2047ae6f43b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d343920cc98247a8aaa16f48506b1694": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d542096ea2d441ba91698399af48a2af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dab2171a31a04d35afa13f4be8e9c93a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de349f93b59c4cd289aa135d32d17b3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec447ebb8af94a8ab3338cb153f66ff5",
      "max": 666,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_864d448419da4711936a0e584c96964b",
      "value": 666
     }
    },
    "de85041e44f7432b9c58de466a95968a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5dfa372bace44d592dfb6174ede3566": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea54b3f282184db195845f1fdbc86f9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0d397c5d5eb467e9dba76d983e24926",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d343920cc98247a8aaa16f48506b1694",
      "value": 456318
     }
    },
    "ea6ab54e02f04d37993b2c4887d5c7ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a5a39bfc9f0429a910120e8e3cabe8f",
      "placeholder": "​",
      "style": "IPY_MODEL_4d614251c35b4ab3b9d7214909c80aae",
      "value": " 124/124 [00:00&lt;00:00, 7.46kB/s]"
     }
    },
    "ebddae646de9494f872de523a517d130": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec447ebb8af94a8ab3338cb153f66ff5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ec83dd4e6c3c461da065a91cc7adba45": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5477dbf0abb4af2a50201ab646532c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f5e6051d53584f1ba67af09b95f6bd50": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b64a6d701ebd4802be30cda4ed10783e",
      "placeholder": "​",
      "style": "IPY_MODEL_2232393916e349deb72bc20ac61c3bd7",
      "value": "tokenizer.json: 100%"
     }
    },
    "f6ce06ee38e54481815974ff06451f24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b85f192e77dc4bc8a8053289c1e9b8c9",
       "IPY_MODEL_744b41f6f55847e59e4c2985897ebb3f",
       "IPY_MODEL_70eaf260d9994bcab22ab1de76500107"
      ],
      "layout": "IPY_MODEL_cb8a173015aa4fc9aebc31fd364529c8"
     }
    },
    "ff237f2ab642427ba2c184031c6b2b5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
